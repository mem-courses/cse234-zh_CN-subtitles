1
00:00:11,050 --> 00:00:14,550
好的。谢谢你来参加。
Okay. Yeah. Thanks for coming.

2
00:00:14,550 --> 00:00:18,150
那我们开始今天的内容吧。
Yeah. Let's get started with today's content.

3
00:00:18,150 --> 00:00:27,570
好的。那么，我们再来回顾一下整体思路。
Yeah. So, again, recap the big picture.

4
00:00:27,570 --> 00:00:34,590
我们已经知道如何用数据流图来表示一个机器学习程序。
So we know how to represent a machine learning program, dataflow graph.

5
00:00:34,590 --> 00:00:41,249
我们也知道它的前向和反向计算，是由自动微分库驱动的。
We also know it's competition forward, backward, which was powered by AD libraries.

6
00:00:41,249 --> 00:00:45,850
你们其实已经在作业里实现过了，对吧？
You guys already implemented in your homework, okay?

7
00:00:45,850 --> 00:00:49,010
在这个图里，我们能得到很多东西。
And in that graph, we get a lot of things.

8
00:00:49,010 --> 00:00:52,069
其中之一就是有很多很多的算子。
One is, many many operators.

9
00:00:52,069 --> 00:00:56,969
所以我们要讨论如何优化这些算子，特别是那些重要的算子，
So we talk about how to optimize operators, especially those important ones,

10
00:00:56,969 --> 00:00:59,510
对，就是，呃，利用加速器来实现。
right, uh, using accelerators.

11
00:00:59,510 --> 00:01:02,189
另一个是我们有一个图，对吧。
The other is we have a graph, right.

12
00:01:02,189 --> 00:01:03,650
我们需要运行这个图。
We need to run that graph.

13
00:01:03,650 --> 00:01:08,490
所以我们必须优化这个图，我们之前也讨论过这个问题。我记得是上节课。
So we have to optimize graph, and we talk about that. I think last lecture.

14
00:01:08,490 --> 00:01:15,059
有自动化的方法，也有模板化的方法，对吧，无论是由专家还是由一些编译器完成。
There are automatic ways, there are template ways, right, either by experts or by some compilers.

15
00:01:15,059 --> 00:01:19,810
好的。那么基本上我们在那个图中已经准备好了大部分元素，对吧？
Okay. Then basically we have most of the elements ready in that graph, right?

16
00:01:19,810 --> 00:01:22,649
我们现在要协调执行过程，好吗？
We are going to orchestrate the execution now, okay?

17
00:01:22,649 --> 00:01:26,249
我们需要把数据放到图里，然后尝试得到结果，好吗？
We need to put data into the graph and try to get results, okay?

18
00:01:26,249 --> 00:01:31,230
今天我们将开始接触运行时的部分，好吗？
And today we are going to start touching on that part that is run time, okay?

19
00:01:31,230 --> 00:01:35,990
所以在我讲运行时之前，再次回顾一下，对吧？
So before I talk about run time, so just again, recap, right?

20
00:01:35,990 --> 00:01:38,029
我们正在做这种优化。
We are doing this kind of optimization.

21
00:01:38,029 --> 00:01:43,690
嗯，我们的目标是优化参数，我们优化的方式是，
Uh, our goal is trying to optimize the parameters, and we optimize it in a way that is,

22
00:01:43,690 --> 00:01:45,470
嗯，一个循环，对吧？
uh, a loop, right?

23
00:01:45,470 --> 00:01:47,310
所以我们在数据集上循环。
So we're looping over the dataset.

24
00:01:47,310 --> 00:01:54,379
每次我们取出一批数据，然后，嗯，我们计算梯度
And every time we fetch a batch of data and, uh, we calculate gradients

25
00:01:54,379 --> 00:01:58,460
并且不断地更新参数，直到收敛为止。
and update parameters again and again until convergence.

26
00:01:58,460 --> 00:02:04,600
好的，这里有一个非常非常重要的概念，叫做“批次”。
Okay. So here, there's a very, very important concept, batch.

27
00:02:04,600 --> 00:02:12,460
这个词在机器学习、并行计算、
This word is abused in many many different areas in machine learning in parallelism

28
00:02:12,460 --> 00:02:15,220
数据处理、大数据等很多领域都被滥用了，几乎每个人都在用“批次”这个词。
in data processing big data, like everyone using batch.

29
00:02:15,220 --> 00:02:19,539
所以在我讲运行时之前，我想先，嗯，确保我们理解一致，
So before I talk about runtime, I want to first, um, make sure we are on the same page

30
00:02:19,539 --> 00:02:21,400
当我们说“批次”的时候，明白吗？
when we talk about batch, okay?

31
00:02:21,400 --> 00:02:25,154
所以批次的意思是，有几分钟的时间。
So what batch means, there are a few minutes.

32
00:02:25,154 --> 00:02:28,030
在本课程的语境下，
So in the context of this course,

33
00:02:28,030 --> 00:02:31,349
我们基本上是在讨论部署时的批次，对吧？
I think we are basically talking about batch in the context of deploying, right?

34
00:02:31,349 --> 00:02:39,010
那么在部署，特别是在部署训练时，批次是什么意思？
So here in deploying, especially in deploying training, what does batch mean?

35
00:02:39,210 --> 00:02:45,309
它基本上是我们从更大的数据集中获取的一组数据样本，
It's basically a group of data samples we get from the bigger data set that can be

36
00:02:45,309 --> 00:02:47,609
可以在同一个计算图上执行，对吧？
executed on a graph in the same wrong, right?

37
00:02:47,609 --> 00:02:50,109
这就是一个批次。
So that is a batch.

38
00:02:50,109 --> 00:02:55,849
我们为每个批次计算梯度，然后将梯度应用到参数上。
And we derive a gradient for each batch, and we apply the gradient to the parameters.

39
00:02:55,849 --> 00:02:58,949
这就叫做随机梯度下降，对吧。
And that is called stochastic gradingt descent. Right.

40
00:02:58,949 --> 00:03:07,029
好的。但你可能也经常听到小批次和微批次，对吧？
Okay. But many places, you probably also hear about minibatch and micro baatch, right?

41
00:03:07,029 --> 00:03:10,170
那么它们和批量有什么不同？
So how are they different from batch?

42
00:03:11,170 --> 00:03:16,349
好的。其实minibatch是一个来自优化领域的术语。
Okay. So minibatch actually, it is a term from option.

43
00:03:16,349 --> 00:03:20,470
明白了吗？在优化中，我们有批量操作和小批量操作。
Okay? So in optim edition, we have batch of open addon and minibatch opmentation.

44
00:03:20,470 --> 00:03:26,109
在这里，基本上我们把这个minibatch的概念稍微泛化了一下，变成了依赖中的mini batch。
And here, so basically we generalize this minibatch a little bit into the mini batch in depending.

45
00:03:26,109 --> 00:03:30,730
基本上，在依赖中的minibatch，你可以认为minibatch等同于batch，好吗？
Basically in depending minibatch, you can think minibatch equals to batch, okay?

46
00:03:30,730 --> 00:03:36,729
也就是说，每次我们采样一批数据，在优化领域，人们称之为minibatch数据。
That is every time we sample bachel data, and in optim edition, people call that minibatchel data.

47
00:03:36,729 --> 00:03:39,129
好的，那什么是macro batch？
Okay? And what is macro baatch?

48
00:03:39,129 --> 00:03:41,569
macro比mini小，对吗？
So macro is smaller than mini, right?

49
00:03:41,569 --> 00:03:45,049
那什么是micro batch？有人知道吗？
So what is micro baatch? Anyone knows.

50
00:03:47,010 --> 00:03:50,509
其实你可以从它的名字看出来，对吧。
Okay. You can tell that from its name, right.

51
00:03:50,509 --> 00:03:53,069
所以微批次肯定是比批次还要小的东西。
So micro baatch is definitely something that even smaller than a batch.

52
00:03:53,069 --> 00:03:56,789
好的，我已经告诉你了，批次和小批次其实是一样的意思，只是说法不同。
Okay, I already told you that batch equals to minibatch in differently.

53
00:03:56,789 --> 00:03:59,430
所以微批次的数量比批次还要少。
So microbatch is a smaller quantity than batch.

54
00:03:59,430 --> 00:04:04,210
也就是说，我们可以把一个批次分成很多很多更小的批次，我们把
That is we can fer split a batch into many, many smaller batches and we call

55
00:04:04,210 --> 00:04:05,809
这种小批次叫做小批次。
that small batch a minibatch.

56
00:04:05,809 --> 00:04:09,430
哦，抱歉，是微批次。我刚才搞混了，抱歉。对，微批次。
Oh, sorry, microbatch. I was confused, sorry. Yeah. Micro baatch.

57
00:04:09,430 --> 00:04:13,409
好的，你可能会想，为什么我们需要微批次和批次，
Okay. And you're probably wondering why we need a micro baatch and a batch,

58
00:04:13,409 --> 00:04:14,829
这个我待会儿会详细讲解。
and I'm going to review that later.

59
00:04:14,829 --> 00:04:17,609
当我们开始讨论内存的时候，这是一个非常重要的概念，
Okay, it's a very important concept when we start talking about memory,

60
00:04:17,609 --> 00:04:20,850
当我们开始讨论并行处理的时候，明白了吗？
when we start talking about parlesms, okay?

61
00:04:20,850 --> 00:04:24,849
但请记住，微批次比批次要小，好吗？
But just remember, microbatch is a smaller thing than batch, okay?

62
00:04:24,849 --> 00:04:28,490
所以一个批次包含很多个微批次，明白吗？
So a batch has many microbatches, okay?

63
00:04:28,770 --> 00:04:36,110
好，我们回到优化的第二个领域，我们学习这种优化的理论和算法，
Okay, back to the second area in optimation we study this kind of opiation kind of

64
00:04:36,110 --> 00:04:38,909
尤其是梯度下降。
theory and algorithms, especially grading descent.

65
00:04:38,909 --> 00:04:41,829
我觉得那里的研究者用的术语有点不一样。
I think people there, they use a slightly different term.

66
00:04:41,829 --> 00:04:45,749
以SGD为例，对于梯度下降来说，
So they have two kinds of, for example, for SGD, for grading descent,

67
00:04:45,749 --> 00:04:47,070
他们有两种算法。
they have two kinds of algorithms.

68
00:04:47,070 --> 00:04:49,049
一种叫做全批量梯度下降。
One is called a full batch grading descent.

69
00:04:49,049 --> 00:04:51,569
另一种叫做随机梯度下降。
The other is called stochastic grading descent.

70
00:04:51,569 --> 00:04:54,969
明白了吗？所以在深度学习中，我们大多数时候用的就是随机梯度下降，对吧？
Okay? So in deep learning, we mostly just use stochastic grading descent, right,

71
00:04:54,969 --> 00:04:59,269
因为每次我们只是抽取一个小批量，然后尝试进行梯度更新。
because every time we just sample a small batch, and we try to perform grading updates.

72
00:04:59,269 --> 00:05:04,029
但是在选项中，呃，有一种梯度下降的版本。
But in option, uh, there's, like, ina version of grading descent.

73
00:05:04,029 --> 00:05:08,309
基本上，他们会在整个数据集上计算梯度。
That is basically, they will basically calculate grading over the entire dataset.

74
00:05:08,309 --> 00:05:14,029
好的，这意味着你的批量等于你的，呃，数据的大小，也就是整个数据集。
Okay, which means that your batch is equal to your, uh, size of data, okay, entire data set.

75
00:05:14,029 --> 00:05:16,889
这就叫做批量梯度下降。
So that is called a batch batch gradient descent.

76
00:05:16,889 --> 00:05:19,920
但我们通常不会用那个，原因取决于为什么。
But we don't use that depending on why.

77
00:05:19,920 --> 00:05:22,590
是的，因为数据太大了，我们做不到。
Yeah, because data is too large, we cannot do that.

78
00:05:22,590 --> 00:05:23,950
我们没有足够的内存，对吧？
We don't have enough memory, right?

79
00:05:23,950 --> 00:05:30,570
好吗？好的。只是想确保你理解了批量、小批量和微批量的区别。
Okay? Okay. Just to make sure that you try to understand the batch and minibatch and microbatch.

80
00:05:30,570 --> 00:05:34,889
好吗？在大数据处理领域，比如在Spark社区中，
Okay? And in big data processing, for example, in the community of how to spark,

81
00:05:34,889 --> 00:05:37,390
人们也会用这个词“批处理”，对吧？
people also use this word batch, right?

82
00:05:37,390 --> 00:05:43,349
通常这个词用来区分两种处理范式。
So usually this word is used to distinguish it from two kind of processing paradigm.

83
00:05:43,349 --> 00:05:44,850
一种叫做批处理。
One is called batch processing.

84
00:05:44,850 --> 00:05:47,149
另一种叫做流处理。
The other is called streaming processing.

85
00:05:47,149 --> 00:05:51,210
你也可以从这个词的含义中看出来，批处理
And you can also probably tell from the meaning of this word that batch processing

86
00:05:51,210 --> 00:05:52,390
基本上就是离线处理。
is basically offline processing.

87
00:05:52,390 --> 00:05:57,290
也就是说，我给引擎一个任务，这个任务是要处理
That is I'm giving the engine a task, and this task is trying to process

88
00:05:57,290 --> 00:06:00,849
一个非常大的数据集，把数据转换成另一种格式。
the data in a very big dataset into another format.

89
00:06:00,849 --> 00:06:05,589
我会把所有数据，也就是整个数据集直接输入到引擎里，然后等待结果。
And I directly fed all the data, the entire data set to the engine and I wait for the results.

90
00:06:05,589 --> 00:06:08,150
这就叫做离线批处理，对吧？
That is called offline batch processing, right?

91
00:06:08,150 --> 00:06:11,729
相比之下，流式处理基本上就是在线处理。
And in contrast, streaming processing is basically online processing.

92
00:06:11,729 --> 00:06:15,789
例如，当你提交谷歌查询时，呃，你可以在任何时间提交，
For example, when you submit Google query, uh, you can submit it anytime,

93
00:06:15,789 --> 00:06:17,269
对吧，这取决于你什么时候使用谷歌。
right, depending when you use Google.

94
00:06:17,269 --> 00:06:19,609
但在谷歌这边，他们基本上是在接收你的数据。
But on Google side, they are basically receiving your data.

95
00:06:19,609 --> 00:06:23,409
他们在监听你的数据，并试图对你的查询做出响应。
They are listening to your data and try to basically respond to your query.

96
00:06:23,409 --> 00:06:26,529
而为了响应你的查询，他们需要处理你的数据。
And in order to respond to your query, they need to process your data.

97
00:06:26,529 --> 00:06:30,350
他们还需要接收来自世界各地各种用户的大量查询，对吧？
And they need to receive many many queries from all kinds of users in the world, right?

98
00:06:30,350 --> 00:06:32,010
这就是流式处理。
That is streaming processing.

99
00:06:32,010 --> 00:06:39,029
好的。所以在大数据处理中，呃，当我们看到批处理时，基本上意味着离线。
Okay. So in big data processing, uh, well when we see batch, it basically means offline.

100
00:06:39,029 --> 00:06:43,089
明白了吗？当我们看到流处理时，基本上意味着在线。
Okay? And when we see streaming is basically means online.

101
00:06:43,089 --> 00:06:45,709
好吗？明白了吗？很酷。
Okay? Clear? Cool.

102
00:06:45,709 --> 00:06:49,610
所以今天的大部分课程，我们都会专注于，你知道的，
So most of today's course, we are going to focus on, you know,

103
00:06:49,610 --> 00:06:52,050
批处理、小批量和微批量，好吗？
batch minibatch, and microbatch, okay?

104
00:06:52,050 --> 00:06:54,870
很好。我还把表格放在这里了。
Cool. And I also put the table here.

105
00:06:54,870 --> 00:07:00,410
我想我之前已经重复过这些内容了，但你们可以之后再复习这个表格。
I think I already repeat the content in previously, but you can review this table layer.

106
00:07:00,410 --> 00:07:03,950
这个表格给出了一些非常清晰的定义，
So it gives some, like, a very clear definition of

107
00:07:03,950 --> 00:07:07,429
说明了在不同语境下我们所说的“批”是什么意思。
what do we mean when we see batch in different contexts.

108
00:07:08,040 --> 00:07:12,340
好的，有了这些，我们开始讲今天的学习目标。
Okay, with that, uh, we start talking about today's learning goal.

109
00:07:12,340 --> 00:07:16,600
我们要讨论内存和调度，因为为了让我们能够运行
Okay, we are going to talk about memory and scheduling, because in order for us to run

110
00:07:16,600 --> 00:07:22,360
计算图和算子，在我们的目标设备上，我们需要遵循内存约束，
the comping graph and operators, on our target device, we need to subject to memory constraints,

111
00:07:22,360 --> 00:07:24,559
因为我们的内存是有限的，对吧？
because we have limited memory, okay?

112
00:07:24,559 --> 00:07:30,219
我们现在要讲几个非常重要的内存组织技巧。
We're talking about a few very important memory organization techniques.

113
00:07:30,219 --> 00:07:33,740
我相信这节课非常实用，也非常有用。
And I believe this lecture is very practical and very useful.

114
00:07:33,740 --> 00:07:35,940
如果你能掌握这些内容，
And if you are able to get it,

115
00:07:35,940 --> 00:07:41,320
我认为你可以很快直接应用到实际中，因为这些技巧已经被广泛采用了，
I think you can directly apply it the other day because this is very well adopted techniques

116
00:07:41,320 --> 00:07:43,919
尤其是在深度学习和大模型训练中。
in pets in deep learning in large model training.

117
00:07:43,919 --> 00:07:48,479
好吗？如果时间允许，我们还会开始讨论
Okay? And if time permits, we are going to talk about start talking

118
00:07:48,479 --> 00:07:50,979
下一个重要的话题——量化。
about our next big topic that is quantization.

119
00:07:50,979 --> 00:07:54,420
很明显，我把这些内容放在一起讲，是因为量化
And apparently, I put this together because quantization,

120
00:07:54,420 --> 00:07:58,240
它最初的目标就是为了节省内存，对吧？
uh, its original goal is trying to save memory, right?

121
00:07:58,240 --> 00:08:02,159
但在某个时候，你知道，媒体也开始让量化变得更快，
But at some point, you know, media starts to make quantization also faster,

122
00:08:02,159 --> 00:08:04,359
所以它也可以加速计算。
so it can also accelerate the compute.

123
00:08:04,359 --> 00:08:10,790
明白了吗？很好。那么为什么内存会成为问题，就是因为这个原因，对吧？
Okay? Cool. So why memory is an issue because of this, right?

124
00:08:10,790 --> 00:08:14,449
我想我在不同的语境和不同的课程中反复讲过这个图，
I think I repeat this figure again and again in different contexts in different lectures,

125
00:08:14,449 --> 00:08:18,390
内存层级的B，因为一台计算机的限制，
B of memory hierarchy, because of the limit of one computer,

126
00:08:18,390 --> 00:08:24,509
嗯，所以我们的计算机会把东西存储在内存层级里，在每一层更高的层级，
um, so our computer store things in memory hierarchy, and at every layer at a higher layer,

127
00:08:24,509 --> 00:08:26,870
内存速度更快，但也更昂贵。
the memory is faster, but it's more expensive.

128
00:08:26,870 --> 00:08:30,769
在较低的层级，内存速度要慢得多，但我们有很多空间。
At lower layer, the memory is much slower, but we have a lot of space.

129
00:08:30,769 --> 00:08:36,225
嗯，这基本上就是我们把GPU放进计算机时的内存层级结构。
Um, so this is basically the memory hierarchy when we put GPUs into a computer.

130
00:08:36,225 --> 00:08:41,099
它可以让你了解每一层大概有多少内存。
And it gives you a sense like, uh, how many memory you have at each layer.

131
00:08:41,099 --> 00:08:45,180
下飞机的竞争，计算是在哪里进行的？
Deplaning competition, where is the computing happening?

132
00:08:45,180 --> 00:08:47,160
是在设备上的GPU上进行的。
It's on the device on GPUs.

133
00:08:47,160 --> 00:08:52,779
基本上，大多数时候我们只是把数据放在全局内存，也就是HBM。
Basically, most of the time we just put things on uh global memory which is HBM.

134
00:08:52,779 --> 00:08:56,060
有时候我们会把数据放在RAM里，也就是CPU的内存，
Sometimes we put data in RAM, which is the CPU memory,

135
00:08:56,060 --> 00:08:59,620
然后我们会从磁盘一直读取数据到RAM。
and we read data from disc all the way to the RAM.

136
00:08:59,620 --> 00:09:05,799
总的来说，现在你可以认为RAM是一种非常充足的资源，因为如果你去，
In general, today, you can think RAM is very abundant resources because if you go,

137
00:09:05,799 --> 00:09:10,160
呃，打开标签页，尝试切换任何GPU实例，你会发现有很多RAM，
uh tabs and you try to swap any GPU instances, you get a lot of RM for example,

138
00:09:10,160 --> 00:09:12,400
至少有一TB的RAM。
at least one terabytRMs.

139
00:09:12,400 --> 00:09:18,529
好的。然后更高级别的内存主要用于加速计算，
Okay. And then the higher level of memory is mostly used for accelerating computation,

140
00:09:18,529 --> 00:09:22,989
比如做分块、做我们在上一节课讲过的那些操作。
for example, doing telling, doing, those kind of things we covered in previous lecture.

141
00:09:22,989 --> 00:09:26,929
这里重要的一点是，我们的全局内存有限，
The important thing here is, we have limited global memory,

142
00:09:26,929 --> 00:09:30,969
但我们仍然想训练一个相当大的模型，那该怎么做呢？
but we still want to train a decent large model, so how to do that?

143
00:09:31,590 --> 00:09:36,750
嗯，在我们讨论那些增强技术之前，
Um, so before we talk about those ougmenting techniques,

144
00:09:36,750 --> 00:09:42,209
我认为我们需要考虑的一个重要问题是，内存是一种非常不同的资源，
I think one important thing we need to think is memory is very different resources

145
00:09:42,209 --> 00:09:43,830
与计算资源相比，好吗？
compared to compute, okay?

146
00:09:43,830 --> 00:09:46,550
在计算方面，我们希望尽可能快。
So in computer, we want to be as fast as possible.

147
00:09:46,550 --> 00:09:51,230
所以我们要最大化我们的计算速度，这样在有限的计算能力下，
So we want to maximize our computing speed so that given limited computing power,

148
00:09:51,230 --> 00:09:52,430
我们可以计算得更快。
we can compute much faster.

149
00:09:52,430 --> 00:09:54,029
我们可以更快地得到结果。
We can get our results faster.

150
00:09:54,029 --> 00:09:57,530
但在内存方面，我们的目标并不是尽量减少内存。
But in memory, our goal is not trying to minimize memory.

151
00:09:57,530 --> 00:10:03,210
好的，我们的目标是确保在执行计算图时的峰值内存
Okay, our goal is trying to make sure our peak memory when we execute the computing graph

152
00:10:03,210 --> 00:10:04,900
小于可用内存。
is smaller than available memory.

153
00:10:04,900 --> 00:10:07,209
不需要去最小化它。我们只需要确保
Don't how to minimize it. We just need to make sure

154
00:10:07,209 --> 00:10:09,749
峰值内存小于允许的内存。
the peak memory is smaller than the one that is allowed.

155
00:10:09,749 --> 00:10:14,770
明白了吗？为了让这更清楚一点，
Okay? So to make this even more clear,

156
00:10:14,770 --> 00:10:19,010
我认为，通常我们不知道如何管理内存。我们不知道如何管理内存。
I think, normally we don't how to me memory. We don't how to me memory.

157
00:10:19,010 --> 00:10:22,669
只要我们能以某种方式减少内存，使其能够适应GPU
As long as we can basically reduce memory in a way that can fit into GPO

158
00:10:22,669 --> 00:10:24,109
就可以了。我们并不想最小化它。
are good. We don't want to minimize it.

159
00:10:24,109 --> 00:10:27,480
好的，我们也不想最大化内存。
Okay. And we also do not want to mi max memory.

160
00:10:27,480 --> 00:10:29,650
所谓最大内存，基本上就是峰值内存。
So max memory is basically the peak memory.

161
00:10:29,650 --> 00:10:32,810
我们同样也不想最小化峰值内存，对吧？
We also do not want to minimize the peak memory, okay?

162
00:10:32,810 --> 00:10:38,849
所以go的基本思路是，我们要确保最大内存，也就是峰值内存，小于
And so go is basically we try to make sure that max memory, which is a peak memory is smaller than

163
00:10:38,849 --> 00:10:41,430
可用内存，除非有特殊说明。
available memory unless otherwise specified.

164
00:10:41,430 --> 00:10:42,830
这一点我稍后会详细讲。
And I'm going to talk about it later.

165
00:10:42,830 --> 00:10:45,470
记住，这里的go和计算是非常不同的。
Okay? Remember this go, is very different from compute.

166
00:10:45,470 --> 00:10:55,069
好的。那么为了理解内存消耗的来源，
Okay. Okay. So in order to understand where the memory consumption comes from,

167
00:10:55,069 --> 00:10:57,749
我觉得我们需要一步一步来，对吧。
I think we want to do it step by step, right.

168
00:10:57,749 --> 00:11:03,649
我们首先要理解深度学习程序是如何消耗内存的，对吗？
We want to first understand how deep learning programs, consume memory. Okay?

169
00:11:03,649 --> 00:11:06,569
那么我问的第一个问题是，给定这样一个组合图，
Then the first question I ask is given this kind of combiniograph,

170
00:11:06,569 --> 00:11:09,229
我记得我们在第三次课上做过这个，对吧？
I think we did this in our third lecture, right?

171
00:11:09,229 --> 00:11:12,550
这是一个带有均方误差损失的MSC单层神经网络。
This is MSC one layer neural network with MSE loss.

172
00:11:12,550 --> 00:11:18,890
好吗？面对这种连续的计算图，内存消耗的来源是什么？
Okay? Given this kind of continuo graph, what is the source of memory consumption?

173
00:11:21,090 --> 00:11:27,329
有人想回答吗？好，那我来讲一下。
Anyone want to answer? Okay, let me go through that.

174
00:11:27,329 --> 00:11:31,630
主要有几类资源需求，我希望你们记住这一点，好吗？
So there are major needs resources, and I want you to remember this, okay?

175
00:11:31,630 --> 00:11:35,669
这样将来你们接触任何部署程序时，总是要试着
So in the future, when you touch any deploying program, you always try to figure

176
00:11:35,669 --> 00:11:37,729
从这些资源中分析使用情况。
out the usage from these resources.

177
00:11:37,729 --> 00:11:40,909
好，第一个就是模型权重，对吧？
Okay? The first one is model weights, right?

178
00:11:40,909 --> 00:11:45,030
给你一个模型，无论它是否经过训练，你都需要分配
You are given a model. No matter it's trained or not, you basically need to allocate

179
00:11:45,030 --> 00:11:47,150
内存来存储模型权重。
memory for stored model weights.

180
00:11:47,150 --> 00:11:49,490
比如说，你想训练一个rest。
For example, you want to train a rest.

181
00:11:49,490 --> 00:11:53,789
一家餐厅通常有好几百万个参数，你需要把这些参数存储起来。
A restaurant typically has several minions of parameters and you need to store the minions of

182
00:11:53,789 --> 00:11:55,650
这些参数需要存储在HBM内存中。
parameters in memory in HBM.

183
00:11:55,650 --> 00:11:58,869
当你尝试训练第三个模型时，它有175亿个参数。
When you try to train three, it has 175 bunion.

184
00:11:58,869 --> 00:12:00,690
你必须存储这175亿个参数。
You have to store this 175 opinion.

185
00:12:00,690 --> 00:12:05,209
这就是内存消耗的一个来源，对吧？
So that is a source of memory consumption, right?

186
00:12:05,209 --> 00:12:09,470
一旦你把这些参数存储到TPO内存中，接下来你需要做什么呢？
And once you store the motorways into your TPO memory, what you need to do is you

187
00:12:09,470 --> 00:12:10,589
你需要进行计算，对吧？
need to perform compute, right?

188
00:12:10,589 --> 00:12:14,410
你需要按照计算图结构运行各种算子。
You need to run run run operators following the graph structure.

189
00:12:14,410 --> 00:12:18,649
每次你运行算子时，算子会产生
So every time when you run operators, your operator is going to produce

190
00:12:18,649 --> 00:12:20,169
一些比参数还要多的数据，对吧？
something that is more than motorways, right?

191
00:12:20,169 --> 00:12:22,529
比如说，激活值，我们称之为激活值。
For example, activations, we call that activations.

192
00:12:22,529 --> 00:12:27,920
所以，第二个内存消耗的来源基本上就是中间激活值。
So so the second source of memory consumption is basically the intermediate activation values.

193
00:12:27,920 --> 00:12:34,119
比如说，你输入一个数据批次，然后用你的模型和权重W相乘，
For example, you give a data badge and you multiply the data batch with your model with W

194
00:12:34,119 --> 00:12:36,720
通过一个矩阵运算，然后你得到输出。
through a metam and you produce output.

195
00:12:36,720 --> 00:12:39,760
而这个输出基本上就是中间激活值。
And the output is basically intermediate activation values.

196
00:12:39,760 --> 00:12:44,080
你必须以某种方式把它们存储在内存中，因为你需要
And you have to store them in memory in some way because you have to

197
00:12:44,080 --> 00:12:48,239
在反向传播时存储它们，这样你才能用它们来计算梯度，对吧？
store them in the backward pass, you can use it to drive your gradients, right?

198
00:12:48,239 --> 00:12:52,099
所以第二个来源就是中间激活值。
So the second source is intermediate activ values.

199
00:12:52,099 --> 00:12:53,840
那么还有第三个来源。
So there's a third source.

200
00:12:53,840 --> 00:13:01,220
那是什么？数据，数据，你可以看到数据基本上就是输入。
What is that? Data, Data, you can see the data is basically the input.

201
00:13:01,220 --> 00:13:03,420
第一版的中间激活值。
The first version of intermediate acting values.

202
00:13:03,420 --> 00:13:06,700
是的，总体结构其实很小。
Yeah. Gross stutre is pretty small.

203
00:13:06,700 --> 00:13:10,499
它基本上就是代码。没错，正是如此。
It's basically code. Yes, exactly.

204
00:13:10,499 --> 00:13:12,160
梯度。所以你必须存储梯度。
Gradients. So you have to store gradients.

205
00:13:12,160 --> 00:13:16,760
但因为最终你要复制梯度，并将梯度应用到参数上，
But because eventually, you want to dup the gradients and apply the gradiens to the parameters to

206
00:13:16,760 --> 00:13:18,719
得到下一次迭代，然后重复这个过程。
get the next iteration and you repeat.

207
00:13:18,719 --> 00:13:20,520
但不仅仅是梯度。
But, it's more than gradients.

208
00:13:20,520 --> 00:13:25,839
为什么？因为我们用的是非常复杂的优化方法，而且我们应用更新的方式
Why? Because we use very complicated opimeods and the way we apply updates is

209
00:13:25,839 --> 00:13:28,180
并不是简单地把梯度加到参数上。
not just we simply add the gradients to the parameters.

210
00:13:28,180 --> 00:13:31,879
有时候我们会做更复杂的操作，对吧？
Sometimes we do a lot more complicated manipulation, right?

211
00:13:31,879 --> 00:13:35,500
我们可能想通过某种方式操作梯度，比如配合某些时机。
We probably want to manipulate the gradients by timing it with something

212
00:13:35,500 --> 00:13:39,599
然后画出一些东西，再把它应用到参数上，以此来稳定训练，对吧？
and plot something and then apply it to the parameter to stabilize the training, right?

213
00:13:39,599 --> 00:13:43,459
这通常是在一些高级优化器里完成的，比如Adam，对吧？
And this is typically done in some advanced opium meters such as atom, right?

214
00:13:43,459 --> 00:13:45,760
所以我们把这部分称为优化器状态。
So we call this part opitor states.

215
00:13:45,760 --> 00:13:50,879
好的，这基本上给你一个全局的概念，是什么消耗了内存。
Okay. So that basically give you a global picture what consumes memory in

216
00:13:50,879 --> 00:13:55,440
深度学习程序有三部分：模型权重、激活值和优化器状态。
deep learning program three parts, model weights, activations and open menor states.

217
00:13:55,440 --> 00:14:00,879
我觉得你需要记住这个，因为这是一个非常重要的因素。
And I think you need to memorize this because, um, a very important factor later

218
00:14:00,879 --> 00:14:04,420
当我们之后讨论深度模型，比如大型语言模型时，
when we talk about deep model, a big models like activities language model,

219
00:14:04,420 --> 00:14:07,760
我们会一遍又一遍地反复强调这些内容，好吗？
we are basically grinding this again and again and again, okay?

220
00:14:07,760 --> 00:14:11,950
很好，这样你就有了一个全局的认识。
Cool. Okay, that'll give you a global picture.

221
00:14:11,950 --> 00:14:14,530
然后，为了分析内存消耗，
And then, in order to analyze the memory consumption,

222
00:14:14,530 --> 00:14:16,429
我觉得我们基本上关心两件事，对吧。
I think we basically care about two things right.

223
00:14:16,429 --> 00:14:20,109
记住我们的目标是确保峰值内存小于可用内存。
Remember our goals we make sure the peak memory is smaller than available memory.

224
00:14:20,109 --> 00:14:22,689
好吗？但是如何确定峰值内存呢？
Okay? But how to determine peak memory?

225
00:14:22,689 --> 00:14:27,829
所以为了确定峰值内存，嗯，我们需要搞清楚两件事，对吧。
So in order to determine peak memory, um we need to figure out two things, right.

226
00:14:27,829 --> 00:14:29,529
那些东西的大小是多少，对吧？
What is the size of those things, right?

227
00:14:29,529 --> 00:14:34,029
比如说，模型参数有多大，中间激活值有多大，
For example, how large is the model parameters, how large is the intermediate activations,

228
00:14:34,029 --> 00:14:35,890
操作状态有多大？
and how large is the opimeter state?

229
00:14:35,890 --> 00:14:40,990
这就是大小。明白吗？第二点是生命周期。为什么会有峰值内存？
That's the size. Okay? The second is lifetime. Why is peak memory?

230
00:14:40,990 --> 00:14:43,589
因为有时候某些内存会被使用，但有时候不会。
Because sometimes some memory will be used, but sometimes not.

231
00:14:43,589 --> 00:14:45,130
所以我们关心的是峰值。
So we care about the peak.

232
00:14:45,130 --> 00:14:48,210
为了找出峰值，我们需要确保我们理解
In order to figure out the peak, we need to make sure we need to understand

233
00:14:48,210 --> 00:14:51,729
深度学习程序中的每一个张量，当我们让你运行程序时，
each tensor in the deep learning program when we ask you the program,

234
00:14:51,729 --> 00:14:54,729
它们什么时候是“存活”的，什么时候不是，对吧？
when it all be alive and when it all not alive, right?

235
00:14:54,729 --> 00:14:58,930
因为我们不关心中间步骤的张量，因为我们不会为它们分配内存。
Because we don't care about the step tensors because we don't allocate memory for them.

236
00:14:58,930 --> 00:15:03,130
我们只在需要的时候分配内存，所以我们关心它们的生命周期。
We only allocate memory when we need them, so we care about their lifetime.

237
00:15:03,130 --> 00:15:06,850
明白了吗？所以在接下来的讲座内容中，
Okay? So in the following lectures, uh, in the following contents of this lecture,

238
00:15:06,850 --> 00:15:10,489
我们会一直关注程序中的不同张量，但是我们会，
we are going to always look at different tensors in the program, but we are going to, uh,

239
00:15:10,489 --> 00:15:13,610
我希望大家记住，我们基本上只关心它们的两点。
I want you guys to remember, we basically care about two things of them.

240
00:15:13,610 --> 00:15:17,400
一个是大小，另一个是生命周期，明白了吗？
One is the size. Second is the lifetime, okay?

241
00:15:17,400 --> 00:15:20,450
好的，让我们从一个非常简单的例子开始。
Okay, let's start with a very simple example.

242
00:15:20,450 --> 00:15:25,090
这是一个非常简单的神经网络，对吧，一个输入，两个线性层，一个激活层，
This is a very simple neural network, right, one input, two linear, one al,

243
00:15:25,090 --> 00:15:27,790
还有一个损失，总共五个节点。
and one loss, five nodes.

244
00:15:27,790 --> 00:15:33,070
假设我们不对它进行训练，假设这个模型已经训练好了，我们只是做推理，
Assume we don't twin it, assuming this model is already trained, we just perform inference,

245
00:15:33,070 --> 00:15:37,049
那我们来试着分析一下这个推理图的内存使用情况。
and let's try to analyze the memory usage of this inference graph.

246
00:15:37,049 --> 00:15:42,119
好的。那么为了进行推理，我们基本上是获取输入数据，对吧，
Okay. So in order to inference, we basically get input data, right,

247
00:15:42,119 --> 00:15:46,259
然后我们把数据从左到右全部传递过去。那么我们需要多少内存呢？
and we fit the data all the way from left to right. So how many memory we need?

248
00:15:46,580 --> 00:15:52,519
这三个项目，在整个推理过程中，
So these three items, for motorways, during the entire inference process,

249
00:15:52,519 --> 00:15:56,520
我们基本上可以把它们丢弃吗？
we can we basically, throw them away?

250
00:15:56,520 --> 00:15:59,279
不能，对吧，因为我们必须让它们一直保留在内存中
We cannot, right, because we help to keep them alive on

251
00:15:59,279 --> 00:16:02,679
GPU用于执行这种从左到右的计算。
GPUs in order to perform this kind of computation from left to right.

252
00:16:02,679 --> 00:16:07,199
那么对于高速公路来说，它将在整个生命周期内保持活跃，对吧？
So for motorways it'll be alive across the lifetime throughout the lifetime, okay?

253
00:16:07,199 --> 00:16:13,060
那么对于激活来说，就是我们如何处理激活内存。
So for activations, like how we basically deal with activation memory.

254
00:16:13,360 --> 00:16:17,039
首先，我们有一个输入，就是数据，对吧？
So at the first time, we have a input which is data, right?

255
00:16:17,039 --> 00:16:20,959
然后我们把数据输入到线性层，得到输出，对吧？
And then we fit the data into linear, we get output, right?

256
00:16:20,959 --> 00:16:24,714
然后我们可以把数据丢弃。
And then can through the data away.

257
00:16:24,714 --> 00:16:28,209
是的，我们不关心那些数据，对吧，因为只要我们得到了线性的输出，
Yes, we don't care about that, right, because as long as we get the output of the linear,

258
00:16:28,209 --> 00:16:32,649
我们基本上就可以丢弃和舍弃这些数据，因为我们可以继续
we can basically through the data and discard the data because we can proceed

259
00:16:32,649 --> 00:16:37,869
按照这个图结构往下走，最终我们还是能得到损失和准确的结果，对吧？
following this graph structure and eventually we can still get the loss, accurate results, right?

260
00:16:37,869 --> 00:16:42,470
所以基本上，一旦我们完成了线性的计算，就可以丢弃数据。
So basically, once we proceed the computation of linear, we can discard the data

261
00:16:42,470 --> 00:16:46,289
我们需要保留线性层的输出，也就是中间激活，对吧？
and we need to keep the output of the linear, which is the intermediate activation, right?

262
00:16:46,289 --> 00:16:51,169
然后我们不断重复这个过程，把这些缓冲区在不同的算子之间轮换使用。
And then we keep doing this keep doing this, we rotating those buffer across different operators.

263
00:16:51,169 --> 00:16:56,169
所以基本上，这告诉我们在进行推理时，只需要O级别的内存来计算
So basically, this tells us if we perform inference, we just need O memory for computing

264
00:16:56,169 --> 00:17:01,529
Depar网络最后一层的输出，通过循环使用两个缓冲区，对吧。
the final output of layer Depar network, by cycling through two buffers, right.

265
00:17:01,529 --> 00:17:04,409
好的，有道理，对吧？
Okay. Makes sense, right?

266
00:17:04,409 --> 00:17:09,209
好的，酷。那么对于权重来说，我们已经准备好了，
Okay, cool. So we already any For weights,

267
00:17:09,209 --> 00:17:11,969
我认为权重的生命周期基本上贯穿整个推理过程。
I think lifetime is basically throughout the entire inference.

268
00:17:11,969 --> 00:17:16,290
对于激活值，我们只关心当前在哪一层，对吧？
For activations, we only care about where we are now at which layer we are, okay?

269
00:17:16,290 --> 00:17:19,529
对于开放状态，我们没有这个东西。
For open matter states, we don't have it.

270
00:17:19,529 --> 00:17:21,429
这是推理阶段，我们不用关心那个。
It's inference. We don't care about that.

271
00:17:21,429 --> 00:17:23,765
好的。这是一个简单的问题，对吧？
Cool. This is an easy one, okay?

272
00:17:23,765 --> 00:17:29,639
那我们还需要估算我们创建的缓冲区的大小，对吗？
Then we also need to estimate the size of which buffer we created, right?

273
00:17:29,639 --> 00:17:34,919
所以我们要考虑权重是如何存储的，权重的大小是多少，以及我们如何存储中间激活值，
So how we store weight, what's the size of the weight and how we store intermediate activations and

274
00:17:34,919 --> 00:17:35,979
还有它们的大小是多少。
what's the size of them.

275
00:17:35,979 --> 00:17:40,739
而决定大小的一个关键因素其实是，
And one key factor that we need to determine the size is basically,

276
00:17:40,739 --> 00:17:42,499
我们用什么精度来存储，对吧？
in what precisions we store, right?

277
00:17:42,499 --> 00:17:45,720
我们用什么数据表示方法来存储数据？
What what data resentation we use to store data?

278
00:17:45,720 --> 00:17:50,460
呃，这里我给出深度学习中三种非常典型的数据表示方法。
Uh, Here I give three very typical data resentations in deep learning.

279
00:17:50,460 --> 00:17:53,134
第一种是浮点数0.32。
The first one is floating 0.32.

280
00:17:53,134 --> 00:17:57,729
好的，第二种是浮点数B float 16。
Okay. The second one is flowing points B float 16.

281
00:17:57,729 --> 00:18:00,729
第三个是流式的0.16，对吗？
And third one is flowing 0.16, okay?

282
00:18:00,729 --> 00:18:05,210
我们已经说过，第一个基本上是32位的。
We already call it the first one is basically 32 bits.

283
00:18:05,210 --> 00:18:08,550
好的，第二个是BF16，第三个是P16，对吗？
Okay. Second one BF 16, third one P 16, okay?

284
00:18:08,550 --> 00:18:13,030
这些都是我们在深度学习中非常常用的数据类型。
These are very very common used data types, we use in deping.

285
00:18:13,030 --> 00:18:19,800
那么你们有多少人知道数据是如何按照这些位来存储的？
So do you know how actually, how many of you know how data was stored following this bis?

286
00:18:19,800 --> 00:18:21,730
很好，很好，很好。
Cool, cool, cool.

287
00:18:21,730 --> 00:18:23,209
我们稍后会再回到这个问题。
We are going to come back to this later.

288
00:18:23,209 --> 00:18:24,409
好的，这一点非常重要。
Okay. This is very important.

289
00:18:24,409 --> 00:18:29,429
我需要确保你们所有人都确切知道这些位是如何存储数据的，因为
I need to make sure all you guys know exactly how it has stored in this bits because in order to

290
00:18:29,429 --> 00:18:31,869
要想更深入地理解连接，我们必须先明白这一点。
understand deeper in contadation, we have to understand this.

291
00:18:31,869 --> 00:18:33,489
但我们稍后会回到这个话题。
But we're going to come back to this later.

292
00:18:33,489 --> 00:18:37,649
但这里，核心的信息是，对于张量中的每个数值，
But here, the big message is essentially for each value in a tensor,

293
00:18:37,649 --> 00:18:45,510
我们可以用32位（也就是4字节）或者16位（也就是2字节）来存储它。
we can either store it in 32 bits, which equals to four bytes or 16 bits, which equals to two bytes.

294
00:18:45,510 --> 00:18:49,770
我们还有一些更低精度的方式，但这里我们先专注于这三种，好吗？
And we have some other lower precision, but here, let's focus on the three, okay?

295
00:18:49,770 --> 00:18:54,069
顺便说一下，B float 16，B代表什么？
By the way, the B flow 16. What does B stand for?

296
00:18:54,200 --> 00:18:58,660
什么？对，是Bring。我也不知道为什么，这有点奇怪。
Sorry? Yes. Bring. I don't know why, but it's a lame.

297
00:18:58,660 --> 00:18:59,860
这是谷歌想出来的，好吗？
Come up by Google, okay?

298
00:18:59,860 --> 00:19:06,330
好的，酷。嗯，我们稍后会再回到这个话题。
Yeah. Cool. Um, oh, welcome back to this later.

299
00:19:06,330 --> 00:19:10,949
好的，那我现在要做一个非常实际的例子。
Okay. Then I'm going to do a very practical example.

300
00:19:10,949 --> 00:19:16,950
这是我从非常有名的GBD论文中引用的一张表。
Okay. This is a table. I quoted from the very phenomenon GBD paper.

301
00:19:16,950 --> 00:19:21,349
这可能是过去十年里最好的论文之一，对吧？
One of the best paper of probably in the past ten years, right?

302
00:19:21,349 --> 00:19:24,809
这是描述GBD的表格，
And this is the table that describes the GBD,

303
00:19:24,809 --> 00:19:28,189
展示了从最小的GBD r到
different archectures all the way from the smallest GBD r

304
00:19:28,189 --> 00:19:32,630
实际的GPs，也就是拥有1750亿参数的模型。
to the actual GPs that is 175 billion parameters.

305
00:19:32,630 --> 00:19:38,069
你可以阅读这些列，基本上可以看懂这些列名，对吧？
And you can read you can basically read the columns, the column names, right?

306
00:19:38,069 --> 00:19:39,870
第一列是模型名称。
The first column is model lame.

307
00:19:39,870 --> 00:19:42,329
第二列是参数数量，对吧？
Second one, second is lumber parameters, right?

308
00:19:42,329 --> 00:19:45,029
它让你了解这个模型有多少参数。
It gives you a sense, how many parameters that model has.

309
00:19:45,029 --> 00:19:47,109
第三列是层数。
Third column amber layers.

310
00:19:47,109 --> 00:19:49,430
好的，第一列是D模型。
Okay. First column D model.

311
00:19:49,430 --> 00:19:54,549
这里的D模型基本上代表的是transformer的隐藏维度。
So here D model basically stands for the hidden dimension of transformers.

312
00:19:54,549 --> 00:19:58,870
明白了吗？你可以把它理解为dpering中的H。
Okay? So you can understand it as the H in dpering.

313
00:19:59,390 --> 00:20:06,879
在多头注意力中是每个头的隐藏维度，我们会在语言模型的这一层再回到这里。
In head is amber he in much head attention, come back to this layer in LM. Okay.

314
00:20:06,879 --> 00:20:12,230
而Di head是指头部的维度。
And the Di had is DHAd is the head dimension.

315
00:20:12,230 --> 00:20:14,530
也是每个头的隐藏维度。
Okay. Also a hidden dimension for each head.

316
00:20:14,530 --> 00:20:18,670
batch size基本上就是我们用来训练这个模型的批量大小，
And the batch side is basically the bat side we use to train this model,

317
00:20:18,670 --> 00:20:20,829
对吧？还是内存批量，对吧？
right? Still memory batch, right?

318
00:20:20,829 --> 00:20:23,170
所以它基本上就是批量安装的数量。
So it's basically the batch install castra des.

319
00:20:23,170 --> 00:20:27,850
学习率基本上就是我们如何更新参数的。
Okay. And learning rate is basically how we update updates to the parameters.

320
00:20:27,850 --> 00:20:30,730
明白了吗？我们将用这个例子来说明。
Okay? We're going to use this example.

321
00:20:30,730 --> 00:20:34,315
我们尝试分析这些模型的内存使用情况，好吗。
We try to analyze the memory usage of these models, okay.

322
00:20:34,315 --> 00:20:38,080
所以记住有三个部分，对吧，模型权重，
So remember three parts right model ways, um,

323
00:20:38,080 --> 00:20:41,959
激活值和操作状态。我们一个一个来讲。
uh, activations and opon states. We'll start one by one.

324
00:20:41,959 --> 00:20:46,180
好的，那对于模型权重，我们先看最大的那个，好吗？
Okay. So for model ways, let's just look at the biggest one, okay?

325
00:20:46,180 --> 00:20:49,439
最大的那个有1750亿个参数，对吧？
The biggest one has 175 billion parameters, right?

326
00:20:49,439 --> 00:20:53,919
对于每个参数，我们要么用16位，要么用32位来存储，对吧？
And for each parameter, we either store it in 16 bits or 32 bits, right?

327
00:20:53,919 --> 00:20:55,359
那我们用了多少内存呢？
So how many memory we use?

328
00:20:55,359 --> 00:20:57,619
我们基本上就是把两个值相乘，对吧？
We basically times two values together, right?

329
00:20:57,619 --> 00:21:04,879
然后我们得到这个数，1750亿乘以2或4。
And we get, um, this number 175 billion times two or four.

330
00:21:04,879 --> 00:21:12,259
所以基本上，为了存储TBD模型的权重，我们需要350GB或70GB的内存。
So basically, in order to store TBD models was, we need a 350 giga or 70 gigabyte memory.

331
00:21:12,259 --> 00:21:15,460
你有多少H100的显存？
How many memory you have H 100?

332
00:21:16,140 --> 00:21:21,759
80，对吧？这意味着在H100上训练这个模型是不可能的，对吗？
80, right? Which means that it's impossible to train this model on H 100, right?

333
00:21:21,759 --> 00:21:22,360
这是个问题。
That's a problem.

334
00:21:22,360 --> 00:21:23,919
好的，我们很快会解决这个问题。
Okay. We're going to address that problem soon.

335
00:21:23,919 --> 00:21:26,140
但这基本上就是现实的检验。
But this is basically the reality check.

336
00:21:26,140 --> 00:21:31,640
好的，实际上，我们用的是两字节。
Okay. And in fact, um, we use two bites.

337
00:21:31,640 --> 00:21:35,979
我们为TPT-3用了16位精度。
We use 16 bits for that, for TPT three.

338
00:21:36,430 --> 00:21:42,549
好的，所以估算模型大小或者参数量的经验法则是，
Okay, so the room thumb to estimate the model size, the primter size, basically,

339
00:21:42,549 --> 00:21:46,050
我们首先要检查精度，比如我们在看论文时，
we first check the precision for example, when we read the machinery in paper,

340
00:21:46,050 --> 00:21:49,170
我们会检查模型参数用了什么精度。
we check what precision used to model parameters.

341
00:21:49,170 --> 00:21:53,430
就像我说的，大多数模型都是以16位或32位存储的。
Like I said, most models they are stored either in 16 bits or 32 bits.

342
00:21:53,430 --> 00:21:56,190
一旦我们确定了精度，基本上就是用2或4去乘以模型参数的数量来得到模型的大小。
And once we figure out precision, we basically use times

343
00:21:56,190 --> 00:21:58,810
明白了。好。接下来我们开始估算激活值或者说第二部分的内存消耗，好吗？
two or four to get the size of the model parameters.

344
00:21:58,810 --> 00:22:06,879
对于激活值，我会举几个例子，因为就像我说的，
Okay. Clear. Okay. Then we start estimating activations or second, uh, memory consumption, okay?

345
00:22:06,879 --> 00:22:10,839
在数据流图中，不同的算子，它的激活值是不同的，这取决于
So for activations, I'm going to run through a few examples because like I said,

346
00:22:10,839 --> 00:22:15,179
算子的定义和计算方式，我们基本上会
uh, for different operators in the dataflow graph, its activation is different depending

347
00:22:15,179 --> 00:22:18,519
覆盖几个非常重要的算子。好的。
on its operator definition and competion we are going to basically

348
00:22:18,519 --> 00:22:21,099
还记得这个吗？这是二维卷积。我在第二节课讲过。
cover a few very important operators. Okay.

349
00:22:21,099 --> 00:22:26,219
基本上就是你用一个滤波器在特征图上滑动，
Still remember this one, this is column two D. I covered that in my second lecture.

350
00:22:26,219 --> 00:22:29,340

It's basically like you apply a filter through the fissure map,

351
00:22:29,340 --> 00:22:33,099
你的输入是上一层产生的激活值，而你的输出
Your input is the activation produced by the previous layer and your output

352
00:22:33,099 --> 00:22:34,660
基本上就是你自己的激活值。
is basically your own activation.

353
00:22:34,660 --> 00:22:38,179
所以为了估算第二列D的激活值大小，
So in order to estimate the size of the activation for column two D,

354
00:22:38,179 --> 00:22:40,299
我们基本上要看输入和输出的大小，对吧？
we basically look at input and oppose size, right?

355
00:22:40,299 --> 00:22:43,799
那么对于输入，我们有一个四维张量，对吧？
So for input, we have four dimensional tensor, right?

356
00:22:43,799 --> 00:22:46,180
BS代表批量大小。
BS which stands for byte size.

357
00:22:46,180 --> 00:22:48,654
你输入给神经网络的图片数量。
Number of images you give to that neural network.

358
00:22:48,654 --> 00:22:51,969
NC是通道数，对吧？
NC is the number of channels, right?

359
00:22:51,969 --> 00:22:55,349
对于第一层的输入，通道数等于三，
For the first layer input, the number channel equals to three,

360
00:22:55,349 --> 00:22:58,669
因为图片是用RGB表示的，明白吗？
because the image was represented using RTB, okay?

361
00:22:58,669 --> 00:23:04,129
那么WN HI基本上就是输入特征图的高度和宽度，对吧？
And then WN HI is basically height and weight of input feature map, right?

362
00:23:04,129 --> 00:23:12,369
所以基本上你的输入是BSN WHI，然后你应用一个滤波器，基本上就得到BSN C WSU，
So basically your input, um, uh, is BSN WHI, and you apply a filter and you basically get BSN C WSU,

363
00:23:12,369 --> 00:23:16,330
其中WSU代表特征图的大小。
where WSU stands for the size of the fissure map.

364
00:23:16,330 --> 00:23:20,595
因此，你的激活基本上就是输出特征图的大小。
Therefore, your activation is basically the size of the output fure map.

365
00:23:20,595 --> 00:23:22,259
那我们有多少个数值呢？
So how many values we have?

366
00:23:22,259 --> 00:23:23,759
我们只需要把这些都相乘，对吧？
We just time all these together, right?

367
00:23:23,759 --> 00:23:26,780
然后我们还需要乘以每个元素的大小。
And then we need to time the size of element.

368
00:23:26,780 --> 00:23:30,360
所以这取决于你用什么精度来存储数值。
So it depends on what kind of precision you use to store value.

369
00:23:30,360 --> 00:23:34,400
但大多数卷积网络中，我们用的是32位。
But mostly in most coverution networks, we use 32 bits.

370
00:23:34,400 --> 00:23:36,840
好，这和transformer稍微有点不同，明白吗？
Okay, it's slightly different from transformer, okay?

371
00:23:36,840 --> 00:23:38,559
所以那个值应该是四。
So that value should be four.

372
00:23:38,559 --> 00:23:41,869
在大多数网络中，好的，大多数通信网络中。
In most nutwors, okay, most commotion networks.

373
00:23:41,869 --> 00:23:47,359
好的。然后我们做一个非常简单的例子，就是一个矩阵乘法。X 是我们的输入。
Okay. Then we do a very simple one, which is a met M. X is our input.

374
00:23:47,359 --> 00:23:49,699
W 是权重，C 是输出。
W weights and C is the output.

375
00:23:49,699 --> 00:23:53,460
所以我们估算激活，非常简单。
So we estimate the activation, very easy.

376
00:23:53,460 --> 00:23:56,259
输入是三维张量。
Input three dimensional tensor.

377
00:23:56,259 --> 00:24:01,200
原始矩阵的形状是 times 三，但我们有一个 B 维度，
The original matrix was by the shape of times three, but we have a B side dimension,

378
00:24:01,200 --> 00:24:03,480
对吧，所以把它们全部加在一起。
right, so wet all them together.

379
00:24:03,480 --> 00:24:08,619
呃，W 的形状是 by P。所以当我们应用这个矩阵乘法时，
Uh the W is of the shape by P. So when we apply this met M,

380
00:24:08,619 --> 00:24:11,099
我们基本上得到的是 Bs 乘以 M 乘以 P，对吧？
we basically get Bs times M times P, right?

381
00:24:11,099 --> 00:24:12,880
那就是我们的输出激活。
That's our output activation.

382
00:24:12,880 --> 00:24:20,519
好的。当然，你需要根据你使用的过程来乘以元素的大小。
Okay. Of course, you need to multiply a size of element, depending on what procedure you use.

383
00:24:20,519 --> 00:24:22,924
很好，这个很简单，对吧？
Cool. This one is easy, right?

384
00:24:22,924 --> 00:24:25,170
好的，当然，是关于transformers的。
Okay, sort of, of course, transformers.

385
00:24:25,170 --> 00:24:28,829
好吗？我们会详细讲解这个transformer，就像我说的那样。
Okay? We are going to ground this transformer a lot and like I said.

386
00:24:28,829 --> 00:24:32,930
那么transformers的激活大小是多少？
So what is the size of transformers activation?

387
00:24:32,930 --> 00:24:36,389
呃，不，我们先忽略所有中间操作。好的。
Uh, no, let's ignore all the intermediate operators. Okay.

388
00:24:36,389 --> 00:24:38,009
我们只关注输入和输出。
We only focus on input and output.

389
00:24:38,009 --> 00:24:40,650
好的。那么它的输入是什么？
Okay. So what input it takes?

390
00:24:43,560 --> 00:24:47,059
是的，所以第一个维度肯定是大小，对吧？
Yeah, so the first dimension definitely by size, right?

391
00:24:47,059 --> 00:24:49,039
所以我们输入一批序列。
So we fit a batch of sequences.

392
00:24:49,039 --> 00:24:51,640
但问题是第二维或第三维是什么？
But the question is what's the second or third dimension?

393
00:24:51,640 --> 00:24:55,179
嗯，这里有一个嵌入维度，对吧。
Uh. So there's a embedding side, right.

394
00:24:55,179 --> 00:24:58,800
嵌入维度基本上就是你如何把单词编码成嵌入向量。
Embedding size is basically how you encode the word into embeddings.

395
00:24:58,800 --> 00:25:01,639
但和Mat Mo相比，这里多了一个维度，基本上
But compared to Mat Mo there's one more dimension which basically

396
00:25:01,639 --> 00:25:03,039
我们并没有对序列进行建模，对吧？
we're not modeling sequences, right?

397
00:25:03,039 --> 00:25:05,920
所以我们有一个序列长度的维度，明白了吗？
So we have a sequence lens dimension, okay?

398
00:25:05,920 --> 00:25:09,979
所以我们的输入基本上是，呃，批量大小乘以H，
So our input is basically uh, batch size times H,

399
00:25:09,979 --> 00:25:13,039
H是嵌入维度，还有序列长度，好吗？
H is embedding size and sequas, okay?

400
00:25:13,039 --> 00:25:15,019
在这里，你可能已经发现了不同之处。
And here, you probably spot the difference.

401
00:25:15,019 --> 00:25:18,219
所以基本上，激活实际上会随着一些因素增长，对吧。
So basically the activation actually grows with a few things, right.

402
00:25:18,219 --> 00:25:22,720
在之前的数学模型中，它只随着比如电池容量和边缘增长，
In the previous math model, it only grows with, for example, battery size and edge,

403
00:25:22,720 --> 00:25:24,860
但在这里你加入了一个词维度和序列长度。
but here you add one word dimension with sequence sans.

404
00:25:24,860 --> 00:25:29,719
这就是为什么，在语言模型社区里，人们总是试图
That's why, uh, in the language model community, people always trying to

405
00:25:29,719 --> 00:25:34,399
开发能够建模更长上下文的模型，而这会带来很多问题，
develop something that model longer contexts, and that will raise a lot of problems in

406
00:25:34,399 --> 00:25:37,419
无论是在计算模型还是在这种内存中都会有问题。
either computing model and also in this kind of memory.

407
00:25:37,419 --> 00:25:43,079
明白了吗？同样的，你基本上会产生完全一样的输出结果，好吗？
Okay? And same thing, you basically produce exactly the same ship output, okay?

408
00:25:43,079 --> 00:25:46,750
嗯，这里是激活的部分。
Um, so here's the activating sides.

409
00:25:46,750 --> 00:25:51,289
所以这里我把问题简化了一点，因为我只关注输入和输出。
So here I simplify the problem a little bit because I only to focus on the input and output.

410
00:25:51,289 --> 00:25:55,930
但实际上，在中间，有很多操作符，比如softmax教学，
But actually, in between, right, there are so many operators, for example, teaching softmax,

411
00:25:55,930 --> 00:26:01,949
他们实际上也会产生一些激活值，嗯，不过这里我们先跳过这个部分。
and they actually also create some activation values, um, and but here, let's skip that for now.

412
00:26:01,949 --> 00:26:03,129
但之后我们会再回到这里。
But later we'll come back to this.

413
00:26:03,129 --> 00:26:11,379
好的，酷。那么接下来我们回到这个GBD的例子，对吧？
Okay. Cool. Okay. With that, then we come back to this GBD example, right?

414
00:26:11,379 --> 00:26:16,279
我们了解了transformers的活动和规模，而GBD其实就是
We understand the activity and size of, uh, transformers, and GBD is

415
00:26:16,279 --> 00:26:18,120
transformer的解码器。
essentially transformers, decoders.

416
00:26:18,120 --> 00:26:23,339
所以现在我们应该能够估算GVD三中的激活值了，对吧？
So we now should be able to estimate the activations in GVD three, okay?

417
00:26:23,339 --> 00:26:26,959
那么现在，让我们把问题简化一点。
So now, let's simplify problem a little bit.

418
00:26:26,959 --> 00:26:28,519
假设序列长度等于一。
Let'sume sequence is equal to one.

419
00:26:28,519 --> 00:26:32,760
好的，也就是说我们只建模一个token、一个词的序列。
Okay. That is we just model sequences of one token, one word.

420
00:26:32,760 --> 00:26:35,275
那么，激活值是多少呢？
Okay. So what is the activation?

421
00:26:35,275 --> 00:26:38,229
所以我们基本上是从那个表格中获取一些数值，对吧？
So we basically fact some values from that table, right?

422
00:26:38,229 --> 00:26:44,789
记住，我们需要一个电池大小、六层和一个隐藏维度，对吧？
Remember, we need a battery size, six lens and a hidden dimension, right?

423
00:26:44,789 --> 00:26:47,630
就像我说的，D模型基本上就是一个隐藏维度。
Like I said, the D model basically is a hidden dimension.

424
00:26:47,630 --> 00:26:54,329
明白了吗？所以每个TPDray，也就是transformer层的激活基本上是BS乘以序列长度再乘以
Okay? So the activation of each TPDray um, transformer layer is basically BS time sequence and time

425
00:26:54,329 --> 00:26:57,290
D模型，我们看最后一行，好吗？
Dmdel we look at the last row, okay?

426
00:26:57,290 --> 00:27:00,650
所以电池大小基本上是320万。
So the battery size is basically, 3.2 million.

427
00:27:00,650 --> 00:27:05,699
这是一个相当大的批量大小，好吧，隐藏维度是12288。
That's a pretty large bat size, okay, and the hidden dimension is 12288.

428
00:27:05,699 --> 00:27:07,929
好的，然后六层等于一，对吧？
Okay. And the six line is equal to one, right?

429
00:27:07,929 --> 00:27:09,690
我说过，我认为这是个问题。
I said, I think the problem.

430
00:27:09,690 --> 00:27:13,630
所以你可以看到，为了20GB的数组，
So you can see, in order to 20 GB array,

431
00:27:13,630 --> 00:27:15,209
我只是把六行建模成一行。
I just model like a six line equal to one.

432
00:27:15,209 --> 00:27:18,449
我已经需要这么多的活动内存了，对吧？
I already need this many of active memory, right?

433
00:27:18,449 --> 00:27:20,289
就像我说的，我已经告诉过你了，
So, like I said, I already told you that

434
00:27:20,289 --> 00:27:22,849
GBs 是用 P16 训练的，对吧？
GBs was trained using P 16, right?

435
00:27:22,849 --> 00:27:25,709
基本上，第一个数字是 78 GB。
So basically the first number 78 giga.

436
00:27:25,709 --> 00:27:28,130
所以一张卡只有 8 GB。
So one Y only have eight giga.

437
00:27:28,130 --> 00:27:32,029
所以在这里执行一次前向传播是不可能的。
So it's impossible to perform a single forward pass here.

438
00:27:32,029 --> 00:27:35,669
这只是针对一层。我们有多少层？
This is just for one layer. Like how many layer we have?

439
00:27:35,750 --> 00:27:40,050
就在那边，对吧？是 96 层，好吗？
It's over there, right. It's 96 layers, okay?

440
00:27:40,050 --> 00:27:41,829
是啊，这有点疯狂，对吧？
Yeah, this is a little bit crazy, right?

441
00:27:41,829 --> 00:27:44,419
你可以想象一下，我们需要多少个GP才能达到20GB的三倍。
You can imagine how many GP we need to 20 GB three.

442
00:27:44,419 --> 00:27:51,610
好的。我想再次强调的是，在每个transformer层内部的每个算子，
Okay. One thing I want to emphasize again is for each operator inside of transformer layer,

443
00:27:51,610 --> 00:27:52,870
我们也有激活值。
we also have activations.

444
00:27:52,870 --> 00:27:55,329
我们稍后会回到这个话题，好吗？
We'll come back to this later. Okay.

445
00:27:55,930 --> 00:28:01,130
很好。我们在激活值这个问题上还是一致的。
Cool. We are still on the same page for activations.

446
00:28:01,130 --> 00:28:04,990
好的？现在，让我们转到第三方的open minor状态。
Okay? Now, let's move to the third party open minor states.

447
00:28:04,990 --> 00:28:09,109
记住，我们需要推导梯度，还需要操作这些梯度，最终
Remember, we need to derive gradients and we need to manipulate the gradients and eventually

448
00:28:09,109 --> 00:28:10,950
应用这些梯度来更新参数。
apply the gradits to update the parameters.

449
00:28:10,950 --> 00:28:13,329
我们依然以DVD为例。
We all still use DVDs are example.

450
00:28:13,329 --> 00:28:19,370
我们都在尝试弄清楚，我们需要多少open mind states的内存来存储open states。
We all try to figure out how many open mind states memory we need to store opin states.

451
00:28:19,490 --> 00:28:21,590
我们来切奶酪吧，好吗？
Let's cut and cheese, okay?

452
00:28:21,590 --> 00:28:25,490
我们不讨论单纯形，因为我们不关心SGD，对吧。我们只关心Adam。
We don't talk about simplices because we don't care about SGD, right. We only care about atom.

453
00:28:25,490 --> 00:28:28,150
我已经告诉过你，他们优化的所有模型都是用Adam。
I already told you that all the models they are optimizing atom.

454
00:28:28,150 --> 00:28:30,590
那我们就直接讲这个算法，Adam吧。
So how about we just pass this algorithm, Adam.

455
00:28:30,590 --> 00:28:32,729
好吗？这是一个Adam算法。
Okay? This is a AAM algorithm.

456
00:28:32,729 --> 00:28:34,990
这段话我引用自Adam的论文。
I quote it from Adam paper.

457
00:28:34,990 --> 00:28:38,229
好的，我让你们看一下这个内容。
Okay, I'll let you look at this a little bit.

458
00:28:38,229 --> 00:28:40,969
大概15秒钟吧。
Maybe 15 seconds.

459
00:28:59,400 --> 00:29:01,739
好的，很棒，我们继续往下讲。
Okay, cool, let's pass this.

460
00:29:01,739 --> 00:29:08,780
所以这个Adam算法和普通SGD之间唯一的区别是——
So the only difference that between this atom algorithm and vana SGD.

461
00:29:08,780 --> 00:29:11,759
所以还记得在普通的SGD中，我们基本上是用梯度对吧。
So remember in vana SGD, we basic the gradings, right.

462
00:29:11,759 --> 00:29:14,599
我们把它乘以一个很小的学习率，然后
We multiply it by a small learning rate and then

463
00:29:14,599 --> 00:29:18,819
呃，把它加到参数上或者调整参数来进行更新。
uh add or malance into the parameter to get the update.

464
00:29:18,819 --> 00:29:20,420
所以唯一的区别在于
So the only difference between

465
00:29:20,420 --> 00:29:26,960
Adam和普通的随机梯度下降的区别基本上是，呃，为了实际计算
Adam and and vana stochastical grading is basically, uh, in order to actually calculate

466
00:29:26,960 --> 00:29:31,620
要应用到参数上的那个值，呃，我们需要对梯度进行一些操作
the value that is going to be applied into the parameter, uh, we need to manipulate the gradients

467
00:29:31,620 --> 00:29:34,560
使用一阶和二阶矩。
using uh the first and second moment.

468
00:29:34,560 --> 00:29:40,219
一阶矩其实就是均值，二阶矩其实就是方差。
The first moment is basically the mean and the second moment is basically the uh the variance.

469
00:29:40,219 --> 00:29:45,939
那么为了做到这一点，我们要做的是，在应用不同的小批量数据时
So in order to do that, what do we do is, as we apply different mini different batches

470
00:29:45,939 --> 00:29:52,460
我们基本上要保存一些状态来跟踪优化过程中的矩。
and we basically mean some states to track the moment across the optimi intrajectory.

471
00:29:52,460 --> 00:29:56,880
每次我们计算新的梯度时，我们都会更新动量。
And every time when we calculate a new gradient, we are going to update the moment

472
00:29:56,880 --> 00:29:58,699
对于一阶和二阶动量，明白了吗？
for first and second moment, okay?

473
00:29:58,699 --> 00:30:04,199
如果你看这行，最后一行，我们是如何应用这个主要的更新的，就是，
And if you look at this line, the last line, how we apply this primary update, that is, uh,

474
00:30:04,199 --> 00:30:11,199
我们对这个梯度进行归一化，然后除以标准差，然后应用它。
we take we normalize this gradient and divide it by standard deviation, okay, and we apply it.

475
00:30:11,199 --> 00:30:13,079
明白了吗？基本上，你可以这样理解，
Okay? Basically, you can think,

476
00:30:13,079 --> 00:30:16,999
SGD和Adam的区别在于我们以某种方式对梯度进行了归一化。
The difference between SGD and atom is we normalize the gradients in some way.

477
00:30:16,999 --> 00:30:20,960
但为了归一化梯度，我们必须维护一阶和二阶动量。
But in order to normalize the gradients, we have to maintain first and second moment.

478
00:30:21,280 --> 00:30:28,199
问题在于，我们需要存储一阶和二阶动量，以及最终的轨迹。
The problem is here, we need to store the first and second moment, along the ultimate intrajectory.

479
00:30:28,199 --> 00:30:30,659
我们来看一下需要多少内存。
Let's look at how many memory we need.

480
00:30:30,659 --> 00:30:36,699
好吗？对于梯度，呃，我们所需的内存基本上是，
Okay? So for gradient, uh, the memory we needed is basically,

481
00:30:36,699 --> 00:30:39,899
应该是参数的数量相同，对吧？
uh, should be the same size of parameters, right?

482
00:30:39,899 --> 00:30:41,999
那么有多少参数，就有多少梯度？
So how many parameters, how many gradients?

483
00:30:41,999 --> 00:30:44,860
对于梯度来说，基本上就是元素数量的倍数。
So for gradient, basically in times size of element.

484
00:30:44,860 --> 00:30:48,839
好吗？一阶矩也是元素数量的倍数，对吧？
Okay? The first moment is also in time cell moment, right?

485
00:30:48,839 --> 00:30:53,799
就是均值，对吧。二阶矩也是元素数量的倍数。
It's mean, right. And the second moment is also in times set of element.

486
00:30:53,799 --> 00:31:01,379
我们需要为Adam执行的总操作状态，基本上是三倍的元素数量。
The total op state we need to perform atom is basically three times size of 11.

487
00:31:01,379 --> 00:31:05,999
这个有点疯狂，对吧，因为我记得CP三本身，
Okay. This is a little bit crazy, right, because I remember the CP three itself,

488
00:31:05,999 --> 00:31:08,499
参数数量已经非常庞大了，对吧？
the parameters is already pretty huge, right?

489
00:31:08,499 --> 00:31:13,619
但这里我们基本上又把它们增加了三倍，为了进行更新。
But here we are basically triple them again, in order to perform updates.

490
00:31:13,619 --> 00:31:16,060
是的，这基本上就是最优状态。
Yeah. That's basically the optimal state.

491
00:31:16,060 --> 00:31:20,040
我觉得在这一点上，你已经大致了解了分布情况
And I think at this point, you already understand kind of the distribution

492
00:31:20,040 --> 00:31:23,160
关于内存消耗的分布。
between the um memory consumption.

493
00:31:23,160 --> 00:31:29,299
相比于参数，参数本身已经很大了，我认为这个消耗甚至更大，对吧，
Um, compared to parameters, which is already large, I think, this one is even larger, right,

494
00:31:29,299 --> 00:31:31,680
因为它是参数的三倍。
because it's three times of the parameters.

495
00:31:31,680 --> 00:31:34,040
而且你还会有很多很多中间激活值，
And also, you got many many intermediate activations,

496
00:31:34,040 --> 00:31:38,720
它的大小取决于你的模型、隐藏维度等等。
and it could be large depending on how your model, hidden dimension is and whatever.

497
00:31:38,720 --> 00:31:48,119
很好。那我们接下来聊聊，我们已经讨论过大小了，对吧？
Cool. Okay. So next let's talk about we already talked about size, right?

498
00:31:48,119 --> 00:31:50,079
接下来我们要讨论生命周期。
Next, we are going to talk about lifetime.

499
00:31:50,079 --> 00:31:56,679
那么哪些张量是存活的，哪些张量不是，因为我们需要为这些存活的张量分配
So what tensors can be live and what tensors can be that tensors, because we want to allocate

500
00:31:56,679 --> 00:31:57,879
内存。
memory for those live tensors.

501
00:31:57,879 --> 00:32:03,160
好吗？所以现在我们基本上需要增强逆向图。
Okay? So now we need to basically enhance the inverse graph

502
00:32:03,160 --> 00:32:04,880
稍微增强一下，把它变成一个训练图。
a little bit to make it a training graph.

503
00:32:04,880 --> 00:32:06,839
好吗？训练图和逆向图的区别在于，
Okay? So the difference between training graph and

504
00:32:06,839 --> 00:32:09,140
逆向图基本上是我们有一个反向图，
inverse graph is basically we have a backward graph,

505
00:32:09,140 --> 00:32:12,640
对吧，同时我们还有一个优化状态更新图。
right, and we also have optimized state updating graph.

506
00:32:12,640 --> 00:32:14,519
好的，我们把这些全部结合在一起。
Okay, we combine all together.

507
00:32:14,519 --> 00:32:16,340
所以这里，嗯，呃，
So here, um, uh,

508
00:32:16,340 --> 00:32:19,720
我做了一些简化，把梯度更新图去掉了。
I simplify a little bit, I remove the grading updated graph.

509
00:32:19,720 --> 00:32:23,920
好的，我基本上把反向图映射到照片图上。
Okay. I basically map the backward graph to photograph.

510
00:32:23,920 --> 00:32:28,899
我们其实可以把这些合并成一行，就像这样。
And we can basically fold this together into one line that is like this.

511
00:32:28,899 --> 00:32:33,119
好的，我们把边折叠起来，前折、后折、前折、后折。
Okay, we fold the edges, forward, backward, forward, backward.

512
00:32:35,400 --> 00:32:44,779
嗯，那对于模型参数，我们可以随时描述它们吗？
Uh, so for model parameters, can we describe them at any time?

513
00:32:44,779 --> 00:32:50,719
不能，因为我们需要用它们来计算梯度或者应用更新，对吧？
No, Because we need them to either, calculate gradings or apply updates, right?

514
00:32:50,719 --> 00:32:53,059
所以大多数情况下，我们不能描述它们。
So mostly, we cannot describe them.

515
00:32:53,059 --> 00:32:56,039
那对于激活值呢，我们可以吗？
Um For activations, can we?

516
00:32:58,730 --> 00:33:02,529
在某种程度上可以。但在这个例子里，
To some extent can. But in this example,

517
00:33:02,529 --> 00:33:08,649
我举了一个我们不能描述的例子，因为和推理前向图相比，
I show example where we cannot because um compared to the inference forward graph,

518
00:33:08,649 --> 00:33:11,570
每次我们计算一层时，可以丢掉那一层之前的所有内容。
every time we calculate the layer, we can throw anything before that layer.

519
00:33:11,570 --> 00:33:17,270
但和那个例子相比，这里每次我们计算一层，前向传播后，
But compared to that example here, every time we calculate layer, after it's forward,

520
00:33:17,270 --> 00:33:20,609
我们不能立刻丢弃它，因为在某个时刻我们还会用到。
we cannot throw it away immediately because at some point we will come

521
00:33:20,609 --> 00:33:24,589
回来后我们又需要激活值来计算梯度。
back and we need that activation again to derive the greits.

522
00:33:24,589 --> 00:33:26,669
所以在大多数情况下，
So in most cases,

523
00:33:26,669 --> 00:33:28,770
我会说我们无法舍弃这些激活值。
I would say we cannot describe the activations.

524
00:33:28,770 --> 00:33:32,989
但稍后我们会讨论一些优化方法，这些方法允许我们
But later we're going to talk about some optimizations where we will be allowed to describe to

525
00:33:32,989 --> 00:33:35,770
查看内存，但需要付出一定的代价。
see memory, but pay some cost.

526
00:33:36,420 --> 00:33:41,299
是的，总结一下，基本上，因为我们需要保留中间值
Yeah. So to summarize, basically, uh, because the need to keep intermediate values

527
00:33:41,299 --> 00:33:43,299
用于反向传播的梯度计算。
around for the grading steps.

528
00:33:43,299 --> 00:33:46,779
所以训练早期的神经网络相比于推理阶段，
So training earlier neural network, compared to inference,

529
00:33:46,779 --> 00:33:51,639
现在我们基本上需要打开所有内容，因为我们要保留每一层的激活输出，
now we need to basically open because we need to keep the activations for every layers output,

530
00:33:51,639 --> 00:33:53,499
对吧，这样才能进行反向传播。
right, in order to perform backward.

531
00:33:53,499 --> 00:33:57,619
这就是训练和推理之间的区别。
So this is the difference between training and inference.

532
00:33:59,250 --> 00:34:05,630
好的，现在我们来总结一下TPD速率的案例。
Okay, now, let's do summarization for the TPD rate case.

533
00:34:05,630 --> 00:34:13,089
好的。对于参数来说，我们有1750亿乘以16位或32位。
Okay. So for parameters, we have 175 billion times either 16 bit or 32 bit.

534
00:34:13,089 --> 00:34:20,050
好的？对于激活值，嗯，呃，在transformer的边界上，基本上就像我刚才说的，有78G，
Okay? For activations, um, uh, at the transformer boundary we basically have, like I said, 78 giga,

535
00:34:20,050 --> 00:34:21,689
并且我们有96层。
and we have 96 layers.

536
00:34:21,689 --> 00:34:24,509
那基本上就是7K，
That is basically seven K,

537
00:34:24,509 --> 00:34:28,149
488G的激活值内存。
488 giga memory for activations.

538
00:34:28,149 --> 00:34:32,449
嗯，就像我说的，这个并不准确，因为transformer是一个复合层。
Um, like I said, this is not accurate because transformer is a composite layer.

539
00:34:32,449 --> 00:34:35,129
还有很多其他算子会消耗比这更多的内存。
There are many many other operators that still consume more memory than this.

540
00:34:35,129 --> 00:34:41,460
好的。对于优化器状态，就像我说的，我们有三份拷贝。一份是梯度。
Okay. And for optimal states, like I said, um, we have three copies. One is gradient.

541
00:34:41,460 --> 00:34:48,540
另一个是第一阶矩和第二阶矩，三倍的N，也就是175个二进制。
The other is first moment second moment, three times N, which is 175 binar.

542
00:34:48,540 --> 00:34:53,519
明白了吗？这里有一点你需要记住，就是当我们在
Okay? And here, one thing that you need to remember is, when we apply atom on

543
00:34:53,519 --> 00:34:58,700
任何类型的精度或模型上应用atom时，我们始终保持32位精度。
whatever kind of precisions or model, we always keep the precision in 32.

544
00:34:58,700 --> 00:35:04,059
明白吗？我稍后会再讲这个，因为，从高层次的直觉来说，当我们应用
Okay? I will come back to this later because, but high level intuition is when we apply

545
00:35:04,059 --> 00:35:07,999
这种操作时，我们希望有高精度，
this kind of obrand we want to have high precision in order

546
00:35:07,999 --> 00:35:10,740
以确保我们的优化是准确的。
to basically make sure our optimization is accurate.

547
00:35:10,740 --> 00:35:12,850
是的，我们不想损失精度。
Yeah, we don't want to lose precision.

548
00:35:12,850 --> 00:35:16,260
这意味着这里我们要乘以四个字节，
Which means that here, we have to time that by four bytes,

549
00:35:16,260 --> 00:35:20,079
这样我们又得到了12乘以175 GB。
and that gives us another 12 times 175 giga.

550
00:35:20,079 --> 00:35:24,859
这基本上就是我们需要多少内存来运行20GB三的整体情况。
So this is basically the global picture of how many memory we need to 20 GB three.

551
00:35:24,859 --> 00:35:26,559
好吗？你可以做一些计算。
Okay? You can do some math.

552
00:35:26,559 --> 00:35:29,299
之后，把这些数字加起来，然后除以80。
Afterwards, you add all this number together and divide it by 80.

553
00:35:29,299 --> 00:35:32,100
这就是我们至少需要的GB数，23GB。
That is the number of GB we need 23. At a minimum.

554
00:35:32,100 --> 00:35:36,059
好的。是的。因为我觉得至少你得先把模型
Okay. Yeah. Because I think at a minimum, you need to first put the model

555
00:35:36,059 --> 00:35:37,539
放进足够的内存里，对吧？
into that enough memory, right?

556
00:35:37,539 --> 00:35:39,459
是的，你不用关心需要多长时间。
Yeah, you don't care about how long it takes.

557
00:35:39,459 --> 00:35:41,640
至少你得提供那么多内存。
At least you need to offer that memory.

558
00:35:41,640 --> 00:35:43,059
好吗？酷。
Okay? Cool.

559
00:35:43,059 --> 00:35:46,399
有点疯狂，对吧？顺便说一下，这个模型是500B。
A little bit crazy, right? And this model is 500 B, by the way.

560
00:35:46,399 --> 00:35:49,799
是的，比如说，Deep S是500、600B。
Yeah, for example, deep S 500 600 B.

561
00:35:49,799 --> 00:35:52,124
所以基本上是这个的四倍。
So basically four times of this.

562
00:35:52,124 --> 00:35:58,569
很好。现在你基本上对内存中发生的事情有了一个整体的了解，对吧？
Cool. Okay, now, you basically have a global picture of what is going on in the memory, right?

563
00:35:58,569 --> 00:36:04,229
生命周期，嗯，如何存储数据以及三个部分，
Lifetime, um, how to store the data and three parts,

564
00:36:04,229 --> 00:36:08,429
嗯，参数、激活和开放内存状态。
um, parameter, activation and open memory states.

565
00:36:08,429 --> 00:36:14,389
接下来我们要讨论下一个系统问题，就是如何优化内存使用。
And we're going to proceed to our next system issue, right, how we can optimize the memory usage.

566
00:36:14,389 --> 00:36:20,589
好吗？这一部分我们将会讲到，这基本上是主要的
Okay? For this part, we are going to cover, and this is basically the main

567
00:36:20,589 --> 00:36:24,409
新兴学习系统的趋势，也就是如何把这个庞大的模型
trend emerging learning system that is how to basically put this gigantic model

568
00:36:24,409 --> 00:36:26,549
放到许多设备上，并确保它们能够协同工作。
many many devices and make sure they work.

569
00:36:26,549 --> 00:36:28,769
我将会讲两个部分。
I'm going to cover two parts.

570
00:36:28,769 --> 00:36:33,349
一个是我们如何进行单设备内存组织，当然还有，
One is how we do this kind of single device memory oganation and of course,

571
00:36:33,349 --> 00:36:35,610
单个设备有一个限制，因为你只有80GB的内存。
single device has a limit because you only have 80 giga.

572
00:36:35,610 --> 00:36:39,770
所以在某个时候，你需要开始把任务分配到很多很多设备上。
So at some point you know how to start distributing jobs across many, many devices.

573
00:36:39,770 --> 00:36:43,189
这基本上就引出了我们的第二部分，
So that will basically brings us to the second part with

574
00:36:43,189 --> 00:36:45,469
但我想稍后再谈这部分内容。
plazon but I want to talk about this part later.

575
00:36:45,469 --> 00:36:49,749
好的，那我们先来看一下如何在单个设备上优化内存。
Okay. So let's look at how we optimize memory on single device.

576
00:36:49,790 --> 00:36:54,830
那么，嗯，显然，嗯，在激活阶段，
So, um, apparently, um, uh, during the activation section,

577
00:36:54,830 --> 00:36:59,430
我其实已经稍微暗示过大家，关于激活，其实，
I already kind of hinted to you guys that, uh, for activation, uh, actually,

578
00:36:59,430 --> 00:37:01,690
激活的某些部分是可以被丢弃的。
some part of the activation can be discarded.

579
00:37:01,690 --> 00:37:03,770
好的，这里有一个例子。
Okay. And here is the example.

580
00:37:03,770 --> 00:37:11,550
在这个例子中，我想展示的其实是，嗯，记得，
So, uh in this example, what I tried to show is basically, um, uh, Remember,

581
00:37:11,550 --> 00:37:17,489
当我们先进行前向传播，然后再进行反向传播时，比如你看第二层，
when we first perform forward and then perform backward and say, if you look at the second layer,

582
00:37:17,489 --> 00:37:20,549
一旦我们完成前向传播，就是生成a的激活值。
once we perform forward, it's activation of the generate a.

583
00:37:20,549 --> 00:37:23,529
我们基本上可以把这个激活值传递到下一层，并尝试
We can basically propagate that activation to the next layer and try to

584
00:37:23,529 --> 00:37:30,189
计算剩下的各层，并思考它的生命周期，也就是这个激活值什么时候会再次被需要？
compute the rest of layers and think about is lifetime, o when will that activation be needed again?

585
00:37:33,160 --> 00:37:38,279
所以前向传播之后，第二层的激活值，
So basically following backward cation, the activation of the second layer,

586
00:37:38,279 --> 00:37:43,299
它被需要的时候，基本上就是你的反向传播一直回到那一层的时候。
when it is needed is basically your backward proceed all the way to that layer.

587
00:37:43,299 --> 00:37:46,939
所以当你的反向传播真正到达当前层时，你又需要它了。
So when your backward actually reaches the current layer, you need it again.

588
00:37:46,939 --> 00:37:50,979
这意味着我们有一个窗口可以丢弃那部分内存，对吧？
Which means that there's a window where we can discard that part of memory, right?

589
00:37:50,979 --> 00:37:52,839
而且我们不再需要它了。
And we don't need that.

590
00:37:52,839 --> 00:37:57,659
只要那个窗口过去了，当我们的反向传播再次到达那一层时，
As long as that window passed and when our backward proceeded to that layer again,

591
00:37:57,659 --> 00:38:00,444
我们基本上可以反映那部分内存，对吧。
we can basically reflect that part of memory, right.

592
00:38:00,444 --> 00:38:02,069
呃，为什么我们要这样做？
Uh, why we do this?

593
00:38:02,069 --> 00:38:05,650
因为我们可以看到内存。基本上在那个时间窗口内，
Because we can see memory. We can basically during that time window,

594
00:38:05,650 --> 00:38:10,730
只要我们能描述那段内存，我们就可以计算出更多空间来
as long as we can describe the memory, we can count more space to basically

595
00:38:10,730 --> 00:38:13,929
容纳其他传感器，比如说，是的。好的。
accommodate the other sensors, for example, yeah. Okay.

596
00:38:13,929 --> 00:38:19,830
所以这基本上让你有个概念，比如我们如何做单设备增强。
So this basically give you some idea, like, how we can do single device augmentation.

597
00:38:19,830 --> 00:38:26,389
这里的关键思想是，呃，对于很多层的激活来说，这些激活在
The key idea here is, uh, for many activations in layers, so the activity is not needed again

598
00:38:26,389 --> 00:38:28,449
后向传播到达那一层之前其实并不需要再次用到。
until the background pass comes to that layer.

599
00:38:28,449 --> 00:38:32,230
好的。所以这给了我们一个时间窗口，可以丢弃内存。
Okay. So that gives us a time window where we can discard memory.

600
00:38:32,230 --> 00:38:36,469
明白了吗？所以我们的想法是可以丢弃其中一部分。
Okay? So the idea is we can discard some of them.

601
00:38:36,469 --> 00:38:41,004
好的。我们会在需要的时候重新计算它们。
Okay. And we recompute them when it's needed again.

602
00:38:41,004 --> 00:38:45,459
对吧？因为，比如说，如果你看第二层，我们可以丢弃它，然后当
Right? Because, for example, if you look at the second layer, we can discard it and when

603
00:38:45,459 --> 00:38:49,089
反向传播到达那一层时，我们可以从头开始重新计算到那一层。
the backroa reaches that layer, we can recompute from the beginning all the way.

604
00:38:49,089 --> 00:38:52,820
这样就能再次得到一个数值。重新计算会给你完全相同的结果，
To get a value again. And the recomputation will give you exact same results,

605
00:38:52,820 --> 00:38:54,300
对吧，因为你有这个程序。
right, because you have the program.

606
00:38:54,300 --> 00:38:56,740
你有数据流图的结构。
You have the data flow graph structure.

607
00:38:56,740 --> 00:39:02,279
明白了吗？所以这个技巧基本上叫做，嗯，重新计算。
Okay? So this trick is basically called, um, recomputation.

608
00:39:02,279 --> 00:39:10,720
人们给它起了很多不同的名字，重新计算、记忆化、梯度检查点激活，
People give you many different kind of names, recompeton, mtization and grad checkpoint activation,

609
00:39:10,720 --> 00:39:13,439
检查点激活，各种各样的名字。
checkpoint activation, all kinds of names.

610
00:39:13,439 --> 00:39:18,060
但你基本明白这个意思，就是我们可以对某些层进行重新计算。
But you basically get the idea that is, um, we can recompute certain layers.

611
00:39:18,060 --> 00:39:22,859
我们可以使用浮点运算。我们可以用计算机来补充内存，如果我们的内存不够。明白吗？
We can use flops. We can use computers to treat for memory if we don't have enough memory. Okay?

612
00:39:22,859 --> 00:39:28,759
那我们来看一下这是怎么运作的。这里，我有一个新的网络，对吧。
So let's see how this works. So here, um, I have a new network, right.

613
00:39:28,759 --> 00:39:33,899
在我的基础版本中，我基本上会保留所有的内存，
In my vanina version, I will basically, preserve all the memory and

614
00:39:33,899 --> 00:39:37,079
以及每一层产生的所有中间张量。
all the intermediate tensors produced at every layer.

615
00:39:37,079 --> 00:39:39,900
所以在这里，如果一层的内存被保留了，
So here, if a layers memory is preserved,

616
00:39:39,900 --> 00:39:42,319
我基本上会用橙色来标记它。
I basically mark it with orage color.

617
00:39:42,319 --> 00:39:46,299
所以你还记得在我的基础版本里，所有的模块、所有的节点
So you'll still remember at my Vaina version, all the blocks, all the nodes are

618
00:39:46,299 --> 00:39:48,059
基本上都有一个颜色，对吧。
basically has a color right.

619
00:39:48,059 --> 00:39:50,260
但在这里，我会稍微做一些不同的处理。
But here, I will do it slightly differently.

620
00:39:50,260 --> 00:39:54,300
所以我要做的是，我还是会继续进行我的四步通信。
So what I'm going to do is, I still proceed with my four communication.

621
00:39:54,300 --> 00:40:00,159
但我做的是，我只保存那些带颜色节点的层的输出，
But what I do is I only save those layers output at those nodes with a color,

622
00:40:00,159 --> 00:40:03,000
然后我会丢弃所有其他层的输出。
and I discard all the other layers output.

623
00:40:03,000 --> 00:40:08,779
好的，所以在这一步零，其实我已经把内存减少了一半，对吧？
Okay. So here in the step zero, basically only, I already reduce the memory by half, right?

624
00:40:08,779 --> 00:40:12,039
我只保存了第二、第一和第六层。
I only store, the second, the first, and the sixth.

625
00:40:12,039 --> 00:40:15,084
好的，我丢弃了第一、第三和第五层。
Okay. I discarded the first, the third and fifth.

626
00:40:15,084 --> 00:40:20,389
好的，当我进行反向传播时，从最后一层到第一层，
Okay. What I do is, when I proceed with my backward pass, from the last layer to the first layer,

627
00:40:20,389 --> 00:40:24,430
嗯，如果我遇到一个中间张量被保留的层，
um, if I hit a layer where the intermediate tensor was preserved,

628
00:40:24,430 --> 00:40:26,570
我没问题，我只需要计算梯度。
I have no problem, I just calculate the readings.

629
00:40:26,570 --> 00:40:30,690
但如果我遇到一个激活张量被丢弃的层，
But if I hit a layer where it's activation tensor was discarded,

630
00:40:30,690 --> 00:40:36,389
我会尝试在前面的层中找到一个被保留的层
what I do is I try to find a layer in the I try to find the previous layer that is

631
00:40:36,389 --> 00:40:39,289
最接近缺失中间值的那一层。
closest to that layer that is missing intermediate value.

632
00:40:39,289 --> 00:40:43,769
然后我再次从那一层开始进行前向传播。
And I launch a forward pass again, starting from that layer.

633
00:40:43,769 --> 00:40:50,670
所以在第二步中，你可以看到，当我传播到倒数第二个张量时，
So in the second step one, you can see, when I propagated to the second to last tensor,

634
00:40:50,670 --> 00:40:54,419
最后一个节点，我发现我缺少中间值。
last note, I found that I'm missing inter medial value.

635
00:40:54,419 --> 00:41:00,960
所以我基本上会找到在那一层之前的一层，然后触发
So I basically find out the layer that is basically one layer before that layer and I triggered

636
00:41:00,960 --> 00:41:03,900
再次进行前向计算以获得它的输出。
the forward commutation again to get its output.

637
00:41:03,900 --> 00:41:05,139
然后，一旦我得到了输出，
And then once I get output,

638
00:41:05,139 --> 00:41:06,900
我基本上就进行了反向传播。
I basically drove the adiens.

639
00:41:06,900 --> 00:41:13,240
明白了吗？所以这个想法其实就是在神经网络的几个位置、几层做检查点。
Okay? So the idea is basically a checkpoint at a few positions, a few layers of the neural network.

640
00:41:13,240 --> 00:41:17,660
我只在这些检查点保存张量，其他所有的值都丢弃了。
And I only save the tensors at those checkpoints and I described all the other values.

641
00:41:17,660 --> 00:41:18,999
在反向传播过程中，
And during the backward pass,

642
00:41:18,999 --> 00:41:24,140
我总是尝试从Cloris检查点重新计算丢失的张量。
I already I always try to recompute the missing tensors from the Cloris checkpoint.

643
00:41:24,140 --> 00:41:28,059
好的，酷。
Okay. Cool.

644
00:41:28,059 --> 00:41:32,909
嗯，采用这种策略，我们基本上可以探索几种不同的方案，对吧？
Uh, with this strategy, we can basically explore a few alternatives, right?

645
00:41:32,909 --> 00:41:37,689
所以第一种方案基本上就是，我们什么都不丢弃，对吧？
So the first alternative is basically, we don't describe anything, right?

646
00:41:37,689 --> 00:41:42,790
这基本上就变成了Vania版本，也就是我们在每一层都保存所有的中间
That is basically reduced to the Vania version, that is, we save all the intermediate

647
00:41:42,790 --> 00:41:44,509
张量。
tensors at every layer.

648
00:41:44,509 --> 00:41:48,870
所以在这种情况下，反向传播时其实不需要额外的计算，
So in this case, we don't actually need any extra compute at the backward,

649
00:41:48,870 --> 00:41:52,009
但我们必须消耗大量的内存空间。
but we have to consume a lot of memory space.

650
00:41:52,009 --> 00:41:57,209
另一个极端就是，我们丢弃所有的中间张量，对吧？
Another extreme is basically, we discard all the intermediate tensors, right?

651
00:41:57,209 --> 00:41:59,150
我们什么都不保存，明白吗？
We don't save anything, okay?

652
00:41:59,150 --> 00:42:05,309
在反向传播时，我们会发现每一层的输出都会被丢弃，所以我们必须
And at backward pass, we'll find that every layers output will be discarded, so we have to

653
00:42:05,309 --> 00:42:07,629
重新计算，因为我们没有检查点，对吧？
recompute we don't have a checkpoint, right?

654
00:42:07,629 --> 00:42:10,969
所以我们不得不一遍又一遍地从输入重新计算。
So we have to recompute from the input again and again.

655
00:42:10,969 --> 00:42:15,049
明白了吗？在这种情况下，我们确实节省了大量内存，但是，
Okay? So in that case, we save a lot of memory, but,

656
00:42:15,049 --> 00:42:18,510
嗯，我们要付出很高的计算成本。
um, we are going to pay a lot of cost on compute.

657
00:42:18,510 --> 00:42:25,750
那么问题来了，我们能不能想出一种策略，
Then the question is, can we figure out, um, some strategy that is probably

658
00:42:25,750 --> 00:42:29,110
能够在计算和内存之间取得最佳平衡。
best strike a balance between compute and memory.

659
00:42:29,110 --> 00:42:32,430
那么，如何找到这样一种策略呢？
So so how to find a strategy.

660
00:42:32,430 --> 00:42:36,049
我们实际上可以把这个成本建模为两个部分，明白吗？
We can actually model this cost as two terms, okay?

661
00:42:36,049 --> 00:42:38,649
第一个术语基本上是检查点的成本。
The first term is basically the checkpoint cost.

662
00:42:38,649 --> 00:42:44,849
所以假设对于线性神经网络，我们每K层做一次检查点，对吧？
So assume that way for liner near network, okay, we are going to checkpoint every K layers, right?

663
00:42:44,849 --> 00:42:48,809
所以基本上检查点的成本就是总成本除以K，
So basically the checkpoint cost is basically all divided by K,

664
00:42:48,809 --> 00:42:51,809
对吧，因为我们需要检查这么多次。
right, because we need to check this many times.

665
00:42:51,809 --> 00:42:55,569
假设我们检查这么多次，那么在反向传播时，
Assuming we check this many times, then we also need to during the background pass,

666
00:42:55,569 --> 00:42:57,669
我们还需要额外支付计算成本，对吧？
we have to pay extra computing costs, right?

667
00:42:57,669 --> 00:43:00,129
就像我说的，额外的计算基本上就是我们找到
Like I said, the extra computing is basically like we find

668
00:43:00,129 --> 00:43:02,809
第K个检查点，然后继续向前计算。
the Case checkpoint and we proceed forward.

669
00:43:02,809 --> 00:43:05,814
明白了吗？所以基本上的计算成本就是这样。
Okay? So basically the computing cost is basically okay.

670
00:43:05,814 --> 00:43:12,259
好的。那么如果我们采用这种检查点策略，我们可以把R的成本写成
Okay. And so if we apply this kind of a checkpoint realgy, we can write R cost as

671
00:43:12,259 --> 00:43:16,259
这个求和项是O，然后除以K加o。
this sumation term which is O and divided by K plus o.

672
00:43:16,259 --> 00:43:19,259
当这个方程是最优的时候，基本上就是
And when this equation is optimal, it's basically when the

673
00:43:19,259 --> 00:43:20,819
第一项和第二项相等，对吧？
first term and second term is equal, right?

674
00:43:20,819 --> 00:43:25,440
这意味着对于一个接近的神经网络，
So which means that we basically need to for a near neural network,

675
00:43:25,440 --> 00:43:28,940
我们需要做的是每隔几层做一次检查点。
what we need to do is we checkpoint every square layers.

676
00:43:28,940 --> 00:43:32,140
这样基本上就能实现最优的检查点策略。
That basically gives us optimal checkpoint reality.

677
00:43:32,140 --> 00:43:34,299
好吗？那我有个问题。
Okay? And then I have a question.

678
00:43:34,299 --> 00:43:38,559
那在这种情况下，总的重新计算成本是多少？
So in this case, what is the total recommit cost?

679
00:43:48,320 --> 00:43:51,779
是的，O N。
Yeah. O N.

680
00:43:51,779 --> 00:43:55,599
那是什么？其实就是一次前向传播，对吧。
So what is? It's basically one forward pass, right.

681
00:43:55,599 --> 00:44:00,819
基本上，为了计算，嗯，为了完成前向传播，
So basically in order to compute the, um, in order to finish the forward,

682
00:44:00,819 --> 00:44:04,840
为了完成反向传播，你需要再执行一次前向传播。
in order to finish the backward, you need to perform one more forward.

683
00:44:04,840 --> 00:44:09,959
明白了吗？这意味着如果你按照这个策略去做部署训练，
Okay? Which means that if you follow this strategy and you go and do diploying training,

684
00:44:09,959 --> 00:44:12,379
你的部署计算会有一些变化。
uh, your deploying computation will change a little bit.

685
00:44:12,379 --> 00:44:15,255
之前，你的部署计算是一次前向，一次反向。
Previously, your deploying computing is one forward, one backward.

686
00:44:15,255 --> 00:44:19,090
对吧？如果你内存不够并且使用这个策略，
Right? And if you don't have enough memory and you use this strategy,

687
00:44:19,090 --> 00:44:21,510
它基本上就变成了一次前向，一次反向。
it basically becomes one forward, one backward.

688
00:44:21,510 --> 00:44:23,829
并且在反向传播时，你还要再做一次前向传播。
And during that backward, you pay one more forward.

689
00:44:23,829 --> 00:44:26,190
这样就变成了两次前向和一次反向。
It becomes two forward and backward.

690
00:44:26,190 --> 00:44:32,530
明白了吗？记住这一点。为什么？因为这对你估算GPU需求非常有帮助。
Okay? Remember this. Why? Because this is very helpful for you to estimate your uh, GPU addition.

691
00:44:32,530 --> 00:44:37,649
好吗？嗯。我想稍后再谈这个，但我们先记住这点，好吗？
Okay? Yeah. So, I want to talk about it later, but let's remember this, okay?

692
00:44:37,649 --> 00:44:42,249
基本上，如果你做这种方块式的检查点，
So basically, if you do this square uh, checkpointing,

693
00:44:42,249 --> 00:44:48,615
你基本上就是用计算来换取内存，然后进行两次前向一次反向。
you basically use computer to trade for memory and you perform two forward one backward.

694
00:44:48,615 --> 00:44:55,539
好的。如果我们把这种策略的效果可视化一下，你会看到，
Okay. And if we visualize this kind of, like, effectiveness of this strategy, you can see,

695
00:44:55,539 --> 00:45:01,139
如果我们不做检查点，随着我们的提交进行，内存会一直增长，
if we don't do checkpointing, as our commit goes, our memory will grow all the way

696
00:45:01,139 --> 00:45:03,959
然后变得很平稳，对吧？
and then become plain, right?

697
00:45:03,959 --> 00:45:08,579
但如果我们做这种检查点，我们基本上可以让这个曲线变得平缓一些。
But if we do this kind of checkpointing, we can basically flatter this scribe a little bit.

698
00:45:08,579 --> 00:45:12,960
我们仍然会消耗很多内存，原因是像我刚才说的，我们需要存储参数。
The reason we still consume a lot of memory because like I said, we need to store parameters.

699
00:45:12,960 --> 00:45:15,624
是的，我们需要存储梯度之类的东西。
Yeah, we need to store gradients, this kind of thing.

700
00:45:15,624 --> 00:45:22,790
实际上，这种检查点策略被非常广泛地采用。
Okay. And in practice, this, um, checkpointing strategy is very, very well adopted.

701
00:45:22,790 --> 00:45:28,170
我认为你可以很容易地从Pyroc找到一些现成的EPS。
And I think you can find some very readily available EPS from Pyroc.

702
00:45:28,170 --> 00:45:31,769
在piracy中，有一个叫torch uses checkpoint的EPI。
In piracy, there's a EPI called torch uses checkpoint.

703
00:45:31,769 --> 00:45:37,049
在一些非常著名的库中，比如deep speed和hug in accelerating，
And in some very famous library, for example, deep speed and hug in accelerating,

704
00:45:37,049 --> 00:45:39,709
你都可以找到这种激活检查点的方式。
you can find this kind of activation checkpointing.

705
00:45:39,709 --> 00:45:44,089
只要你打开这个功能，并且提供所需的几个参数，
And as long as you turn this on and you give it a few parameters needed,

706
00:45:44,089 --> 00:45:47,489
它基本上就会帮你实现这种策略。
it will basically help you implement this strategy.

707
00:45:47,489 --> 00:45:52,449
好的，我会在不同的位置做检查点来节省内存。
Okay, I will checkpoint at different places to save memory.

708
00:45:53,039 --> 00:46:01,519
好，有什么问题吗？很棒。激活检查点，这基本上就是我们
Okay. Any question here? Cool. Activation check point, that is basically, our

709
00:46:01,519 --> 00:46:04,799
第一个单设备内存优化策略。
first single device memory otenation strategy.

710
00:46:04,799 --> 00:46:07,279
那我们接下来再讨论一下，好吗？
Then let's discuss a little bit, okay?

711
00:46:07,279 --> 00:46:14,799
就像我说的，它也被不同的人叫做不同的名字。
So, like I said, it's also called it's also called by different people in different names.

712
00:46:14,799 --> 00:46:19,759
比如说，有人叫它重新计算、重新商品化，或者一些花哨的名字，明白吗。
For example, recompetition remerchanization, or fancy names, okay.

713
00:46:19,920 --> 00:46:24,239
我的第一个问题是，什么时候该开启它，什么时候不该开启它。
My first question when and when not enable it.

714
00:46:25,039 --> 00:46:32,419
当你有足够的内存时，也就是你的峰值内存小于可用内存时，
So when you have enough memory, that when your peak memory is smaller than the available memory,

715
00:46:32,419 --> 00:46:34,559
你应该这么做吗？不应该。
should you do this? No.

716
00:46:34,559 --> 00:46:37,054
因为这样会让推荐变慢，对吧。
Because it was slow down recommendation, right.

717
00:46:37,054 --> 00:46:42,169
是的，所以基本上，呃，除非你发现
Yeah. So basically, uh, don't turn this on until like basically you find that

718
00:46:42,169 --> 00:46:45,149
你的峰值内存大于可用内存，否则不要开启它。
your pick memory is greater than over memory.

719
00:46:45,149 --> 00:46:48,729
但在很多很多现在的深度学习库、深度学习代码中，
But in many many today deep learning libraries, deep learning code, they

720
00:46:48,729 --> 00:46:50,209
他们基本上默认就把这个功能打开了。
basically turn this on by default.

721
00:46:50,209 --> 00:46:55,829
所以我觉得在你后续的研究中，如果你发现了什么，希望你能把它补充进去，好吗？
So I think later in your research, if you find I hope you can inspell that, okay?

722
00:46:55,829 --> 00:47:00,829
因为就像我说的，如果这个默认是开启的，而你发现你的P内存实际上比borrary还小，
Because as I said, if this return on by default and you find that your P memory is actually smaller

723
00:47:00,829 --> 00:47:04,929
那么基本上你的训练程序会被减慢四分之三，
than borrary basically your training program will be slowed down by three fourths,

724
00:47:04,929 --> 00:47:09,069
对吧，因为你需要多进行一次前向计算，好吗？
right, because you pay one additional forward, okay?

725
00:47:10,300 --> 00:47:13,519
所以最优的检查点策略。
So the optimal checkpoint policy.

726
00:47:13,519 --> 00:47:19,119
我想我给你一个理论上的最优解，就是我们每隔一个平方数进行一次检查点，
I think I give you a theoretical optimal that is we checkpoint in every square,

727
00:47:19,119 --> 00:47:20,819
但实际上会更复杂，为什么呢？
but it's more complicated, why?

728
00:47:20,819 --> 00:47:25,359
因为在我的例子里，我假设每一层都是一样的，所有层都相同。
Because in my example, I assume every layer is equal. All the layers are the same.

729
00:47:25,359 --> 00:47:28,859
但在很多真实的情况、实际工作中，
But in many real cases, real works.

730
00:47:28,859 --> 00:47:32,509
我们需要找出最适合设置检查点的策略。
We need to figure out what is the best strategy to checkpoint at.

731
00:47:32,509 --> 00:47:37,159
嗯，原因是有些神经网络的结构非常异构。
Uh, the reason is because some neur networks, they have a very heterogeneous

732
00:47:37,159 --> 00:47:42,879
架构中的每一层都不同，每一层产生的激活值大小也会略有不同。
archecture R layer are different, and layer will produce a slightly different size of activation.

733
00:47:42,879 --> 00:47:47,439
所以你总是希望在激活值较小的位置做检查点，而不是较大的地方，对吧？
So you always want to checkpoint a place where the activation is small rather than large, right?

734
00:47:47,439 --> 00:47:49,799
因为这样做的效果是一样的。
Because if you because the effect will be the same.

735
00:47:49,799 --> 00:47:52,119
但是如果你在激活值很大的张量上做检查点，你就需要付出更多的内存。
But if you checkpoint a large distensor you have to pay more memory.

736
00:47:52,119 --> 00:47:57,299
明白了吗？所以到底在哪里做检查点，其实取决于神经网络的架构。
Okay? So where to checkpoint actually depends on the neural network archecture, okay.

737
00:47:57,299 --> 00:48:01,119
嗯，这也会影响到重新计算的成本，因为你希望在
Uh, I could also influence the recomputing cost because you want to checkpoint on

738
00:48:01,119 --> 00:48:06,094
可以很容易、成本很小地重新计算的位置做检查点，而不是成本很高的地方。
some place where you can easily recompute with very small cost instead of large cost. Okay.

739
00:48:06,094 --> 00:48:12,329
很酷。其实有不少研究专门在探讨
Cool. And there's a decent number of lines of research basically studying for

740
00:48:12,329 --> 00:48:13,810
针对不同类型神经网络的检查点策略。
different types of neural networks.

741
00:48:13,810 --> 00:48:15,330
我应该在哪里做检查点？
Where should I checkpoint?

742
00:48:15,330 --> 00:48:20,570
但一般来说，嗯，对于transformers或者语言模型，我们通常怎么做检查点？
But in general, um, for transformers for language models, what do we do basically checkpoint

743
00:48:20,570 --> 00:48:22,169
在transformer层的边界处做检查点。
at the transformer layer boundary.

744
00:48:22,169 --> 00:48:32,749
好的，好的。这种检查点激活策略的缺点基本上是
Okay. Okay. And the disadvantage of this checkpoint activation strategy is basically

745
00:48:32,749 --> 00:48:36,669
它只能减少激活所占用的内存，对吧？
it's only able to reduce the memory for activations, right?

746
00:48:36,669 --> 00:48:40,209
你没办法把这个方法应用到参数或者操作状态上。
There's no way that you can apply this two parameters or to operar states.

747
00:48:40,209 --> 00:48:42,369
好的，明白。
Okay. Cool.

748
00:48:42,369 --> 00:48:49,709
这里有问题吗？如果没有，我们继续讲Bomer优化策略。
Any question here? Okay. Then let's move forward to our Bomer optimization strategy.

749
00:48:49,709 --> 00:48:55,129
我们的第二个策略基本上叫做梯度累积，这也是
Okay. Our second strategy is basically called grading accumulation, and this is also a very

750
00:48:55,129 --> 00:48:57,829
当今框架中非常常用的一种策略。
well adopted strategy in today's frameworks.

751
00:48:57,829 --> 00:49:00,869
好吗？其实这个想法非常简单。
Okay? So the idea is actually pretty simple.

752
00:49:00,869 --> 00:49:04,649
我们发现无论你在图中使用什么算子，都是一样的。
So we find that no matter what operator use right in your graph.

753
00:49:04,649 --> 00:49:08,709
基本上，激活内存与批量大小是线性关系，对吧。
So basically the activity memory is linear to the batty size. Right.

754
00:49:08,709 --> 00:49:12,129
但是为了计算，比如说，我们的内存不够用，明白吗？
But in order to compute, say, we don't have enough memory, okay?

755
00:49:12,129 --> 00:49:14,409
因为我们的批量集很大，内存不够用。
Because our biset is large, we don't have en memory.

756
00:49:14,409 --> 00:49:17,209
但我们仍然想用给定的批量集继续计算。
But we still want to proceed our commutation with the given batsts.

757
00:49:17,209 --> 00:49:20,989
原因可能是因为批量集经过了超参数调优，
The reason is because probably the bisit was hyperparameter tuned so

758
00:49:20,989 --> 00:49:22,349
这样可以带来更好的结果。
that it can give you a better results.

759
00:49:22,349 --> 00:49:28,310
那么，为了实现这一点，我们的做法是，不是直接在给定的批量集上计算梯度，
Okay? So in order to do that, what we do is, um, instead of directly compute gradients

760
00:49:28,310 --> 00:49:34,649
而是基本上在循环中进行计算，明白吗？
on the given batsts what we do is basically computing in a loop, okay?

761
00:49:34,649 --> 00:49:38,509
所以现在我们要讲的是微批处理的概念。
So here it comes, uh into the concept of microbatch.

762
00:49:38,509 --> 00:49:41,709
好的，我们要做的基本上就是把原始批次分成很多，
Okay. What we do is basically we split the original batch into many,

763
00:49:41,709 --> 00:49:45,419
很多更小的批次，我们把每个小批次称为微批次。
many smaller batches, and we call each batch a micro baatch. Okay.

764
00:49:45,419 --> 00:49:48,040
我们要做的是遍历所有的微批次。
What do we do is we loop over all the micro baatges.

765
00:49:48,040 --> 00:49:51,089
每次我们只对一个宏批次做前向和反向传播。
A time we only do a foreign backward over the macro baatch.

766
00:49:51,089 --> 00:49:53,899
记住，激活只与批次大小线性相关。
Remember, activation is only linear to batch size.

767
00:49:53,899 --> 00:49:56,579
现在我们实际上把大的批次变成了
And now we effectively reduce large bases into

768
00:49:56,579 --> 00:49:59,959
微批次，我们有效地减少了激活所需的内存。
microbees we effectively reduce the activating memory.

769
00:49:59,959 --> 00:50:05,839
但我们要做的是，每次在微批次上计算梯度时，我们并不更新参数。
But what we do is every time when we calculate the gradient on the microbatch we do not update.

770
00:50:05,839 --> 00:50:08,399
明白吗？我们把它存储在内存里，我们就放在那里。
Okay? We put it in memory, we put it there.

771
00:50:08,399 --> 00:50:11,039
随着这个循环进行，我们不断累积。
We keep accumulating as this loop goes.

772
00:50:11,039 --> 00:50:14,879
最终，当我们完成原始批次中的所有微批次后，
And eventually, once we finish all the microbatch in the original batch,

773
00:50:14,879 --> 00:50:19,599
我们把所有的梯度累加在一起，然后进行更新。明白了吗？
we accumulate all the gradit together and we do update. Okay?

774
00:50:19,599 --> 00:50:23,019
现在就变成了，你要执行很多很多次前向传播，对吧？
Now, it becomes, you perform many, many, many forwards, right?

775
00:50:23,019 --> 00:50:29,074
你累积梯度，然后应用梯度更新，并不断重复这个过程，明白吗？
You accumulate the gradients, and then you apply grads and you keep doing this, okay?

776
00:50:29,074 --> 00:50:34,169
这是一种非常直接的减少激活内存的策略，
And this is a very straightforward strategy for reducing activity memory because

777
00:50:34,169 --> 00:50:38,269
因为你只是用更小的基础批次来计算，对吧？
you simply are calculating uh, using a smaller base sets, okay?

778
00:50:38,269 --> 00:50:41,589
但你依然可以达到使用更大批次的同样效果。
But you can still achieve the same results of using a larger bad set.

779
00:50:41,589 --> 00:50:52,809
好的，明白了。在petrog里实现这种梯度累积非常简单。
Okay? Cool. Here, it's very easy to implement this kind of grading accumulation in petrog.

780
00:50:52,809 --> 00:50:55,630
所以这里我给你两个示例程序。
So here I give you two example programs.

781
00:50:55,630 --> 00:50:57,949
所以在左边，基本上就是一段Petrich程序
So on the left hand side, it's basically a piece of

782
00:50:57,949 --> 00:51:04,349
你遍历数据加载器，获取一个批次，然后在神经网络中进行反向传播
Petrich program where you iterate over a data loader, you get a batch and you perform backwards

783
00:51:04,349 --> 00:51:08,789
并且执行一次最优的更新步骤。
through the neural network and you perform one optimal step.

784
00:51:08,789 --> 00:51:12,409
但在右边，你可以看到，唯一的区别是，
But in the right hand side, you can see, uh, the only difference, uh,

785
00:51:12,409 --> 00:51:16,109
我基本上把原始批次拆分成了微批次。
I basically split the original batch into microbatches.

786
00:51:16,109 --> 00:51:21,729
每次我都在微批次上计算梯度并进行累积，
And every time I drove the gradients on the microbatch and I accumulate them,

787
00:51:21,729 --> 00:51:27,519
只有当累积到一定数量的微批次时才进行更新。明白了吗？
and I only perform my updates when a certain number of microbatches are. Okay.

788
00:51:27,519 --> 00:51:31,119
我们可以很容易地实现它，并立即分布内存。
We are easy to implement it immediately distribute memory.

789
00:51:31,119 --> 00:51:35,299
很好，这部分有问题吗？
Cool. Any questions on this part?

790
00:51:35,500 --> 00:51:38,640
很好，这些都是非常实用的技巧。
Cool. These are very practical techniques.

791
00:51:38,640 --> 00:51:42,620
如果你遇到任何问题，随时可以尝试这个方法，这会非常有效。
If you fice any problem, feel free to try this, and this will be super effective.

792
00:51:42,620 --> 00:51:46,779
那么让我们继续在单一设备上进行最后的内存节省技巧。
Then let's proceed with our last memory saving technique on single device.

793
00:51:46,779 --> 00:51:50,239
最后的内存节省技巧，也是最强大的一个。
Last memory saving techniques, the most powerful one.

794
00:51:50,239 --> 00:51:55,579
所以记得，我们有一个内存层次结构，对吧，大多数深度学习计算
So si remember, we have a memory hierarchy, right uh, most of the dep running comton they

795
00:51:55,579 --> 00:51:59,279
基本上都在中间层，也就是TPHBM上运行。
basically operate on the middle layer, which is the TPHBM.

796
00:51:59,279 --> 00:52:05,740
但实际上我们拥有大量的内存，那就是CPRM，比如说，
But we actually are given a big amount of memory that is the CPRm And like for example,

797
00:52:05,740 --> 00:52:11,400
你可以比较一下，大多数AWS云实例，它们有超过一太字节的CPRM。
you can compare most of the AWS Cloud instances, they have more than one tibite of CPRM.

798
00:52:11,400 --> 00:52:16,559
那么问题来了，我们能否利用那部分内存来确保
So then the question is, can we actually leverage that part of memory to make sure

799
00:52:16,559 --> 00:52:21,519
即使我们的峰值内存大于TPHBM，我们的计算仍然可以继续？
our competition still prossed even if our peak memory is greater than the TPHBM?

800
00:52:21,519 --> 00:52:24,269
好的，答案是可以。那么怎么做到呢？
Okay. The answer is yes. So how do that.

801
00:52:24,269 --> 00:52:25,649
这其实很简单，对吧？
It's pretty straightforward, right?

802
00:52:25,649 --> 00:52:27,929
这是我在之前例子中展示的照片。
So here is the photograph

803
00:52:27,929 --> 00:52:30,189
我在之前的例子里展示过。
I showed in my previous example.

804
00:52:30,189 --> 00:52:33,069
我们进行前向传播和反向传播。
We perform forwards layer and the backwards layer.

805
00:52:33,069 --> 00:52:38,129
那如果在某一层，我们实际上无法训练怎么办？
So what if at some layer, we cannot actually, um, do training because

806
00:52:38,129 --> 00:52:40,049
因为峰值内存超过了HBM。
the pick memory is greater than HBM.

807
00:52:40,049 --> 00:52:45,229
那我们该怎么做呢？每次我们在一层上进行计算时，
So what do we do is, um, every time we perform the competition on one layer,

808
00:52:45,229 --> 00:52:47,989
我们基本上会产生输出，对吧？
okay, we basically produce the output, right?

809
00:52:47,989 --> 00:52:50,569
我们把输出传递给下一层。
We give the output to the next layer.

810
00:52:50,569 --> 00:52:57,129
同时，我们让程序基本上交换它的宽度和
And meanwhile, what we do is we ask the program to basically swap its width and

811
00:52:57,129 --> 00:53:01,050
将前一层的激活值从HBM传输到RAM。
activations of the previous layer from HBM to RAM.

812
00:53:01,050 --> 00:53:08,669
明白了吗？也就是说，我们把GM GPU内存中的内容复制到CPR RAM中。
Okay? That is, we make a copy of the contents from the GM GPU memory to the CPR RAM.

813
00:53:08,669 --> 00:53:13,769
基本上，这样可以描述GP内存中的任何内容，
And basically can basically describe anything on the GP memory,

814
00:53:13,769 --> 00:53:16,709
对吧，因为我们的CPRAM里有一份副本。
right, because we have a copy on our CPRAM.

815
00:53:16,709 --> 00:53:20,169
然后在某个时刻，当我们进行反向传播，还需要它们时，
And at some point, when we perform backward, when we still need them,

816
00:53:20,169 --> 00:53:25,489
我们要做的就是把它们从CPRM再交换回GPHBM。
what we do is basically we swap them back from CPRM to our GPHBM.

817
00:53:25,489 --> 00:53:29,789
好的，这基本上就形成了这样一种模式。
Okay. And this basically give us this kind of like a pattern.

818
00:53:29,789 --> 00:53:36,789
每次我们计算一层时，都会把内容复制到CPU内存，
Like every time we compute a layer, uh, we basically copy this content, uh, to CPU memory,

819
00:53:36,789 --> 00:53:40,849
然后通过TPP把它丢弃。
and we throw it through the TPP away.

820
00:53:40,849 --> 00:53:45,769
接着我们继续到下一层，不断地交换出去，交换出去，再交换出去。
And then we proceed to next layer, we swap out, we swap out and swap out.

821
00:53:45,769 --> 00:53:51,649
当我们开始进行反向传播时，每当需要上一层的内容，
And when we start doing backward what we do is whenever we need the content of the previous layer,

822
00:53:51,649 --> 00:53:55,090
我们就会把它从CP内存交换到GPU内存中。
we just swap it in from CP Ram to GPU memory.

823
00:53:55,090 --> 00:54:04,999
好的，明白了。所以这里的swap in和swap out基本上是两个非常原始的操作。
Okay. Cool. So here swapping swap out basically two very uh um primitive operators.

824
00:54:04,999 --> 00:54:06,719
swap in其实就是把数据从
So for swapping is basically swapped from

825
00:54:06,719 --> 00:54:12,439
CPU的DRAM交换到HBM，而swap out则是把数据从HBM交换回CPU的DRAM。
CPO uh DRAM to HBM, and for swap out is basically swapped from HBM to CPU DRAM.

826
00:54:12,439 --> 00:54:16,039
你可以看到这个方法其实非常强大，对吧？
Okay. So you can see this pretty powerful, right?

827
00:54:16,039 --> 00:54:22,959
它比checkpoint和微批处理都要强大得多，为什么呢？
It's much powerful than checkpoint and the basically micro baatching Why?

828
00:54:22,959 --> 00:54:25,459
因为理论上你可以交换任何东西。
Because theoretically you can swap anything.

829
00:54:25,459 --> 00:54:31,659
你可以交换中间张量，可以交换模型权重，也可以交换优化器状态。
You can swap intermediate tensors, you can swap model weights, you can also swap opts.

830
00:54:31,659 --> 00:54:33,799
只要你有CPU内存，你就可以交换任何东西。
As long as you have CPU memory, you can swap anything.

831
00:54:33,799 --> 00:54:37,779
你基本上可以把所有内容作为副本放在CPU内存中，然后为下一层的推荐在GPU上腾出一些空间。
And you can basically put everything as a copy in CPU memory and you make

832
00:54:37,779 --> 00:54:42,100
这是非常强大的。
some space on GPU for the next layer of commendation. This is pretty powerful.

833
00:54:42,100 --> 00:54:44,999
但问题是，这样做非常慢，对吧？
But the problem is that this is super slow, right?

834
00:54:44,999 --> 00:54:49,419
因为当你执行微型传递时，GPU非常快，
Because GPU is super fast when you perform like for micro pass,

835
00:54:49,419 --> 00:54:54,249
但交换基本上受限于带宽，对吧？
But swapping basically is limited by but it's band ways, right?

836
00:54:54,249 --> 00:54:57,809
所以假设你想在内存之间交换一些东西，这是需要时间的。
So suppose you want to swap something between memory ac, it takes time.

837
00:54:57,809 --> 00:55:06,119
那么在实际操作中会发生什么呢？让我拿一下我的笔。
So what happens when you do this in practice, what happens is basically let me get my pen.

838
00:55:06,119 --> 00:55:12,779
好的。所以，比如说，当你现在进行到这里，并完成反向推荐时，你想要
Okay. So say, when you now proceed here, and finish backcmmendation, you want to go

839
00:55:12,779 --> 00:55:15,639
回去，它基本上会在程序中指示，
back and it basically instructs programs in,

840
00:55:15,639 --> 00:55:21,359
我需要这一层的激活值，因为你采用了这种交换策略，
I need the activation of this layer, because you apply this swapping strategy,

841
00:55:21,359 --> 00:55:25,659
所以这些激活实际上是在CPU的内存上进行的。
so those activation actually was on CPU on CPU memory.

842
00:55:25,659 --> 00:55:30,979
你需要做的是等待它们基本上交换回来，然后你才能继续计算。
What you do is you have to wait for them to basically swap back and then you proceed the competon.

843
00:55:30,979 --> 00:55:36,439
问题是计算通常只需要几毫秒，但交换过程会花费
The thing is the computation usually only takes a few miniseconds, but the swapping will take

844
00:55:36,439 --> 00:55:39,159
数十甚至数百毫秒。
tens or even hundreds of miniseconds.

845
00:55:39,159 --> 00:55:44,159
所以你必须等待。理论上，你可以用这种策略来训练，
So you how to wait. Theoretically, you can use this strategy to train uh,

846
00:55:44,159 --> 00:55:51,680
22GB单个GPU只要内存足够大，但那样会花很长时间。
22 GB single GP was sufficiently large ram, but that will take forever.

847
00:55:51,760 --> 00:55:54,859
你可以稍微调整一下这个调度。
You can play around with this schedule litt.

848
00:55:54,859 --> 00:56:00,780
嗯，你可以调整一下这个调度。
Uh, you can play around this schedule it.

849
00:56:00,780 --> 00:56:07,039
也就是说，你尝试安排交换的顺序，比如说，
That is, um, you try to schedule the swapping and swap in a way that is, for example,

850
00:56:07,039 --> 00:56:09,099
当我在计算这一层的时候，
um, when I'm computing this layer,

851
00:56:09,099 --> 00:56:14,464
我知道在接下来的大约100毫秒内，我的计算就会到达这里。
I know that in the next probably 100 milliseconds, my commutation will reach here.

852
00:56:14,464 --> 00:56:18,149
我做的事情其实就是提前进行交换，对吧？
What I do is I basically swap in ahead of time, right?

853
00:56:18,149 --> 00:56:21,089
我在这里进行交换，每当计算已经到达这里的时候。
I swap in here, whenever computing is already here.

854
00:56:21,089 --> 00:56:23,589
所以当计算真正进行到这里时，
So when computing actually proceeded here,

855
00:56:23,589 --> 00:56:26,730
我已经有了目标DPU，所以我可以继续进行。
I already have the continent DPU, so I can proceed.

856
00:56:26,730 --> 00:56:31,249
但现实是，计算速度太快，而交换速度太慢，
But the reality is the computing is so fast that the swapping is so slow that

857
00:56:31,249 --> 00:56:36,409
你实际上无法获得这种可以重叠交换和计算的空间。对吧。
you actually don't get this kind of space to overlap swapping and competion. Yeah.

858
00:56:36,409 --> 00:56:42,649
明白吗？而且，即使你可以用这种方式来解决内存限制，
Okay? And also, because, at some point, even you can use this to address your memory limit,

859
00:56:42,649 --> 00:56:46,589
你仍然会受到计算限制，因为你的计算太慢，
you are still subject to computer constraints because your computing is too slow,

860
00:56:46,589 --> 00:56:50,669
就像你仍然希望加快训练速度，以便让模型达到预期效果。
like you still want to train fast to get your model right.

861
00:56:50,669 --> 00:56:53,689
好吗？这部分有问题吗？
Okay? Any questions on this part?

862
00:56:53,689 --> 00:57:01,309
有。在实际操作中，是的。
Yeah. In practice, yes,

863
00:57:01,309 --> 00:57:08,090
我觉得我的很多学生都用这个方法，因为他们可以等一会儿再拿到结果。
I think many of my students they use this because they can wait a little bit and to get the results.

864
00:57:08,090 --> 00:57:12,509
比如说，如果你资源有限，没有GPU，但你想尝试比如说
For example, if you have limited resources, you don't have GPUs, but you want to try say

865
00:57:12,509 --> 00:57:15,869
一个有300亿参数的Lama模型，你就必须这么做。
a Lama model with 30 billion parameters, you have to do this.

866
00:57:15,869 --> 00:57:27,039
为什么很难说，这取决于模型，但一般来说会慢两倍到十倍。是的。
Why It's hard to see it depends on model, but in general, two times to ten times slower. Yeah.

867
00:57:34,600 --> 00:57:39,539
是的，是的，我已经解释过你可以用流水线，但问题是，比如说，
Yeah, yeah, I already explained that you can use pipeline, but the problem is, for example,

868
00:57:39,539 --> 00:57:41,879
当你到达这部分时，你就没有流水线了，对吧？
when you reach this part, you don't have pipeline, right?

869
00:57:41,879 --> 00:57:45,099
是的，当你计算这一层时，你可以这样切换。
Yeah. Am when you compute this layer, you can swap it this way.

870
00:57:45,099 --> 00:57:49,159
但如果你正在计算这一层，那你就得等了。
But what if you are computing this layer, then yeah, you have to wait.

871
00:57:49,159 --> 00:57:55,560
是的。好吗？不，这种完美的流程其实很难实现。
Yeah. Okay? No, it's very hard to achieve this kind of perfect pipeline.

872
00:57:55,850 --> 00:58:01,689
很好。这其实是一个非常，嗯，呃，实用的现实，至少这个
Cool. This is a very, um, uh, practical reality that at least this

873
00:58:01,689 --> 00:58:04,070
让你可以在你的笔记本电脑上尝试一些大型模型。
allows you to try some big models on your laptop.

874
00:58:04,070 --> 00:58:09,229
好的。但在业界，这就是为什么这种方法不被业界所青睐，因为
Okay. But in industry that's why this matter was not preferred in industry because

875
00:58:09,229 --> 00:58:11,390
他们希望尽快得到结果。
they want to get results as soon as possible.

876
00:58:11,390 --> 00:58:13,330
但这种方法很受学生欢迎。
But this was preferred by students.

877
00:58:13,330 --> 00:58:15,109
好的，很多学生都喜欢这个。
Okay. A lot of students like this.

878
00:58:15,109 --> 00:58:16,730
很好，这就是我介绍它的原因。
Cool. That's why I introduced.

879
00:58:16,730 --> 00:58:19,690
很好。好的，总结一下，
Cool. Okay. To summarize,

880
00:58:19,690 --> 00:58:23,329
我认为我已经讲了三种非常重要的内存优化方法。
I think I covered three, very important memory optations

881
00:58:23,329 --> 00:58:25,249
第一个是梯度检查点，对吧？
the first one grading check pointing, right?

882
00:58:25,249 --> 00:58:27,589
第二个是梯度累积。
Second, gradient accumulation.

883
00:58:27,589 --> 00:58:31,570
第三个是CPO交换。
And the third one, CPO swapping.

884
00:58:31,570 --> 00:58:33,389
好的，我们来总结一下它们的优缺点。
Okay, let's summarize it pros and cons.

885
00:58:33,389 --> 00:58:37,709
好的。对于梯度检查点，这个很直接，对吧？
Okay. So for grading check pointing, um it's straightforward, right?

886
00:58:37,709 --> 00:58:41,090
这个方法挺好的，可以减少中间张量的数量。
It's pretty nice. It can reduce the intermediate tensors.

887
00:58:41,090 --> 00:58:45,009
但是缺点是你要付出更多的计算量，对吧？
But Cs is you paid flops, right?

888
00:58:45,009 --> 00:58:46,410
你的计算速度会变慢。
You slow down your computation.

889
00:58:46,410 --> 00:58:51,329
那么，关于梯度累积，这个看起来很完美，对吧？
Okay. Uh, for grading accumulation, it seems it's perfect, right?

890
00:58:51,329 --> 00:58:53,289
你不需要额外付出计算量，对吧？
You don't how to pay flops, right?

891
00:58:53,289 --> 00:58:56,230
那么有人能告诉我这个方法的缺点吗？
So can anyone tell me what's the drawback?

892
00:59:04,640 --> 00:59:07,930
那并不是主要原因。
That's not the primary reason.

893
00:59:07,930 --> 00:59:12,620
所以一个主要原因是，为了做到这一点，你必须进行拆分，
So one primary reason is in order to do this, you have to split,

894
00:59:12,620 --> 00:59:15,479
把你原本较大的数据分成更小的部分。
your original bad site into a smaller value.

895
00:59:15,479 --> 00:59:18,980
你还记得我们上节课做的选择题吗？
You remember the MCQ question we did last lecture.

896
00:59:18,980 --> 00:59:24,179
所以我们关心算术强度，对吧？因为我们想要启动足够大的内存，
So we care about arithmetic intensity right because we want to launch a big enough memo in order

897
00:59:24,179 --> 00:59:26,620
以便让我们的GPU超额分配任务。
to oversubscribe our GPUs.

898
00:59:26,620 --> 00:59:29,639
为了做到这一点，你基本上需要减小你的数据规模。
In order to do this, you have to basically reduce your body size.

899
00:59:29,639 --> 00:59:34,319
有时候当你减小数据规模时，你会遇到一些问题，比如在计算机端，
So sometimes when you reduce basis, you suffer from, like, cannot you cannot on the computer side,

900
00:59:34,319 --> 00:59:37,780
你无法充分利用GPU，因为你的数据规模变小了，明白吗？
you cannot sutGPs because your basis will reduce, okay?

901
00:59:37,780 --> 00:59:44,139
所以，嗯，在实际场景中，特别是在大模型训练时，你需要确保你有
So, um, in practical scenario, especially in large model training, you want to make sure you have

902
00:59:44,139 --> 00:59:48,039
足够大的伙伴单元，以确保你的GPU利用率很高。
a large enough buddy cells to make sure your GPU utilization is high.

903
00:59:48,039 --> 00:59:51,079
好的。有时候你可能并不想做这种事情。
Okay. Uh, sometimes you probably don't want to do this kind of thing.

904
00:59:51,079 --> 00:59:54,300
好的，但我们稍后会再谈这个问题，这很复杂。
Okay. But we'll come back to this later. It's very complicated.

905
00:59:54,300 --> 01:00:01,319
好的。当速度交换非常强大时，它可以减少参数、
Okay. And when speed swapping is very powerful, it can reduce the memory usage of both parameters,

906
01:00:01,319 --> 01:00:02,919
激活值和内存状态的内存使用量。
activations, and of memory stats.

907
01:00:02,919 --> 01:00:04,639
但问题是它非常慢。
But the problem is it's super slow.

908
01:00:04,639 --> 01:00:08,579
是的，嗯。
Yeah. Um.

909
01:00:08,670 --> 01:00:23,429
所以分批其实就是微批处理。
So grading is essentially microbatching.

910
01:00:31,710 --> 01:00:38,369
是的，因为有时候你想用更大的批量训练，因为收敛性会更好。
Yeah, because sometimes you want to train on bigger body size because the convergence property are

911
01:00:38,369 --> 01:00:40,829
使用不同的体积大小时，结果会不同。
different when you use different body size.

912
01:00:40,829 --> 01:00:44,389
这样说有道理吗？有道理。
Does that make sense? Yeah.

913
01:00:50,470 --> 01:00:56,110
不，因为我不会应用我的梯度，直到我回到原始基准。
No, because I don't apply my gradiens until I reach my original basis.

914
01:00:56,110 --> 01:00:58,389
那就不算了。对，对。
That's out of it. Yeah, yeah.

915
01:00:58,389 --> 01:01:01,370
好的。所以有时候我们只用一个固定的基准。
Yeah. Okay. So sometimes we just use a fixed basis.

916
01:01:01,370 --> 01:01:04,569
为什么？因为这可能和新兴的学习部分有关。
Why? Because this is relay to probably emerging learning part.

917
01:01:04,569 --> 01:01:10,409
我觉得我可以在后面的课程里讲这个，但一般来说，在部署训练任务时，是这样的。
So I think I can cover this in liter lectures, but in general in deploying training job, okay?

918
01:01:10,409 --> 01:01:16,330
如果你用很多节点和大量数据进行训练，你会想用更大的基准。
If you train with many many nodes with many many data prism, you want to use a larger basis.

919
01:01:16,330 --> 01:01:22,990
如果你用很少的节点和较少的数据进行训练，你会想用更小的基准。
And if you train with a small nodes with less data prism, you want to use a smaller basis.

920
01:01:22,990 --> 01:01:28,169
对，这就是我们发现能让你得到最佳模型的方法。
Okay. Yeah, that is something that we find that will give you the best model.

921
01:01:28,169 --> 01:01:33,069
好的，明白。关于CPU交换，有些事情正在发生变化。
Okay. Cool. And for the CPU swapping, something's changing.

922
01:01:33,069 --> 01:01:37,509
我觉得这值得一提。所以我认为媒体和其他一些芯片制造商，
I think it's worth mentioning. So I think media and a few other chipmakers,

923
01:01:37,509 --> 01:01:40,570
他们正在尝试让CPU和GPU之间实现统一内存。
they are trying to make some unified memory between CB and GPU.

924
01:01:40,570 --> 01:01:44,090
其实在你的MacBook上，这已经实现了。
So this is already happening on your MacBook.

925
01:01:44,090 --> 01:01:47,470
基本上，在MacBook上，你只有一种统一内存，
So basically in MacBook, you only have one type of memory that is unified memory,

926
01:01:47,470 --> 01:01:51,690
而CPU和GPU实际上都可以从统一内存中获取内容。
and the CPO and GPU can actually fetch contents from a unified memory.

927
01:01:51,690 --> 01:01:53,029
这意味着
That means that the barrier between

928
01:01:53,029 --> 01:01:55,290
CPU内存和GPU内存之间的障碍基本上正在消失。
CPU memory and DPP memory is basically diminishing.

929
01:01:55,290 --> 01:02:00,229
我相信在五到十年内，CPU和GPU很可能会共享同一块内存。
I believe in like five to ten years, the CPU and GPU, you know, probably will share the same memory.

930
01:02:00,229 --> 01:02:03,529
那时候CPU交换就会变得非常非常高效，对吧？
Then CPU swapping will become very, very, like effective, right?

931
01:02:03,529 --> 01:02:08,530
所以当你试图分析不同技术的优缺点时，
So when you try to analyze the pros and cons of the different techniques,

932
01:02:08,530 --> 01:02:12,689
你可能需要考虑它们的假设条件。
you probably want to consider the assumptions.

933
01:02:12,689 --> 01:02:15,590
这里的假设基本上是我们仍然有一个内存层次结构，
Here the assumption is basically we still have a memory hierarchy,

934
01:02:15,590 --> 01:02:17,590
但我认为这个内存层次结构正在瓦解。
but I think this memory hiarchy is breaking.

935
01:02:17,590 --> 01:02:25,149
很好。好吧，我觉得我们已经完成了今天内容的大部分，
Cool. Okay, I think we finish most part of today's content,

936
01:02:25,149 --> 01:02:28,950
让我们继续进入下一个重要话题，呃，量化。
and let's move forward to the next big topic, uh, uh, quantization.

937
01:02:28,950 --> 01:02:30,530
为什么我们要关心量化？
Why we care about qtation?

938
01:02:30,530 --> 01:02:35,769
呃，我认为我们关心它是因为量化可以有效减少内存占用。
Uh, I think we care about it because quantization can effectively reduce memory.

939
01:02:35,769 --> 01:02:38,910
请记住，无论我们存储参数，记住，
Remember, no matter we store the parameters, remember,

940
01:02:38,910 --> 01:02:43,599
无论我们存储激活值还是对手状态，我们总是把大小乘以，
no matter wetrae the activations or the oponente states, we always multiply the size by,

941
01:02:43,599 --> 01:02:45,509
元素的大小，对吧？
uh, size of elements, right?

942
01:02:45,509 --> 01:02:48,209
元素的大小基本上就是我们存储数据的方式。
And the size of elements is basically how we store the data.

943
01:02:48,209 --> 01:02:50,569
它可以是两个字节、四个字节或者其他。
It could be two bytes, four bytes or whatever.

944
01:02:50,569 --> 01:02:53,430
我们其实可以调整元素的大小。
And we can actually play around with the size of elements.

945
01:02:53,430 --> 01:02:57,350
比如说，如果我们能够用更低的精度来存储数值，
For example, if we are able to store value using a lower precision,

946
01:02:57,350 --> 01:02:59,849
我们就能有效地减少内存的使用，对吧？
we can effectively reduce the memory use, right?

947
01:02:59,849 --> 01:03:05,089
所以，量化基本上就是我们试图减小元素的大小。
So, the quantitation basically we try to reduce the size of element.

948
01:03:05,089 --> 01:03:07,030
好吗？那什么是量化呢？
Okay? So what is quantitation?

949
01:03:07,030 --> 01:03:09,269
其实很容易理解量化，对吧？
So it's very easy to understand quantitation, right?

950
01:03:09,269 --> 01:03:17,489
量化基本上就是把输入从一个连续或很大的集合限制到较小集合的过程。
So anation is basically the process of constraining input from a continuous or otherwise large set

951
01:03:17,489 --> 01:03:19,929
将数值离散化。好的。
of values to a discrete. Okay.

952
01:03:19,929 --> 01:03:23,229
我认为这两张图就是完美的例子。
And I think these two pictures give the perfect example.

953
01:03:23,229 --> 01:03:26,630
左边这张图，我们有一个连续的信号。
So on the left hand side, what we do is we have a continuous signal.

954
01:03:26,630 --> 01:03:31,129
但假设存储这个信号非常昂贵，
But suppose that it is very expensive to store the signal,

955
01:03:31,129 --> 01:03:35,350
我们想要做的是只用几个离散值来存储它。
What we want to do is we want to store it though using only a few discrete values.

956
01:03:35,350 --> 01:03:39,349
我们的做法基本上是在这里找到几个不同的离散值。
The way we do is basically here we find a few different discrete values.

957
01:03:39,349 --> 01:03:42,889
我们尽量去逼近原始信号，对吧？
We try to approximate the original signal as much as possible, right?

958
01:03:42,889 --> 01:03:47,089
好的。右边这张图基本上就是我们如何对图像进行量化，对吧？
Okay. And on the right hand side is basically how we quantat images, right?

959
01:03:47,089 --> 01:03:50,170
我们可以用更高的精度，这样我们就能看到更清晰的图像。
We can either use a higher precision so we see a clear image.

960
01:03:50,170 --> 01:03:52,550
我们也可以对这些数值进行量化。
Okay? We can also utaest the values.

961
01:03:52,550 --> 01:03:57,619
我们限制用于存储像素的比特数，然后我们得到一张略微模糊的图像。
We constrain the number bits we use to storage pixel, then we get a slightly blurred image.

962
01:03:57,619 --> 01:04:02,709
好的。第二个例子实际上会让你完全明白为什么这样做有效。
Okay. And the second example will actually give you a perfect sense why this works

963
01:04:02,709 --> 01:04:06,610
因为请记住，在大多数图像处理程序中，
emerginary Because remember, in most immaginary programs,

964
01:04:06,610 --> 01:04:08,770
我们的目标是，比如说，我们试图对图像进行分类。
our goal is, for example, we try to classify the image.

965
01:04:08,770 --> 01:04:11,389
我们试图判断这张图片实际上是一只猫。
We try to tell the image is actually a cat.

966
01:04:11,389 --> 01:04:16,709
所以在这里，无论我们用多少比特，至少对你来说，你都能看出这两张图片
So here, no matter how many bits we throw at least for you, you can tell both images you

967
01:04:16,709 --> 01:04:18,329
你都会说它们是一只猫，对吧。
will tell they are a cat, right.

968
01:04:18,329 --> 01:04:22,729
所以只要我们的量化可以降低精度，并且只要它
So as long as our quantization can reduce the precision, and as long as it does

969
01:04:22,729 --> 01:04:24,389
不影响模型的判断，
not influence the decision of the model,

970
01:04:24,389 --> 01:04:25,069
我认为这样就可以了。
I think we are good.

971
01:04:25,069 --> 01:04:27,289
我们基本上节省了很多资源，对吧？
We basically save a lot of resources, right?

972
01:04:27,289 --> 01:04:29,289
这就是为什么量化在高层次上有效。
That's why quantiton works at a high level.

973
01:04:29,289 --> 01:04:31,169
好的。但在接下来的几节课中，
Okay. But in the next few lecture,

974
01:04:31,169 --> 01:04:36,019
我会更深入地讲解我们是如何让这种量化在情感上也能起作用的。
I'm going to dive deeper to tell you how we make this quantity work emotionally.

975
01:04:36,019 --> 01:04:38,870
好的，那么情感上是怎么实现的呢？
Okay, so how new emersonaly.

976
01:04:38,870 --> 01:04:44,130
在高层次上，我们的目标就是用更低精度的数据重复来存储，
At a high level, our goal is right to use a lower precision repetition of data to store,

977
01:04:44,130 --> 01:04:48,989
就像我们提到的所有内容，比如模型参数、激活值等等。
like all the things we mentioned, uh, model parameters activations of the United States.

978
01:04:48,989 --> 01:04:50,990
但我们也受到一些限制。
But we are subject to a few constraints.

979
01:04:50,990 --> 01:04:53,410
第一个限制是我们要保持模型的性能不变。
The first one is we want to preserve emergening performance.

980
01:04:53,410 --> 01:04:58,309
也就是说，当我们对猫进行量化时，我们仍然希望模型能识别出猫，对吧？
That is, when we quantize the cat, we still want the model to recognize a cat, right?

981
01:04:58,309 --> 01:05:00,650
我们希望加速计算。
And we want to accelerate the compute.

982
01:05:00,650 --> 01:05:05,190
为什么？因为现在的团队，他们用这种低精度的方式，
Why? Because, uh, today's teamer, they are making this kind of lower presion

983
01:05:05,190 --> 01:05:08,149
比高精度的计算要快得多，对吧？
coming much faster than higher precision, okay?

984
01:05:08,149 --> 01:05:10,370
当然，我们也想减少内存的使用。
Of course, we want to reduce memory.

985
01:05:10,370 --> 01:05:12,690
如果你了解计算机体系结构，你可能已经知道，
And you probably know if you know computer architecture,

986
01:05:12,690 --> 01:05:17,190
如果我们用低精度来做矩阵运算、加法和乘法，
if we use a lower precision to do metamo to do this in addition to do multiplication,

987
01:05:17,190 --> 01:05:18,490
其实可以节省能量。
it actually saves your energy.

988
01:05:18,490 --> 01:05:24,579
对，没错。很棒。那么受到启发，我们接下来要讨论几个问题。
Yeah. Okay. Cool. So inspired, we are going to talk about a few things.

989
01:05:24,579 --> 01:05:27,859
我们先从如何表示数据开始。
We start with how to represent data.

990
01:05:27,859 --> 01:05:32,439
我想大家可能已经在某些计算机或者计算机体系结构课程里学过这个了。
I think many we probably already studied this in some computer or computer architecture course,

991
01:05:32,439 --> 01:05:33,939
但我们会再次回顾一下。
but we are going to revisit again.

992
01:05:33,939 --> 01:05:36,679
然后我们将讨论密码子的基础知识。
And then we are going to talk about the basics of codon.

993
01:05:36,679 --> 01:05:38,659
接着我们会把这些内容放到
And then we are going to put that into context of

994
01:05:38,659 --> 01:05:43,440
合并的背景下，并且我们会讨论后训练条件，这是一项非常重要的技术。
margining and we'll talk about post training condition, which is a very important technique

995
01:05:43,440 --> 01:05:48,439
因为我认为只要你的手机在运行某些机器学习模型，很可能实际上就在使用
because I think as long as your phone is running some machinery models, likely it's actually using

996
01:05:48,439 --> 01:05:50,200
这里的一些技术，也就是后训练条件。
some technique here, post training condition.

997
01:05:50,200 --> 01:05:54,259
好的，然后我们会讨论内容感知训练。
Okay. And then we are going to talk about content aware training.

998
01:05:54,259 --> 01:05:57,880
最后，假设我们都理解了这些，我们会讨论混合表示
And finally, assuming we understand all this, we are going to talk about mixed present

999
01:05:57,880 --> 01:06:04,740
训练。混合表示训练基本上是训练语言模型的关键技术。
training mixed present training is basically the key technique for training language models.

1000
01:06:04,740 --> 01:06:09,619
很好，希望我们能覆盖第一部分内容。
Cool. Hopefully, we can cover the first part.

1001
01:06:09,619 --> 01:06:12,739
如果不是的话，我们先来谈谈数据的表示方式。
If not, let's first talk about representi of data.

1002
01:06:12,739 --> 01:06:15,380
数据在计算机中是如何表示的？
How data was represented in computer?

1003
01:06:15,500 --> 01:06:23,039
基本上是用比特，对吧？所以我们用比特来表示数据，实际上我们有一个标准，
Basically bits, right? So we rep that in bits, and we actually have a standard at standard and

1004
01:06:23,039 --> 01:06:28,159
所有的计算机制造商基本上都试图遵循这个标准来存储数据，
all the computer makers they basically try to follow that standard and try to store data

1005
01:06:28,159 --> 01:06:30,180
遵循那些预定义的标准。
following those predefined standards.

1006
01:06:30,180 --> 01:06:35,199
那么我们先从整数说起，我们有无符号整数，对吧？
So let's talk start with integers, we have unsigned integers, right?

1007
01:06:35,199 --> 01:06:41,619
所以我们基本上，比如说，用这种方式用比特来存储无符号整数，对吧？
So we basically, for example, use this kind of one to a Bs to store unsigned integers, right?

1008
01:06:41,619 --> 01:06:47,499
这里，比特范围基本上是从零一直到二的N次方减一。
So here, the BD range is basically starting from zero all the way to two to N minus one.

1009
01:06:47,499 --> 01:06:50,840
这是无符号或正整数。
This is unsigned or positive integers.

1010
01:06:50,840 --> 01:06:56,219
好的，为了表示负整数，我们通常会这样做，嗯，
Okay? In order to basically represent negative integers, what we do is, um,

1011
01:06:56,219 --> 01:07:01,719
我们稍微修改一下比特流，把第一个比特分配为符号位，对吧？
we modify the bit stream a little bit and we assign the first bid as a sign bit, right?

1012
01:07:01,719 --> 01:07:06,980
所以如果第一个比特是1，那就是正数，否则就是负数。
So if the first bid one is basically positive, otherwise, uh, it's negative.

1013
01:07:06,980 --> 01:07:11,579
明白了吗？我们做的事情其实就是把剩下的n位分配来
Okay? Uh what we do is basically we allocate the rest of sem bits to

1014
01:07:11,579 --> 01:07:17,120
表示数值，这样我们就可以同时表示负数和正数了。
represent the values and in this way, we can represent both uh negative and positive values.

1015
01:07:17,120 --> 01:07:27,039
好的，这种符号表示法有一个问题，我们发现，比如说全是零和第一个是1，
Okay. And one problem of this uh sign Miverepen is, we find that, um, like both the arrow and one,

1016
01:07:27,039 --> 01:07:29,939
第一个比特是1，其余全是0，
the first bit being one and the rest being zero,

1017
01:07:29,939 --> 01:07:32,279
它们都表示同一个数值，就是零。
they are representing the same value, which is zero.

1018
01:07:32,279 --> 01:07:34,700
因为零既可以是正数也可以是负数。
Because zero is either positive or negative.

1019
01:07:34,700 --> 01:07:38,839
这其实就造成了重复。
And this basically was like one repetition.

1020
01:07:38,839 --> 01:07:43,499
我觉得计算机体系结构在这方面非常……
And I think, computer architectures, they are very, um,

1021
01:07:43,499 --> 01:07:45,619
他们在这方面非常苛刻。我对此没有任何意见。
they are very mean on this. I don't with anything.

1022
01:07:45,619 --> 01:07:51,940
好吗？所以我们实际上使用了一种非常，呃，不同的格式，叫做补码重复。
Okay? So what we do is we actually use a very, uh a different kind of formats compliment repetition.

1023
01:07:51,940 --> 01:07:55,420
好吗？我想这就是你们在某些计算机体系结构课程中学到的内容。
Okay? I think that's why you learn that in some computer archiecture

1024
01:07:55,420 --> 01:08:01,989
课程中不是指定相同的位，而是我们基本上，呃，嗯，
course that instead of designate same beat, what we do is basically, Uh, um,

1025
01:08:01,989 --> 01:08:08,870
我们用第一位作为，呃，最大的负值。
we use the first bit as, uh, the largest value of negative.

1026
01:08:08,870 --> 01:08:13,929
然后我们基本上，呃，把剩下的位累加起来，
And then we basically, uh, for the rest of bits we basically accumulate them together and

1027
01:08:13,929 --> 01:08:19,349
然后把它加到，比如这个例子里，是减27，好吗？
then add that to the in this example, it's minus 27, okay?

1028
01:08:19,349 --> 01:08:21,949
这样基本上就给你一个二进制补码的表示。
And this basically give you a orenon of the tooth complement.

1029
01:08:21,949 --> 01:08:27,709
好，有了这个表示方法，我们就不需要等待，呃，一个码，对吧？
Okay. And with this orientation, we don't have to wait, uh, one code, right?

1030
01:08:27,709 --> 01:08:31,209
好的，我希望你们对这个都很熟悉。
Okay. I hope you guys are familiar with this, okay.

1031
01:08:31,209 --> 01:08:38,649
哦。好的，整数在机器学习中很重要，因为有时候我们需要将
Ooh. Okay, integer is important in machine learning because sometimes we want to quantize

1032
01:08:38,649 --> 01:08:43,389
步进宽度量化为整数，比如4位整数、8位整数，这样也能正常工作。
the marching width into integer four integer eight and it still works.

1033
01:08:43,389 --> 01:08:48,409
但我认为我们更关心的是浮点数，因为大多数运算
But I think there's one more we care more about floating points because most of the companion still

1034
01:08:48,409 --> 01:08:50,670
仍然是在浮点数表示下进行的。
are performed on floating point repetitions.

1035
01:08:50,670 --> 01:08:54,209
所以在我讲浮点数之前，我们先来说说定点数。
So before I talk about floating point, we have a fixed point.

1036
01:08:54,209 --> 01:08:56,369
定点数其实很简单。
So fixed point is pretty simple.

1037
01:08:56,369 --> 01:09:01,669
基本上，我们就是在比特中间放一个小数点。
Like we basically, put a decimal point at the middle of the bits.

1038
01:09:01,669 --> 01:09:05,489
在这个小数点的左边就是整数部分。
And to the left of this decimal point is basically integer part.

1039
01:09:05,489 --> 01:09:08,219
右边就是小数部分。
To the right is basically the fraction part.

1040
01:09:08,219 --> 01:09:11,769
好的。按照这种定点数的组织方式，我们可以
Okay. And following this kind of fixed point orgenation we can

1041
01:09:11,769 --> 01:09:18,809
基本上这里表示一些浮点数，嗯，呃，跟着这个来。
basically represent some flowing point numbers here, um, uh, following flowing this one.

1042
01:09:18,809 --> 01:09:21,349
我猜你们对这个已经很熟悉了，对吧？
And I assume you guys are familiar with this one, right?

1043
01:09:21,349 --> 01:09:26,109
好的。但这不是我们在机器学习中用的那个，对吗？
Okay. But this is not the one we used in, uh, machine learning, right?

1044
01:09:26,109 --> 01:09:29,369
定点数主要用于安全领域。
Fixed point is mostly used in, uh, security.

1045
01:09:29,369 --> 01:09:34,609
好的？但在机器学习中，我们不用定点数，因为我们想要
Okay? But in machine learning, we don't use fixed point because we want to basically represent

1046
01:09:34,609 --> 01:09:38,549
用一种叫做浮点数的新格式来表示数字，好吗？
lumber using a new format called floating point, okay?

1047
01:09:38,549 --> 01:09:42,849
那么有人能告诉我浮点数和定点数有什么区别吗？
So anyone can tell me what's difference between flowing point and fixed point.

1048
01:09:45,350 --> 01:09:48,729
好的，我想你们可以从我的名字看出来，浮点数就是
Okay, I think you can tell from my name that the floating point is

1049
01:09:48,729 --> 01:09:51,309
小数点可以从左到右移动。
that the decimal point can float from left to right.

1050
01:09:51,309 --> 01:09:53,789
好的。让我们更深入地探讨一下这个问题。
Okay. Let's dive deeper into this.

1051
01:09:53,789 --> 01:09:59,209
好吗？这个是，呃，三重标准浮点0.32，好吗？
Okay? This is, uh, triple standard floating 0.32, okay?

1052
01:09:59,209 --> 01:10:00,949
我们基本上得到32位。
We basically get 32 bits.

1053
01:10:00,949 --> 01:10:04,249
嗯，呃，第一位是符号位，好吗？
Um, uh, the first bit being s bit, okay?

1054
01:10:04,249 --> 01:10:08,929
蓝色的那些位，我们称之为指数位，好吗？
And the blue kind of bits, we call it expolent, okay?

1055
01:10:08,929 --> 01:10:16,209
嗯，黄色的那些位我们分配了23位，我们称之为小数部分。
Um, and the yellowish kind of bits we have we allocate 23 bit we call it fraction.

1056
01:10:16,209 --> 01:10:22,369
好的，呃，这个和定点数的关键区别是，呃，我们不会
Okay. Uh, so the key difference between this one and the fixed point is that, um, uh, we don't

1057
01:10:22,369 --> 01:10:24,489
实际上把小数点放在任何地方，对吧？
actually put the decimal point anywhere, right?

1058
01:10:24,489 --> 01:10:28,969
所以我们表示浮点数的方式是用这个公式。
So the way that we represent the fol numbers is folly equation.

1059
01:10:28,969 --> 01:10:31,969
好吗？呃，我们首先确定符号，对吧？
Okay? Uh, we first figure out the sign, right?

1060
01:10:31,969 --> 01:10:36,909
然后我们用一加上小数部分p，然后用蓝色部分来确定我们如何
And then we use one plus fraction p, and then we use the blue part to figure out how we

1061
01:10:36,909 --> 01:10:39,749
应该从左到右流向小数点，对吧？
should flow to the decimal point from left to right, right?

1062
01:10:39,749 --> 01:10:45,989
这就是我们计时数值的方法，也就是解释负一、二、七。
And that is how we time value, which is to explain minus one, two, seven.

1063
01:10:45,989 --> 01:10:54,309
你可以把127理解为，呃，就是我们表示指数部分的方式，好吗？
And you can understand 127 as, um, uh, the way that we represent the exponent part, okay,

1064
01:10:54,309 --> 01:10:59,249
基本上就是最大值的一半，也就是我们可以用所有指数位表示的最大值的一半。
I basically the mid value of the largest value, the half of the value of the largest value we can

1065
01:10:59,249 --> 01:11:01,209
好的？因为我们希望小数点可以向左或向右移动，所以我们要减去一，呃，二十七。
represent using all the bits of the expoonent.

1066
01:11:01,209 --> 01:11:05,529
好的。到这里我们还没问题。
Okay? Because we want to float in the uh, we want to float to the decimal point

1067
01:11:05,529 --> 01:11:10,389
很好。那举个例子，我们下面其实有一个例子
either to left or to right, so we have to minus one, uh 27.

1068
01:11:11,100 --> 01:11:16,019
用这个浮点数标准来表示，
Okay. We're still good with this one.

1069
01:11:16,019 --> 01:11:23,099
就是在下面我们用这个浮点数标准来表示，
Cool. So to give you an example, we basically have one example below right

1070
01:11:23,099 --> 01:11:27,779

where we use this floating point standard to represent,

1071
01:11:27,779 --> 01:11:32,179
0.26，五，六，25。
0.26, five, six, 25.

1072
01:11:32,179 --> 01:11:39,059
好的。呃，为了理解这个，我们基本上可以用二进制表示法来匹配这个数字
Okay. Uh, in order to understand that we can basically match this number using binary repentation

1073
01:11:39,059 --> 01:11:42,899
并按照这里的表示公式来做。
and following that repent equation here.

1074
01:11:42,899 --> 01:11:47,539
如果我们进行某种变换，我们会发现它基本上是
And if we perform some kind of like a transformation, we find that it's basically

1075
01:11:47,539 --> 01:11:50,849
一加上小数部分，也就是0.0
one plus of fracking part which is 0.0

1076
01:11:50,849 --> 01:11:58,339
0.0625乘以2的125次方减去127？
0.0 625 times two to 125 minus 127?

1077
01:11:58,339 --> 01:12:02,579
好的。我们基本上可以把这个数字映射到指数部分和小数部分
Okay. And we can basically map this number into the exploding part and the fracking part

1078
01:12:02,579 --> 01:12:07,519
然后我们就得到了浮点数的表示。很酷。
and we get the floating point repent. Cool.

1079
01:12:07,519 --> 01:12:11,459
嗯，如果你对这个不熟悉的话，
Um, yeah, if you're not familiar with this,

1080
01:12:11,459 --> 01:12:15,319
我觉得你可以试试看，
I think you can try to,

1081
01:12:15,319 --> 01:12:19,259
我觉得你基本上可以试着用谷歌搜索，也可以查查维基百科。
I think you can basically try to Google it and you try to search Wikipedia.

1082
01:12:19,259 --> 01:12:22,039
我认为有很多解释说明了
I think there are a lot of explanations describing how

1083
01:12:22,039 --> 01:12:25,659
我们是如何用浮点标准来表示数值的，对吧？
we represent values using flowing point standards, okay?

1084
01:12:25,659 --> 01:12:27,519
好，今天的最后一页幻灯片，
Okay, today's last slide,

1085
01:12:27,519 --> 01:12:31,199
我希望你们每个人都做一下练习。
I want you to one I want all of you to do exercise.

1086
01:12:31,199 --> 01:12:34,459
那么，这个数字是什么？
So what is this number?

1087
01:12:42,710 --> 01:12:45,849
顺便说一下，这是一个浮点数。
By the way, this is a floating point.

1088
01:12:45,849 --> 01:12:48,509
只需要把它带入这个公式，对吧？
Just flow in the equation here, right?

1089
01:12:49,710 --> 01:12:53,469
好，你要做的第一步就是先看符号位，对吧？
Okay. What you do is basically you first start with the sun beat, right?

1090
01:12:53,469 --> 01:12:57,869
然后对于绿色部分，你把它代入指数部分。
And then for the green part, you substitute it into the exponent.

1091
01:12:57,869 --> 01:13:03,369
粉色部分的话，基本上就是在裂解部分完成的。
And for the pink part, you basically, uh, do it in the fracking part.

1092
01:13:03,369 --> 01:13:07,609
然后你用bailar重复法代入所有数值，基本上就能得到结果。
And you subsite all the values using bailar repetition and you probably get the value.

1093
01:13:07,609 --> 01:13:12,609
也就是说，太阳节拍大约是负一百二十，对吧？
That is the sun beat is roughly, uh, minus one, 20, right?

1094
01:13:12,609 --> 01:13:18,549
对于这个数值，如果你读出来的话，基本上是124。
And for this value, if you basically read it is basically 124,

1095
01:13:18,549 --> 01:13:23,509
这个数值基本上是作为分数来解释的。
for this value, it's basically interpreted as a fraction.

1096
01:13:23,509 --> 01:13:28,829
你要做的就是用一减去它们所有的总和。
And what you do is one minus all of them, submission of all of them,

1097
01:13:28,829 --> 01:13:35,124
然后你基本上就得到了虚拟数值，大约是0.15625。
and you basically get the vitual number, which is 0.1 5625.

1098
01:13:35,124 --> 01:13:39,159
好的。我希望你们能做一些额外的分析，因为我接下来要
Okay. I hope you guys can do some extra analysis because I'm going to

1099
01:13:39,159 --> 01:13:41,219
出一道其他考试题。
put one of the other exam question.

1100
01:13:41,219 --> 01:13:42,619
你们必须理解这个，好吗？
You have to understand this, okay?

1101
01:13:42,619 --> 01:13:47,959
嗯，就像我说的，我们要把这个降到更低的位数。
Be uh I said, like I said, we are going to lower this into lower bits.

1102
01:13:47,959 --> 01:13:52,719
比如说，如何用16位、8位来表示同样的数据，对吧？
For example, how to basically represent the same lumber using 16 bits, using eight bits, right?

1103
01:13:52,719 --> 01:13:54,019
什么是封闭的数据？
What is the closed lumber?

1104
01:13:54,019 --> 01:13:56,359
这基本上就是你在测试中要做的数量。
That is basically what quantity in tests you do.

1105
01:13:56,359 --> 01:14:01,579
很好，这就是我今天要讲的内容，下周见。
Cool. That's all I have today and see you next week. Cool.