1
00:00:14,840 --> 00:00:18,579
Okay. Okay, yeah.
Thanks for coming back.

2
00:00:18,579 --> 00:00:20,979
Let's get started. So today

3
00:00:20,979 --> 00:00:23,599
should be a very
lightwek lecture.

4
00:00:23,599 --> 00:00:26,899
We will only cover a
little bit in quantuion.

5
00:00:26,899 --> 00:00:29,339
Yeah. Okay. And then
we hope you will

6
00:00:29,339 --> 00:00:32,619
attend the Thursday's
guest lecture. Yeah.

7
00:00:32,619 --> 00:00:34,960
We hope you ask very
difficult questions

8
00:00:34,960 --> 00:00:36,940
to our guest, okay?

9
00:00:36,940 --> 00:00:41,660
Yeah. Okay, before we
start the main content,

10
00:00:41,660 --> 00:00:43,179
let's do a few MCC.

11
00:00:43,179 --> 00:00:48,019
Um, which of the
following is not one of

12
00:00:48,019 --> 00:00:53,139
the main sources of memory
consumption? D, right?

13
00:00:53,139 --> 00:00:55,020
Training code is pretty minimal.

14
00:00:55,020 --> 00:00:56,459
Okay, compared to anything else.

15
00:00:56,459 --> 00:01:03,399
Okay. So which of the following
statement is false about

16
00:01:03,399 --> 00:01:05,500
grading checkpointing?

17
00:01:17,300 --> 00:01:19,660
Which one?

18
00:01:24,060 --> 00:01:29,249
A, I heard A. So
let's go one by one.

19
00:01:29,249 --> 00:01:31,130
Apply grading
checkpointing during

20
00:01:31,130 --> 00:01:32,729
model training could
save GPO memory.

21
00:01:32,729 --> 00:01:34,330
This one is true because

22
00:01:34,330 --> 00:01:37,009
that's the purpose
of grad checkpoint.

23
00:01:37,009 --> 00:01:39,229
B, grading checkpointing applies

24
00:01:39,229 --> 00:01:41,790
to both model ways
and activations.

25
00:01:41,790 --> 00:01:44,250
Fourth, like I
already emphasized,

26
00:01:44,250 --> 00:01:46,969
grading checkpointing can
only save activations.

27
00:01:46,969 --> 00:01:49,170
The location of
grading checkpointing

28
00:01:49,170 --> 00:01:51,769
affects how much computation
and memory are needed.

29
00:01:51,769 --> 00:01:56,609
Yes, so you need to be
strategic on that, okay?

30
00:01:56,609 --> 00:01:59,629
So it's possible to describe
some of the activation

31
00:01:59,629 --> 00:02:00,770
from memory since they are

32
00:02:00,770 --> 00:02:02,109
only needed during
backward pass.

33
00:02:02,109 --> 00:02:05,250
Yes, that's kind of the explain

34
00:02:05,250 --> 00:02:07,430
how grading check point works.

35
00:02:07,430 --> 00:02:10,329
Okay, this one
take a bit longer.

36
00:02:10,329 --> 00:02:12,250
I already taught this
right in the last lecture.

37
00:02:12,250 --> 00:02:13,429
I want you to calculate it.

38
00:02:13,429 --> 00:02:15,309
Okay. So given this table

39
00:02:15,309 --> 00:02:18,610
from the very famous
GPD three paper,

40
00:02:18,610 --> 00:02:23,869
what is the activation
size of GPD three? 2.7 B.

41
00:02:23,869 --> 00:02:26,450
Okay, the smaller
version of GP three,

42
00:02:26,450 --> 00:02:29,309
assuming that we
are using a P 16

43
00:02:29,309 --> 00:02:31,310
and also assuming that we are

44
00:02:31,310 --> 00:02:33,990
checkpointing at the
transformer boundary.

45
00:02:33,990 --> 00:02:38,149
Okay, I will give you
1 minute to calculate.

46
00:02:51,750 --> 00:02:53,949
Mm.

47
00:03:50,830 --> 00:03:53,349
So what one?

48
00:04:06,870 --> 00:04:10,549
Yeah. The answer is C. Okay.

49
00:04:10,549 --> 00:04:13,389
But I want to do the
calculation directly,

50
00:04:13,389 --> 00:04:17,010
but I will give you some
so basically strategy.

51
00:04:17,010 --> 00:04:22,450
So here, it said that we need
a GPT three, 2.7 B, right?

52
00:04:22,450 --> 00:04:25,290
So we basically index
on this line, right?

53
00:04:25,290 --> 00:04:30,669
Okay? So in order to
estimate the activation,

54
00:04:30,669 --> 00:04:33,030
what parameters we need to know.

55
00:04:33,830 --> 00:04:36,190
I said LP 16, right?

56
00:04:36,190 --> 00:04:37,829
So basically each value

57
00:04:37,829 --> 00:04:40,390
will contribute to
two bites, right?

58
00:04:40,390 --> 00:04:43,389
And it also said we only
care about activation.

59
00:04:43,389 --> 00:04:45,030
So you'll still remember the

60
00:04:45,030 --> 00:04:46,710
activation for
transformer, right?

61
00:04:46,710 --> 00:04:49,229
It's batch size times.

62
00:04:49,950 --> 00:04:52,529
Sequence length,
sequence lengths.

63
00:04:52,529 --> 00:04:58,040
Okay. Times times three
in dimension, right?

64
00:04:58,040 --> 00:05:01,000
So basically D model.

65
00:05:01,760 --> 00:05:04,859
And therefore,

66
00:05:04,859 --> 00:05:07,720
you need to look for the
parameters from the table.

67
00:05:07,720 --> 00:05:11,440
So what is the battery
size? This table

68
00:05:11,440 --> 00:05:13,859
when they train this
2.7 billion model,

69
00:05:13,859 --> 00:05:16,169
it used 1 million batch size.

70
00:05:16,169 --> 00:05:20,019
Okay. But this 1
million body size

71
00:05:20,100 --> 00:05:22,560
is in a unit of tokens.

72
00:05:22,560 --> 00:05:26,420
That is in each batch,
there is 1 million tokens,

73
00:05:26,660 --> 00:05:29,840
this information is
already surfacing the way

74
00:05:29,840 --> 00:05:34,079
because in this original
activation calculation,

75
00:05:34,079 --> 00:05:36,220
we use batize times
sequence length,

76
00:05:36,220 --> 00:05:39,519
this BS is characterized
in amber sequences.

77
00:05:39,519 --> 00:05:41,260
Does that make sense?

78
00:05:41,260 --> 00:05:43,500
Lumber sequences
times sequence length

79
00:05:43,500 --> 00:05:47,790
equals to Number tokens, right?

80
00:05:47,790 --> 00:05:50,830
Okay. Therefore, this product

81
00:05:50,830 --> 00:05:52,650
is essentially just one, right?

82
00:05:52,650 --> 00:05:55,069
Okay. Therefore, this is one.

83
00:05:55,110 --> 00:05:58,490
D model, this t told
you that D model

84
00:05:58,490 --> 00:06:01,790
is essentially, um, here, right?

85
00:06:01,790 --> 00:06:05,410
Uh, 2560. Okay? This is

86
00:06:05,410 --> 00:06:08,769
basically like you time one
minion with this number,

87
00:06:08,769 --> 00:06:12,840
you essentially get the
activation per layer, right?

88
00:06:12,840 --> 00:06:15,349
Okay. So basically each
transformer boundary,

89
00:06:15,349 --> 00:06:16,650
you are going to meet this kind

90
00:06:16,650 --> 00:06:19,849
of activation, this
size of activation.

91
00:06:19,849 --> 00:06:22,410
But the problem
is, in this model,

92
00:06:22,410 --> 00:06:23,770
270 B GPT three,

93
00:06:23,770 --> 00:06:25,210
you have many many
layers, right?

94
00:06:25,210 --> 00:06:27,969
And I think this question
asks that we are going to

95
00:06:27,969 --> 00:06:31,309
apply gradient checkpoint
every layer boundary.

96
00:06:31,309 --> 00:06:32,810
Okay? Which means

97
00:06:32,810 --> 00:06:34,430
each layer, we all
checkpoint once, right?

98
00:06:34,430 --> 00:06:36,750
We only save them and we
discard all the rest, okay?

99
00:06:36,750 --> 00:06:38,050
So basically, we need to time

100
00:06:38,050 --> 00:06:40,550
this value with a number
of layers, right?

101
00:06:40,550 --> 00:06:43,590
So how many layers in 2.7 B?

102
00:06:43,590 --> 00:06:46,410
You can also get from
this table, right?

103
00:06:46,410 --> 00:06:49,989
It's here, 32.
Okay. So basically,

104
00:06:49,989 --> 00:06:52,409
you multiply another 32,

105
00:06:52,409 --> 00:06:54,629
and then you convert
it into gigabytes.

106
00:06:54,629 --> 00:06:58,729
That's your value. Okay?
Makes sense. Cool.

107
00:06:58,729 --> 00:07:01,390
We will ask a different
question in exam.

108
00:07:01,390 --> 00:07:03,609
Okay, a different model,
for example, Lama.

109
00:07:03,609 --> 00:07:07,509
Okay. I hope you
understand this, okay?

110
00:07:07,590 --> 00:07:12,169
Cool. Okay, I think
in math lecture,

111
00:07:12,169 --> 00:07:15,610
we basically, um,
talk about three,

112
00:07:15,610 --> 00:07:17,990
very important memory
ogenation right?

113
00:07:17,990 --> 00:07:21,019
Um, grading check pointing,

114
00:07:21,019 --> 00:07:24,699
micro baatchingO grading
accumulation and swapping.

115
00:07:24,699 --> 00:07:27,040
I think in practice, you
are facing a challenge that

116
00:07:27,040 --> 00:07:30,000
is whenever you meet
kind of auto memory,

117
00:07:30,000 --> 00:07:32,020
how you actually
navigate this space?

118
00:07:32,020 --> 00:07:33,539
Which strategy should I use?

119
00:07:33,539 --> 00:07:36,859
I got this question from
a lot of you after class.

120
00:07:36,859 --> 00:07:38,720
I basically want to
spend a few minutes on

121
00:07:38,720 --> 00:07:39,520
this slide because I think

122
00:07:39,520 --> 00:07:40,799
this slide is pretty important.

123
00:07:40,799 --> 00:07:41,940
In the future, when you start

124
00:07:41,940 --> 00:07:42,940
working on these kind of models,

125
00:07:42,940 --> 00:07:45,759
you are going to basically
navigate the tricks.

126
00:07:45,759 --> 00:07:48,440
Okay? So you start basically

127
00:07:48,440 --> 00:07:51,239
determine desired
training bad size,

128
00:07:51,239 --> 00:07:53,560
right? So why we
start with this?

129
00:07:54,780 --> 00:07:57,620
Because training body
size is a hyperparameter,

130
00:07:57,620 --> 00:07:59,019
it will affect your model

131
00:07:59,019 --> 00:08:00,900
your converged model
accuracy, right?

132
00:08:00,900 --> 00:08:02,780
You basically want
to first figure

133
00:08:02,780 --> 00:08:04,139
out the best training
body sizes for

134
00:08:04,139 --> 00:08:05,619
your job that will basically

135
00:08:05,619 --> 00:08:07,540
yield the highest
accuracy for your job.

136
00:08:07,540 --> 00:08:09,780
Okay? Because Beside is

137
00:08:09,780 --> 00:08:14,139
a super critical parameter
for sochastic green descent.

138
00:08:14,139 --> 00:08:16,820
So basically, you do
some hyperbarn tuning.

139
00:08:16,820 --> 00:08:19,240
You lock down a body size
that essentially give

140
00:08:19,240 --> 00:08:22,280
you seems to give you
very good results.

141
00:08:22,280 --> 00:08:23,840
So in this example, I said,

142
00:08:23,840 --> 00:08:25,700
maybe we are going to
train with a bit size

143
00:08:25,700 --> 00:08:28,059
equal to, uh, 64.

144
00:08:28,059 --> 00:08:30,760
And here, this is the
global biis, okay?

145
00:08:30,760 --> 00:08:33,979
And then we basically
start the training job

146
00:08:33,979 --> 00:08:37,520
with bad size equal to 64,
and we launched a training.

147
00:08:37,520 --> 00:08:40,394
We train with
bedside equal to 64.

148
00:08:40,394 --> 00:08:43,429
And then we check if we
are on memory, right?

149
00:08:43,429 --> 00:08:45,829
And if we don't go on
memory, we are good.

150
00:08:45,829 --> 00:08:48,529
We don't need to do any
memory orenation, right?

151
00:08:48,529 --> 00:08:51,070
But the problem is
maybe this model is too

152
00:08:51,070 --> 00:08:53,510
big or maybe your GPU has
very limited GPU memory,

153
00:08:53,510 --> 00:08:56,290
so it will go out of memory.

154
00:08:56,290 --> 00:08:58,849
Then you essentially
start navigating this

155
00:08:58,849 --> 00:09:01,489
uh choosing which memory

156
00:09:01,489 --> 00:09:03,489
orienting you want
to apply, right?

157
00:09:03,489 --> 00:09:06,789
So there are three I
mentioned, there's swapping,

158
00:09:06,789 --> 00:09:11,349
there's grading accumulation,
there is checkpointing.

159
00:09:11,349 --> 00:09:14,169
So which one you should
start thinking about first?

160
00:09:16,840 --> 00:09:21,340
Checkpoint. Any other answer?

161
00:09:21,340 --> 00:09:24,379
So you should start with
grading accumulation.

162
00:09:24,379 --> 00:09:28,060
Why? Because swapping
is super slow,

163
00:09:28,060 --> 00:09:30,400
you want to finish your
job as much as possible.

164
00:09:30,400 --> 00:09:32,080
And swapping is kind

165
00:09:32,080 --> 00:09:35,000
of determined by the memory
hierarchy bandwidth.

166
00:09:35,000 --> 00:09:37,300
Which I already told
you, today's computer,

167
00:09:37,300 --> 00:09:38,640
the bandwidth between
CP memory and

168
00:09:38,640 --> 00:09:41,440
chipping memory is very
low, so it's very slow.

169
00:09:41,440 --> 00:09:44,120
You want to consider
that at the end,

170
00:09:44,120 --> 00:09:45,899
then you basically
decide between

171
00:09:45,899 --> 00:09:48,900
grading accumulation or
grading checkpointing.

172
00:09:48,900 --> 00:09:51,800
Why we don't start with
grading checkpointing.

173
00:09:51,800 --> 00:09:53,919
Because in green chain pointing,

174
00:09:53,919 --> 00:09:55,140
you are essentially treating

175
00:09:55,140 --> 00:09:56,959
flops for your memory, right?

176
00:09:56,959 --> 00:10:00,020
Which means that once you
turn on green chain pointing,

177
00:10:00,020 --> 00:10:01,560
what happens is your
training job is

178
00:10:01,560 --> 00:10:03,259
going to be slower, right?

179
00:10:03,259 --> 00:10:04,700
Without green check pointing?

180
00:10:04,700 --> 00:10:06,140
Because you are
treating some flops,

181
00:10:06,140 --> 00:10:08,359
treating some compute
for your memory.

182
00:10:08,359 --> 00:10:09,840
And you don't want to do
that because you want to

183
00:10:09,840 --> 00:10:11,220
finish your job as
soon as possible,

184
00:10:11,220 --> 00:10:13,734
right? So you start with, um

185
00:10:13,734 --> 00:10:16,889
You try to start with
gradient accumulation.

186
00:10:16,889 --> 00:10:19,489
Gradient of accumulation,
how I actually navigate it.

187
00:10:19,489 --> 00:10:21,470
So how many steps
should I accumulate?

188
00:10:21,470 --> 00:10:23,450
Because essentially say, I

189
00:10:23,450 --> 00:10:25,690
want to train with a
global bites out of 64.

190
00:10:25,690 --> 00:10:27,209
I hold many many choices,

191
00:10:27,209 --> 00:10:29,849
I can't use a microbtes
equal to one,

192
00:10:29,849 --> 00:10:31,709
but I loop 64 times,

193
00:10:31,709 --> 00:10:33,850
I get equivalent results.

194
00:10:33,850 --> 00:10:36,549
Or I can use a
microbte equal to 32,

195
00:10:36,549 --> 00:10:39,010
I only look twice, right?
I also get the results.

196
00:10:39,010 --> 00:10:45,559
So which one should I use? So I

197
00:10:45,559 --> 00:10:47,740
should always try to
choose a microbody size

198
00:10:47,740 --> 00:10:51,580
that basically the
largest microboy it Why?

199
00:10:51,580 --> 00:10:54,320
Because if I use a
smaller microbic site,

200
00:10:54,320 --> 00:10:56,100
yes, I can solve
my memory issue.

201
00:10:56,100 --> 00:10:57,699
I will get the
equivalent results,

202
00:10:57,699 --> 00:10:59,299
but the problem is
you still remember

203
00:10:59,299 --> 00:11:01,619
when MCQ gave it
to you last week.

204
00:11:01,619 --> 00:11:04,479
The problem is your
metamol and many many,

205
00:11:04,479 --> 00:11:07,699
kind of like computer
intensive operations.

206
00:11:07,699 --> 00:11:11,339
They will have a smaller
dimension. I reduced to one.

207
00:11:11,339 --> 00:11:13,580
Like I said, in GPO,

208
00:11:13,580 --> 00:11:17,540
if you give a very small
metamo or you give a big memo,

209
00:11:17,540 --> 00:11:19,205
they will compute
at the same speed.

210
00:11:19,205 --> 00:11:20,910
Right? Which means
that if you give

211
00:11:20,910 --> 00:11:22,949
a small mathema with some
dimension equal to one,

212
00:11:22,949 --> 00:11:26,049
the utilization of GPO
will become lower, right?

213
00:11:26,049 --> 00:11:28,050
That means that your training
job will be extended.

214
00:11:28,050 --> 00:11:31,190
Although you basically
get the same results.

215
00:11:31,190 --> 00:11:32,569
So basically you
want to maximize

216
00:11:32,569 --> 00:11:34,749
the possible bist
you can use, okay?

217
00:11:34,749 --> 00:11:36,210
So basically, you
start searching

218
00:11:36,210 --> 00:11:39,749
32-16 to eight to four, right?

219
00:11:39,749 --> 00:11:42,010
And you basically alternate
it two variables.

220
00:11:42,010 --> 00:11:43,550
One is the effective microbess.

221
00:11:43,550 --> 00:11:46,129
The other is lab gradient
accumulating steps.

222
00:11:46,129 --> 00:11:48,229
Okay? You try to figure
out the one that is the

223
00:11:48,229 --> 00:11:50,670
biggest you can use without
going out of memory.

224
00:11:50,670 --> 00:11:52,450
Okay? Does that make sense?

225
00:11:52,450 --> 00:11:54,899
Cool. Okay.

226
00:11:54,899 --> 00:11:57,800
Then if you end

227
00:11:57,800 --> 00:12:00,379
up with figuring out
something, then you are good.

228
00:12:00,379 --> 00:12:02,795
Okay, you have a success.

229
00:12:02,795 --> 00:12:04,929
Then you'll start estimating,

230
00:12:04,929 --> 00:12:06,730
say, with this microbes,

231
00:12:06,730 --> 00:12:08,289
I'm going to train
one iteration with

232
00:12:08,289 --> 00:12:10,830
1 minute and I need
to train, say, uh,

233
00:12:10,830 --> 00:12:13,010
ten k iterations, and
you want to figure

234
00:12:13,010 --> 00:12:16,510
out an estimation of your
job completion time, JCT.

235
00:12:16,510 --> 00:12:18,509
You try to figure
out, you try to

236
00:12:18,509 --> 00:12:20,450
determine if this GCT can
meets your expectation.

237
00:12:20,450 --> 00:12:21,910
For example, your
manager probably ask you

238
00:12:21,910 --> 00:12:23,570
to give that results in one week

239
00:12:23,570 --> 00:12:28,149
and if this is too slow
or it's okay, right?

240
00:12:28,149 --> 00:12:30,289
And if you think it's
okay, then you are good.

241
00:12:30,289 --> 00:12:32,069
You can stop here, right?

242
00:12:32,069 --> 00:12:35,889
But if it's not,
uh, what do you do?

243
00:12:39,200 --> 00:12:41,599
Okay. Which means that

244
00:12:41,599 --> 00:12:44,519
you probably need to add
more compute, right?

245
00:12:44,640 --> 00:12:46,820
You need to add more GPOs.

246
00:12:46,820 --> 00:12:48,439
So we'll cover that next week.

247
00:12:48,439 --> 00:12:49,900
We are going to enter the world

248
00:12:49,900 --> 00:12:51,920
with distributing
machinery, paralyzation.

249
00:12:51,920 --> 00:12:54,100
We'll use multiple
GP for this job.

250
00:12:54,100 --> 00:12:57,439
Okay? So I hope

251
00:12:57,439 --> 00:12:59,939
you and me are on the same
page for this branch, right?

252
00:12:59,939 --> 00:13:03,239
So what if at this point you are

253
00:13:03,239 --> 00:13:05,060
not able to figure
out microbasess

254
00:13:05,060 --> 00:13:06,839
that can actually not
go out of memory?

255
00:13:06,839 --> 00:13:08,640
For example, even if
you use a microbes,

256
00:13:08,640 --> 00:13:11,660
equal to one, you are
still out of memory.

257
00:13:11,660 --> 00:13:16,029
So what is this is possible.

258
00:13:16,029 --> 00:13:18,849
For example, the model
is just so huge that H

259
00:13:18,849 --> 00:13:20,549
100 only have 80 gigabyte

260
00:13:20,549 --> 00:13:22,409
you are going to train
on Nama model, right?

261
00:13:22,409 --> 00:13:24,249
And the Nama model, like I said,

262
00:13:24,249 --> 00:13:26,150
seven model you cannot train on

263
00:13:26,150 --> 00:13:27,610
a single GP you
basically equal one.

264
00:13:27,610 --> 00:13:29,269
Yeah. So what you do is

265
00:13:29,269 --> 00:13:31,550
basically you start
considering the second choice,

266
00:13:31,550 --> 00:13:33,469
green check pointing, ok?

267
00:13:33,469 --> 00:13:36,249
You're going to turn
on if this failed,

268
00:13:36,249 --> 00:13:39,209
you are going to turn on
green check pointing.

269
00:13:39,209 --> 00:13:40,869
So you basically turn on

270
00:13:40,869 --> 00:13:42,269
green check pointing
and you start

271
00:13:42,269 --> 00:13:43,410
looping this again and again.

272
00:13:43,410 --> 00:13:46,510
You try to figure out the
largest microbd I can train on

273
00:13:46,510 --> 00:13:48,249
a single GPO with

274
00:13:48,249 --> 00:13:49,829
green check point turn

275
00:13:49,829 --> 00:13:52,105
and also with gradient
communication turno.

276
00:13:52,105 --> 00:13:54,119
Okay. And at some point,

277
00:13:54,119 --> 00:13:55,639
you are going to
figure out one thing.

278
00:13:55,639 --> 00:13:58,299
Maybe you are going to
figure out microbes

279
00:13:58,299 --> 00:14:00,619
that is going to work,
then you are good. Okay?

280
00:14:00,619 --> 00:14:02,759
You basically come back
to this line, right?

281
00:14:02,759 --> 00:14:05,219
You try to determine your
job completion time.

282
00:14:05,219 --> 00:14:07,939
Okay? But if you
still fail what you

283
00:14:07,939 --> 00:14:12,839
do you basically use a
microbe equal to one,

284
00:14:12,839 --> 00:14:15,499
and you turn on
gradient chat pointing,

285
00:14:15,499 --> 00:14:17,379
you turn on gradient
accumulation,

286
00:14:17,379 --> 00:14:19,480
you accumulate 64 times, okay?

287
00:14:19,480 --> 00:14:21,775
You'll still go on the memory.

288
00:14:21,775 --> 00:14:26,269
Then basically, uh, swapping
is the last word work.

289
00:14:26,269 --> 00:14:29,749
You start swapping was
into CP memory and you try

290
00:14:29,749 --> 00:14:31,790
to use memory hierarchy

291
00:14:31,790 --> 00:14:34,509
to barely make this
straining job work.

292
00:14:34,509 --> 00:14:36,950
But in general, uh,

293
00:14:36,950 --> 00:14:38,949
this is a bad choice
because it's going

294
00:14:38,949 --> 00:14:41,730
to probably make your training
job ten times slower.

295
00:14:41,730 --> 00:14:44,530
If your job is not
very time sensitive,

296
00:14:44,530 --> 00:14:46,709
you are good, you just want
to try out ideas, right?

297
00:14:46,709 --> 00:14:49,649
You probably launch the
job on your labs GPU and

298
00:14:49,649 --> 00:14:52,849
wait for two days and check
the results later, okay?

299
00:14:52,849 --> 00:14:55,710
But if you really
meeting deadline, uh,

300
00:14:55,710 --> 00:14:57,789
you probably need
to consider, like,

301
00:14:57,789 --> 00:14:59,550
prioritization,
you add multiples

302
00:14:59,550 --> 00:15:01,710
because when you add multiples,
you also get more memory.

303
00:15:01,710 --> 00:15:04,369
When you get more memory, you
can basically come back to

304
00:15:04,369 --> 00:15:07,730
this regime where you can
train with a large body size,

305
00:15:07,730 --> 00:15:09,630
maximize your GPU addition.

306
00:15:09,630 --> 00:15:12,250
You get more flops, more memory.

307
00:15:12,250 --> 00:15:16,109
Okay. So basically, this is
a practical guide for how to

308
00:15:16,109 --> 00:15:18,209
optimize memory and make sure

309
00:15:18,209 --> 00:15:20,470
you understand its workflow
because it's super useful.

310
00:15:20,470 --> 00:15:21,709
Okay?

311
00:15:22,830 --> 00:15:27,109
Cool. Okay. Today we are going
to continue to talk about,

312
00:15:27,109 --> 00:15:30,030
our topic last week. So
basically quantization.

313
00:15:30,030 --> 00:15:31,970
I think last week,

314
00:15:31,970 --> 00:15:36,029
we talked lit digital
representation of data, right?

315
00:15:36,029 --> 00:15:38,469
So still remember
this figure, right?

316
00:15:38,469 --> 00:15:40,649
I didn't explain pretty
much last week, but,

317
00:15:40,649 --> 00:15:43,190
I hope you basically look
at the slides a little bit

318
00:15:43,190 --> 00:15:45,570
during the weekend and you
understand what is exponent,

319
00:15:45,570 --> 00:15:47,249
what is fraction, and how

320
00:15:47,249 --> 00:15:50,289
we represent floating
point numbers here.

321
00:15:50,289 --> 00:15:53,230
This is our standard
L P 32, okay?

322
00:15:53,230 --> 00:15:57,369
So the intuition behind the
floating point is that, uh,

323
00:15:57,369 --> 00:16:00,429
we basically designate a
few bits for represent

324
00:16:00,429 --> 00:16:04,210
the exponent and by varying
the values of exponent,

325
00:16:04,210 --> 00:16:07,129
we can move the floating
point from left to right.

326
00:16:07,129 --> 00:16:10,450
Therefore, we have a
pretty dynamic range,

327
00:16:10,450 --> 00:16:13,949
and we can also basically
designate a few bits for

328
00:16:13,949 --> 00:16:15,449
the fraction so we can control

329
00:16:15,449 --> 00:16:18,165
the precision of our
floating point numbers.

330
00:16:18,165 --> 00:16:23,180
Okay. And in order to
move the floating point,

331
00:16:23,180 --> 00:16:27,459
you know, about one
also below one,

332
00:16:27,459 --> 00:16:29,239
we have to add exponent bias.

333
00:16:29,239 --> 00:16:32,339
Exploding bias is basically
the median number of

334
00:16:32,339 --> 00:16:33,699
the maximum number that can be

335
00:16:33,699 --> 00:16:36,379
represented using eight
bits in this example.

336
00:16:36,379 --> 00:16:38,599
So it's the number of bits
in the exponent part.

337
00:16:38,599 --> 00:16:43,179
Okay? Yeah. Any
questions on this part?

338
00:16:43,700 --> 00:16:47,880
Cool. Then we are going to
do something more excise.

339
00:16:47,880 --> 00:16:50,359
Okay. So in this figure,

340
00:16:50,359 --> 00:16:52,059
I give you the
equation, how to read

341
00:16:52,059 --> 00:16:54,960
this floating point
representation in 32 bits.

342
00:16:54,960 --> 00:16:58,500
My question is, how
to represent zero.

343
00:17:03,930 --> 00:17:06,489
Okay. But looking
at this equation,

344
00:17:06,489 --> 00:17:07,650
you figure out that it's

345
00:17:07,650 --> 00:17:09,129
impossible to
represent zero, right?

346
00:17:09,129 --> 00:17:12,149
B in the middle part
one plus fraction,

347
00:17:12,149 --> 00:17:15,269
you can see, we always
have bias term one, right?

348
00:17:15,269 --> 00:17:17,690
Way time something
that is not zero.

349
00:17:17,690 --> 00:17:20,089
So basically, this
equation does not

350
00:17:20,089 --> 00:17:23,430
have zeros like p value, okay?

351
00:17:23,430 --> 00:17:25,409
So how to read
zero. So basically,

352
00:17:25,409 --> 00:17:28,010
we have to do some very
artificial thing, okay?

353
00:17:28,010 --> 00:17:30,769
So in order to represent
zero, we basically, uh,

354
00:17:30,769 --> 00:17:33,890
say that if all the
explosion equals zero,

355
00:17:33,890 --> 00:17:37,090
we are going to change the
floating point representation

356
00:17:37,090 --> 00:17:38,769
into that equation.

357
00:17:38,769 --> 00:17:40,529
Okay, if you look at

358
00:17:40,529 --> 00:17:42,509
that equation compared to
the left set equation,

359
00:17:42,509 --> 00:17:44,569
you'll find the first
change is basically,

360
00:17:44,569 --> 00:17:47,129
um, in the middle part,

361
00:17:47,129 --> 00:17:49,049
the left part is one
plus fraction, right?

362
00:17:49,049 --> 00:17:50,669
You know, in the right
part, is fraction.

363
00:17:50,669 --> 00:17:52,450
There's no one plus, okay?

364
00:17:52,450 --> 00:17:55,269
And the second change is
on the power uh power two.

365
00:17:55,269 --> 00:17:58,329
In the left part is basically
exponent minus bias,

366
00:17:58,329 --> 00:18:01,909
but in the right part is
basically one minus the bias.

367
00:18:01,909 --> 00:18:04,210
Okay? This is a little
bit artificial,

368
00:18:04,210 --> 00:18:06,489
but this is how we
represent 14 point numbers.

369
00:18:06,489 --> 00:18:09,440
Um, and we call this
kind of lambers.

370
00:18:09,440 --> 00:18:11,859
So basically, when all the
expolen bits are there,

371
00:18:11,859 --> 00:18:14,479
we call this kind of
lambs sublomal lambers.

372
00:18:14,479 --> 00:18:16,859
Okay. When the expoolen
bits are not there,

373
00:18:16,859 --> 00:18:19,080
we call it mal lambers.

374
00:18:19,080 --> 00:18:21,140
So essentially, for any
floating point retation

375
00:18:21,140 --> 00:18:22,740
we have two ranges.

376
00:18:22,740 --> 00:18:25,119
One is sublomalumbers. The
other is lomal lambers.

377
00:18:25,119 --> 00:18:26,599
Okay. And it's determined by

378
00:18:26,599 --> 00:18:29,054
if the expoolen bits
are there or not.

379
00:18:29,054 --> 00:18:33,509
Okay. Cool. So to answer
my previous question,

380
00:18:33,509 --> 00:18:34,869
in order to represent
zero, what do you

381
00:18:34,869 --> 00:18:36,730
do is basically, uh,

382
00:18:36,730 --> 00:18:38,470
you go into the sublomer range,

383
00:18:38,470 --> 00:18:41,270
and you set or expollen
bits into zero,

384
00:18:41,270 --> 00:18:42,969
meanwhile, you set or fraction

385
00:18:42,969 --> 00:18:44,670
bits into zero is
basically zero.

386
00:18:44,670 --> 00:18:47,530
Okay. Here, you get
rid of the one plus.

387
00:18:47,530 --> 00:18:50,929
Okay. Cool. Then with

388
00:18:50,929 --> 00:18:53,690
this understanding of
lomal and sublom values,

389
00:18:53,690 --> 00:18:57,110
our question is, what is
the minimum positive value?

390
00:18:57,860 --> 00:19:00,339
Okay. So in order to determine

391
00:19:00,339 --> 00:19:03,959
the minimum positive value
represented by this,

392
00:19:03,959 --> 00:19:05,459
we only hear about
minimum positive

393
00:19:05,459 --> 00:19:06,899
value because we
can take the minus

394
00:19:06,899 --> 00:19:10,399
and we get the mini maximum
negative value, right?

395
00:19:10,399 --> 00:19:13,130
So what is the minimum
positive value?

396
00:19:13,130 --> 00:19:15,480
So that can basically
give us a sense

397
00:19:15,480 --> 00:19:18,479
like how precise we can be,
right, using insenation.

398
00:19:18,479 --> 00:19:20,640
Okay? So the minimum
positive value

399
00:19:20,640 --> 00:19:22,340
we just need to
basically investigate,

400
00:19:22,340 --> 00:19:24,279
uh, the left hand side
and the right hand,

401
00:19:24,279 --> 00:19:25,779
what is what is the minimum

402
00:19:25,779 --> 00:19:27,119
and we'll take a
minimum oppose, right?

403
00:19:27,119 --> 00:19:28,999
So on the left hand side,

404
00:19:28,999 --> 00:19:30,580
we basically use that equation.

405
00:19:30,580 --> 00:19:33,500
Uh, What do we do
is basically, um,

406
00:19:33,500 --> 00:19:35,240
because in left hand
side, we cannot

407
00:19:35,240 --> 00:19:37,019
side all the exponent
bits into all the arrow.

408
00:19:37,019 --> 00:19:38,499
So the minimum value we can

409
00:19:38,499 --> 00:19:40,499
do for exponent bits is
basically one, right?

410
00:19:40,499 --> 00:19:44,949
So, uh, we just basically
Assign this value into one.

411
00:19:44,949 --> 00:19:46,890
That's the minimum we
can do for exponent.

412
00:19:46,890 --> 00:19:49,210
And of course for the
fraction, we can do a zero.

413
00:19:49,210 --> 00:19:50,770
That will give the
minimum value.

414
00:19:50,770 --> 00:19:52,169
And we substitute this

415
00:19:52,169 --> 00:19:53,729
into this equation,
we get this one.

416
00:19:53,729 --> 00:19:58,350
It's basically two
to two minus 126.

417
00:19:58,350 --> 00:20:00,744
It's a very small value, okay.

418
00:20:00,744 --> 00:20:02,659
And on the right hand side, we

419
00:20:02,659 --> 00:20:03,959
just use a different
equation, right?

420
00:20:03,959 --> 00:20:06,559
So, because on the
right hand side,

421
00:20:06,559 --> 00:20:09,300
it is sublomal so this
part must be zero.

422
00:20:09,300 --> 00:20:11,239
And what we can do
is for this part,

423
00:20:11,239 --> 00:20:14,119
we are going to choose
the minimum value, right?

424
00:20:14,119 --> 00:20:16,220
Minimum long zero value, okay?

425
00:20:16,220 --> 00:20:18,379
And we basically get
this one. And this is

426
00:20:18,379 --> 00:20:20,700
the minimum value we can
do for the sublomer range.

427
00:20:20,700 --> 00:20:22,620
And if you compare
this two, apparently,

428
00:20:22,620 --> 00:20:24,979
right hand side is
even smaller, right.

429
00:20:24,979 --> 00:20:28,459
So that's basically the
minimum positive value we

430
00:20:28,459 --> 00:20:32,920
can represent using P 32, okay?

431
00:20:34,140 --> 00:20:38,740
Okay. And we also have a few
other artificial things,

432
00:20:38,740 --> 00:20:42,579
artificial values we designated
into this orientation.

433
00:20:42,579 --> 00:20:47,039
For example, in this example,

434
00:20:47,039 --> 00:20:49,359
all the exponent bits is one.

435
00:20:49,359 --> 00:20:51,500
But in the normal range,

436
00:20:51,500 --> 00:20:53,219
okay all the exponent bits are

437
00:20:53,219 --> 00:20:55,980
one and all the
fraction bits are zero,

438
00:20:55,980 --> 00:20:59,220
we basically call this,
uh, positive infinity.

439
00:20:59,220 --> 00:21:01,239
Okay? This is easy to
understand, right.

440
00:21:01,239 --> 00:21:04,560
So basically this value is
maximized, and this is zero.

441
00:21:04,560 --> 00:21:06,840
And we basically use
this resentation

442
00:21:06,840 --> 00:21:09,195
to represent positive infinity.

443
00:21:09,195 --> 00:21:11,270
And the same thing, we just, um,

444
00:21:11,270 --> 00:21:13,069
change the sun
beat a little bit,

445
00:21:13,069 --> 00:21:16,029
we represent the
negative infinity. Okay?

446
00:21:16,029 --> 00:21:18,630
And meanwhile, uh, in
a subnormal range,

447
00:21:18,630 --> 00:21:22,230
what do we do is,
um, not subnormal.

448
00:21:22,230 --> 00:21:25,729
So basically, when all this
value is one, uh, basically,

449
00:21:25,729 --> 00:21:28,710
we will use and when
this sunbat is not used,

450
00:21:28,710 --> 00:21:31,270
we are going to use
this to represent,

451
00:21:31,270 --> 00:21:33,629
um, not a amber.

452
00:21:33,629 --> 00:21:36,249
Okay? And we don't care
about any values in this.

453
00:21:36,249 --> 00:21:38,109
And you can see this
is a big waste, right,

454
00:21:38,109 --> 00:21:41,000
because so basically we waste,

455
00:21:41,000 --> 00:21:43,239
many, many
possibilities in this.

456
00:21:43,239 --> 00:21:47,499
And, yeah. And we use this
to represent no lumber,

457
00:21:47,499 --> 00:21:49,819
and there are a lot
of waste, right,

458
00:21:49,819 --> 00:21:51,899
because we originally could use

459
00:21:51,899 --> 00:21:54,280
this kind of thing to
represent the other values,

460
00:21:54,280 --> 00:21:58,074
um, Okay, and we will
revise this in FVA.

461
00:21:58,074 --> 00:22:00,529
Okay. To summarize, I

462
00:22:00,529 --> 00:22:02,529
think this table gives
you a summarization.

463
00:22:02,529 --> 00:22:05,289
So we have two ranges, right?

464
00:22:05,289 --> 00:22:07,070
Loom range and the sublom range.

465
00:22:07,070 --> 00:22:10,309
Okay? So when you
exposing to zero,

466
00:22:10,309 --> 00:22:13,650
we basically go into
the sublom range.

467
00:22:13,650 --> 00:22:14,910
And in the sublomer range,

468
00:22:14,910 --> 00:22:17,849
we use this equation to

469
00:22:17,849 --> 00:22:21,165
basically convert the
binary into decimal, okay?

470
00:22:21,165 --> 00:22:23,720
And when the exponent
is not zero,

471
00:22:23,720 --> 00:22:25,000
we go into the normal range

472
00:22:25,000 --> 00:22:26,679
and we use different equation.

473
00:22:26,679 --> 00:22:28,220
And that's the equation
that everybody knows,

474
00:22:28,220 --> 00:22:30,260
okay, in normal range.

475
00:22:30,260 --> 00:22:33,320
Okay. And when the
exponent is full.

476
00:22:33,320 --> 00:22:35,239
So basically every
rabit is what?

477
00:22:35,239 --> 00:22:38,299
Then we reserve all the
other all these kind of,

478
00:22:38,299 --> 00:22:40,659
uh repents and we use that as

479
00:22:40,659 --> 00:22:45,300
either positive or negative
infinity or not number.

480
00:22:45,300 --> 00:22:47,780
Okay. And especially
on this range,

481
00:22:47,780 --> 00:22:50,160
we basically waste a lot.

482
00:22:50,160 --> 00:22:53,060
Okay. Any question
on this table?

483
00:22:53,740 --> 00:22:56,979
Okay. Then one
thing you probably

484
00:22:56,979 --> 00:23:01,129
will gradually understand
is, you can see,

485
00:23:01,129 --> 00:23:04,749
if we basically draw all
the lumbers on the x axis,

486
00:23:04,749 --> 00:23:06,349
and we try to uh,

487
00:23:06,349 --> 00:23:08,309
calculate the distance
between two numbers,

488
00:23:08,309 --> 00:23:11,690
we find that this
floating point operation

489
00:23:11,690 --> 00:23:13,129
flowing point lumber is

490
00:23:13,129 --> 00:23:14,769
quite different from
fixed point numbers.

491
00:23:14,769 --> 00:23:16,309
Remember in fixed
point, we basically

492
00:23:16,309 --> 00:23:18,289
designate a decimal point
at some place right.

493
00:23:18,289 --> 00:23:20,809
So basically, we
have equal distance

494
00:23:20,809 --> 00:23:22,209
between different umbers.

495
00:23:22,209 --> 00:23:23,569
But in flowing point number,

496
00:23:23,569 --> 00:23:25,429
if we put the value
on this axis,

497
00:23:25,429 --> 00:23:28,230
you will figure out that
when the value is small,

498
00:23:28,230 --> 00:23:30,830
uh, we actually have
a lot of precision.

499
00:23:30,830 --> 00:23:32,870
So the distance between

500
00:23:32,870 --> 00:23:35,290
any two lambers in this
area is very small.

501
00:23:35,290 --> 00:23:37,550
Okay? But window
value is pretty big,

502
00:23:37,550 --> 00:23:39,290
you can see Uh,

503
00:23:39,290 --> 00:23:41,970
the distance between two
numbers are very big.

504
00:23:41,970 --> 00:23:43,590
That is well less precision

505
00:23:43,590 --> 00:23:46,129
in this area. So why we do this?

506
00:23:46,340 --> 00:23:48,479
So basically, it's a trade off.

507
00:23:48,479 --> 00:23:51,159
So we want to represent
large numbers.

508
00:23:51,159 --> 00:23:53,059
But we also want
to represent very

509
00:23:53,059 --> 00:23:55,119
small like floating
point numbers.

510
00:23:55,119 --> 00:23:56,620
Okay. We want to trade off.

511
00:23:56,620 --> 00:23:58,659
So basically, um, the

512
00:23:58,659 --> 00:24:01,620
exploding best controls
the dynamic range.

513
00:24:01,620 --> 00:24:03,579
If we give it more
like exploding bees we

514
00:24:03,579 --> 00:24:05,539
are going to represent larger
and larger numbers, right?

515
00:24:05,539 --> 00:24:08,900
And the fraction bias
basically give you precision.

516
00:24:08,900 --> 00:24:11,360
If you give more fraction
bias, you have more precision.

517
00:24:11,360 --> 00:24:12,600
That is the distance between

518
00:24:12,600 --> 00:24:15,460
any smaller two small values
is going to be smaller.

519
00:24:15,460 --> 00:24:16,800
So you can represent
more values.

520
00:24:16,800 --> 00:24:18,639
Okay? It's essential trade off.

521
00:24:18,639 --> 00:24:20,399
Okay? And remember this

522
00:24:20,399 --> 00:24:22,980
because we are going to
revis this again and again,

523
00:24:22,980 --> 00:24:25,900
in quantison machine learning.

524
00:24:27,370 --> 00:24:30,149
Okay, with that, I think we can

525
00:24:30,149 --> 00:24:32,170
basically dive deeper into

526
00:24:32,170 --> 00:24:34,649
the true floating
point representations

527
00:24:34,649 --> 00:24:36,270
we used in machine learning.

528
00:24:36,270 --> 00:24:38,849
Okay. And this is LP 32, right?

529
00:24:38,849 --> 00:24:41,289
And I think we dive
pretty deep into that.

530
00:24:41,289 --> 00:24:43,590
It has eight bits of exposed,

531
00:24:43,590 --> 00:24:47,389
23 bits of fraction,
right, total 32.

532
00:24:47,389 --> 00:24:51,289
And because machine learning
becomes so popular.

533
00:24:51,289 --> 00:24:54,569
So Act basically
establish we also try to

534
00:24:54,569 --> 00:24:58,350
leverage another standard
established from AE 75,

535
00:24:58,350 --> 00:25:01,589
six, I basically LP 60 or 16.

536
00:25:01,589 --> 00:25:03,569
So, uh this one

537
00:25:03,569 --> 00:25:06,150
is basically the one
that we use to train.

538
00:25:06,150 --> 00:25:08,529
Language models to
train a lot of models.

539
00:25:08,529 --> 00:25:10,329
Okay, um, transformers.

540
00:25:10,329 --> 00:25:13,249
And in this LP uh 16,

541
00:25:13,249 --> 00:25:15,489
what we do is basically
we use five bits of

542
00:25:15,489 --> 00:25:18,329
exponent and ten
bits of fraction.

543
00:25:18,329 --> 00:25:20,489
Okay. And like I said,

544
00:25:20,489 --> 00:25:22,049
I previous lecture, I think

545
00:25:22,049 --> 00:25:24,889
Google it comes up
of brain flow 16.

546
00:25:24,889 --> 00:25:28,369
Okay? And what they do
is compared to FP 16,

547
00:25:28,369 --> 00:25:30,569
uh, they adjust a little bit,

548
00:25:30,569 --> 00:25:32,470
so they give more bits into,

549
00:25:32,470 --> 00:25:37,294
um, this exponent bits and
they go last bits to fraction.

550
00:25:37,294 --> 00:25:39,679
And I also said that

551
00:25:39,679 --> 00:25:41,839
there's strength
empirical evidence that

552
00:25:41,839 --> 00:25:44,160
this BF 16 is much

553
00:25:44,160 --> 00:25:46,840
better at training
neural networks. So why?

554
00:25:46,840 --> 00:25:49,260
Can anyone give intuition?

555
00:25:55,660 --> 00:25:58,240
So we can reason a
little bit here, okay?

556
00:25:58,240 --> 00:26:02,239
So compare FP 16 and BF 16.

557
00:26:02,239 --> 00:26:03,939
The only difference is BF

558
00:26:03,939 --> 00:26:06,020
16 is going to have a
higher dynamic range.

559
00:26:06,020 --> 00:26:08,059
It can represent larger values,

560
00:26:08,059 --> 00:26:09,860
but it will lose some precision.

561
00:26:09,860 --> 00:26:12,700
Okay? So why this is
good for training?

562
00:26:12,700 --> 00:26:14,360
Because in machinery training,

563
00:26:14,360 --> 00:26:16,320
especially in the early
phase of your training,

564
00:26:16,320 --> 00:26:18,839
you are going to
derive gradits and

565
00:26:18,839 --> 00:26:21,600
your grading can go up and
down. It's very turbulent.

566
00:26:21,600 --> 00:26:25,020
Okay? And if you have a
very small dynamic range,

567
00:26:25,020 --> 00:26:26,660
what you do is at some step,

568
00:26:26,660 --> 00:26:29,279
the gradient slightly explode,

569
00:26:29,279 --> 00:26:30,579
okay and probably will

570
00:26:30,579 --> 00:26:34,599
explode beyond the
dynamic range of P 16,

571
00:26:34,599 --> 00:26:36,380
then you'll go into

572
00:26:36,380 --> 00:26:38,919
infinity value or a lot
of lumber value, right?

573
00:26:38,919 --> 00:26:41,410
And if you apply that
value into your training,

574
00:26:41,410 --> 00:26:43,839
it's going to destroy
your model, right?

575
00:26:43,839 --> 00:26:46,479
That's why, uh, Google
basically found this and,

576
00:26:46,479 --> 00:26:49,279
uh, they figure out
basically bit of 16.

577
00:26:49,279 --> 00:26:50,920
They give more exponent bits

578
00:26:50,920 --> 00:26:53,620
to the 16 bits of orientation,

579
00:26:53,620 --> 00:26:55,320
and it can stabilize
the training

580
00:26:55,320 --> 00:26:57,599
because you can represent
the larger gradients,

581
00:26:57,599 --> 00:26:59,039
either positive or negative.

582
00:26:59,039 --> 00:27:00,920
Okay? That's intuition.

583
00:27:00,920 --> 00:27:06,360
Cool. Any inference what you
do? Which one is better?

584
00:27:10,170 --> 00:27:12,749
In general, I would
say inference,

585
00:27:12,749 --> 00:27:13,989
P six is better because

586
00:27:13,989 --> 00:27:17,770
inference basically
with a stabilizer.

587
00:27:17,770 --> 00:27:20,270
And if you invest
some margining model,

588
00:27:20,270 --> 00:27:22,209
you probably figure
out the most ways of

589
00:27:22,209 --> 00:27:25,370
marginy model are very smaller
values after convergence.

590
00:27:25,370 --> 00:27:25,610
Why?

591
00:27:25,610 --> 00:27:28,010
Because we have a lot of
regulations during training.

592
00:27:28,010 --> 00:27:30,390
We make sure the value will
not go beyond some values.

593
00:27:30,390 --> 00:27:32,689
We'll do greening called
clipping or whatever, right?

594
00:27:32,689 --> 00:27:34,630
And given this assumption,

595
00:27:34,630 --> 00:27:36,349
what happens is in inference,

596
00:27:36,349 --> 00:27:39,270
we probably prefer
more fraction bits.

597
00:27:39,270 --> 00:27:41,929
Why? Because we want
precise results.

598
00:27:41,929 --> 00:27:44,130
Yeah, we want more precision.

599
00:27:44,130 --> 00:27:47,709
Remember this figure, we want
this to be more precise,

600
00:27:47,709 --> 00:27:49,310
so we want more fraction bias.

601
00:27:49,310 --> 00:27:51,229
Okay. This basically give you

602
00:27:51,229 --> 00:27:53,029
some intuition on these kind

603
00:27:53,029 --> 00:27:54,369
of lumbers we have
been talking about.

604
00:27:54,369 --> 00:27:57,389
Okay? Okay.

605
00:27:57,389 --> 00:28:01,349
To summarize, so if you
have more exploded width,

606
00:28:01,349 --> 00:28:04,550
you basically have a larger
range, dynamic range, okay?

607
00:28:04,550 --> 00:28:06,790
If you have more fraction width,

608
00:28:06,790 --> 00:28:08,150
you basically have
more precision.

609
00:28:08,150 --> 00:28:09,569
And eventually, you
want to treat off

610
00:28:09,569 --> 00:28:11,130
between this depending
on your workloads.

611
00:28:11,130 --> 00:28:16,689
Okay. Okay. I hope
this is clear, right?

612
00:28:16,689 --> 00:28:18,210
You understand the
floating point, okay?

613
00:28:18,210 --> 00:28:20,309
We are going to do exercise.

614
00:28:20,309 --> 00:28:25,489
Okay? So what is this lumber?

615
00:28:25,489 --> 00:28:27,970
I will give you 2 minutes.

616
00:28:35,000 --> 00:28:39,039
Maybe I mean it because
you hold my slide. Okay.

617
00:29:32,160 --> 00:29:34,899
Okay, yeah, let's go through it.

618
00:29:34,899 --> 00:29:36,520
So when you read
this kind of number,

619
00:29:36,520 --> 00:29:37,900
you basically start
with two things.

620
00:29:37,900 --> 00:29:39,359
One is you figure out if this is

621
00:29:39,359 --> 00:29:41,040
mal range or sub lomer range.

622
00:29:41,040 --> 00:29:44,539
So this is apparently
normal because you

623
00:29:44,539 --> 00:29:45,959
look at the expoen bits and

624
00:29:45,959 --> 00:29:48,420
it's not zero, so it's normal.

625
00:29:48,420 --> 00:29:49,780
Once you figure out it's normal,

626
00:29:49,780 --> 00:29:50,899
you basically know the equation,

627
00:29:50,899 --> 00:29:53,399
the first equation, then

628
00:29:53,399 --> 00:29:55,240
this sum bit is pretty easy?

629
00:29:55,240 --> 00:29:56,580
We are going to talk
about it later.

630
00:29:56,580 --> 00:29:59,059
But the second thing you
want to figure out is uh,

631
00:29:59,059 --> 00:30:00,520
for this floating point,

632
00:30:00,520 --> 00:30:02,979
you want to figure out the
bias value, like I said,

633
00:30:02,979 --> 00:30:07,199
The bias value is basically
the median the median

634
00:30:07,199 --> 00:30:09,179
of the maximum value
that are represented

635
00:30:09,179 --> 00:30:11,700
by this exponent bis, okay?

636
00:30:11,700 --> 00:30:13,599
So sine minus, okay?

637
00:30:13,599 --> 00:30:15,659
Uh, then we figure out exponent.

638
00:30:15,659 --> 00:30:17,499
So the bias value
is for this one,

639
00:30:17,499 --> 00:30:21,060
this is zero right, this is
the one, two, three, four.

640
00:30:21,060 --> 00:30:24,580
So the maximum value is the
bias is basically, uh, 50,

641
00:30:24,580 --> 00:30:28,300
okay? Okay, that makes sense.

642
00:30:28,300 --> 00:30:30,719
Because the largest value
that can be really is,

643
00:30:30,719 --> 00:30:32,339
I think, 31, right?

644
00:30:32,339 --> 00:30:36,139
Yeah. Okay. So then

645
00:30:36,139 --> 00:30:37,759
you'll basically read
this value, right?

646
00:30:37,759 --> 00:30:39,279
This is the value
you got, right?

647
00:30:39,279 --> 00:30:41,300
And you want to minus the bios.

648
00:30:41,300 --> 00:30:42,940
Okay? And so in decimal,

649
00:30:42,940 --> 00:30:44,719
this value is 17, right?

650
00:30:44,719 --> 00:30:46,580
And in -15, okay,

651
00:30:46,580 --> 00:30:48,500
you get two in decimal.

652
00:30:48,500 --> 00:30:51,399
Okay? The exponent
is basically two.

653
00:30:51,399 --> 00:30:53,759
And then we figure
out the fraction,

654
00:30:53,759 --> 00:30:55,539
the fraction is pretty
easy to rate, right?

655
00:30:55,539 --> 00:30:58,640
This is half, right? This
is half. This is a quarter.

656
00:30:58,640 --> 00:31:01,620
Okay. You basically
add all them together.

657
00:31:02,130 --> 00:31:04,369
Because only these
two values are one,

658
00:31:04,369 --> 00:31:07,749
so you get 0.75,
right, in decimal.

659
00:31:07,749 --> 00:31:10,509
And then you know the
equation the equation

660
00:31:10,509 --> 00:31:13,430
we're going to use is essentially
the mal equation, okay?

661
00:31:13,430 --> 00:31:16,109
So basically, it's minus B

662
00:31:16,109 --> 00:31:18,650
it's normal equation right
so we have a one plus,

663
00:31:18,650 --> 00:31:22,529
plus this value and this
value is two, right?

664
00:31:22,529 --> 00:31:26,450
So this is a decimal,
okay? Makes sense.

665
00:31:26,450 --> 00:31:29,370
Okay. Cool. In exam,

666
00:31:29,370 --> 00:31:31,190
I'm going to give you
a sub lomer value.

667
00:31:31,190 --> 00:31:33,324
Okay. So you figure out.

668
00:31:33,324 --> 00:31:36,359
Cool. Okay. Let's do

669
00:31:36,359 --> 00:31:38,679
a reverse exercise because
it is so important.

670
00:31:38,679 --> 00:31:40,419
I want to make sure
you understand this.

671
00:31:40,419 --> 00:31:43,200
Okay? So this is
Google Brain float,

672
00:31:43,200 --> 00:31:46,480
BF 16, the one that we use
to train transformers.

673
00:31:46,480 --> 00:31:52,779
So my question is, what is
the decimal 2.5 in B 16.

674
00:31:52,779 --> 00:31:55,719
Okay? Again, 1 minute.

675
00:32:31,800 --> 00:32:33,839
Okay.

676
00:33:01,040 --> 00:33:04,040
Okay, let's go through it, okay?

677
00:33:04,040 --> 00:33:06,679
Um, so when you solve
this kind of question,

678
00:33:06,679 --> 00:33:08,559
the intuition is
basically you try

679
00:33:08,559 --> 00:33:11,439
to make sure you compose
a equation like this,

680
00:33:11,439 --> 00:33:15,160
because this is the
representation for flame point.

681
00:33:15,160 --> 00:33:16,359
And you try to represent

682
00:33:16,359 --> 00:33:17,640
this decimal number
into something

683
00:33:17,640 --> 00:33:20,579
that one plus something
times the power of two.

684
00:33:20,579 --> 00:33:24,380
Okay? So apparently,
this is, um,

685
00:33:24,380 --> 00:33:26,779
so because this is
one plus fraction and

686
00:33:26,779 --> 00:33:29,399
this value is greater than
one, right? It's two.

687
00:33:29,399 --> 00:33:31,039
So you have to divide
it by at least two.

688
00:33:31,039 --> 00:33:33,239
Okay? So you basically
divide it by two first.

689
00:33:33,239 --> 00:33:38,739
Right, you get a 1.25
times two power one, okay?

690
00:33:38,739 --> 00:33:40,759
And then you can figure
out this fraction is

691
00:33:40,759 --> 00:33:42,839
essentially a quarter, right?

692
00:33:42,839 --> 00:33:45,859
And this exponent
should be one, right?

693
00:33:45,859 --> 00:33:49,040
Okay. So then you
use this one value,

694
00:33:49,040 --> 00:33:51,540
you basically get the
actual exponent value,

695
00:33:51,540 --> 00:33:57,459
it should be, so basically
bias is this, right?

696
00:33:57,459 --> 00:34:00,939
X minus 127 equal to one,

697
00:34:00,939 --> 00:34:02,839
and X is actually 128,

698
00:34:02,839 --> 00:34:06,500
you try to represent
128 in binary.

699
00:34:06,500 --> 00:34:07,979
Okay. That I give you

700
00:34:07,979 --> 00:34:10,959
basically remember
this is B flow 16,

701
00:34:10,959 --> 00:34:13,740
so you have the many
bits for expolen.

702
00:34:13,740 --> 00:34:15,219
That's why you get this value

703
00:34:15,219 --> 00:34:18,039
pretty longer than the
previous one, okay?

704
00:34:18,039 --> 00:34:20,440
Meanwhile, you have fraction.

705
00:34:20,440 --> 00:34:22,000
You have seven bits of fraction

706
00:34:22,000 --> 00:34:23,540
and here you already
know your fraction

707
00:34:23,540 --> 00:34:26,799
is, um, a quarter.

708
00:34:26,799 --> 00:34:30,835
So basically the second
value here is basically.

709
00:34:30,835 --> 00:34:33,790
Okay. Yeah. I put
it all together,

710
00:34:33,790 --> 00:34:35,369
you also have the same bit.

711
00:34:35,369 --> 00:34:39,050
That's basically this
number in B flow 60.

712
00:34:39,050 --> 00:34:45,469
Cool. Okay.
Interesting. With that,

713
00:34:45,469 --> 00:34:47,649
apparently, we
start talking about

714
00:34:47,649 --> 00:34:50,109
even lower bits, LP eight.

715
00:34:50,109 --> 00:34:52,364
LP eight is the one that,

716
00:34:52,364 --> 00:34:55,119
kind of shift the curve
a little bit because

717
00:34:55,119 --> 00:34:57,539
if you heard about Deep sik with

718
00:34:57,539 --> 00:35:00,079
three and a major claim they

719
00:35:00,079 --> 00:35:03,280
use that they don't use
32, they don't use 16.

720
00:35:03,280 --> 00:35:05,779
They use FPAight to train
their model, right?

721
00:35:05,779 --> 00:35:08,079
I think you have the intuition
why FB eight is better

722
00:35:08,079 --> 00:35:10,019
because whenever you can use

723
00:35:10,019 --> 00:35:12,219
lower bits to train your model,

724
00:35:12,219 --> 00:35:13,800
you are going to
have more flaws.

725
00:35:13,800 --> 00:35:15,980
Your GP will be more powerful.

726
00:35:15,980 --> 00:35:18,619
Okay. And indeed, they

727
00:35:18,619 --> 00:35:20,979
were very successful
in the sense that

728
00:35:20,979 --> 00:35:22,839
they are able to
use this number of

729
00:35:22,839 --> 00:35:26,260
bits to very large
transformative convergence.

730
00:35:26,260 --> 00:35:28,420
That is amazing.
Why is it amazing?

731
00:35:28,420 --> 00:35:30,780
Because, visually,

732
00:35:30,780 --> 00:35:33,639
you can already feel this
because you can see this is 30.

733
00:35:33,639 --> 00:35:35,760
It's so long, right. It can
represent so many numbers.

734
00:35:35,760 --> 00:35:37,279
Okay? This is 16 and

735
00:35:37,279 --> 00:35:39,559
it's a little bit shorter
but still pretty long.

736
00:35:39,559 --> 00:35:42,379
But if you reduce
this by another half,

737
00:35:42,379 --> 00:35:44,380
you basically only
get eight bits.

738
00:35:44,380 --> 00:35:46,619
Okay. And like I said,

739
00:35:46,619 --> 00:35:48,999
you need to reserve the first
bit as a sign bit, right?

740
00:35:48,999 --> 00:35:51,900
So essentially you
have seven bits,

741
00:35:51,900 --> 00:35:55,540
that need to be allocated
between expoden and fraction.

742
00:35:55,540 --> 00:35:57,280
So you don't have many numbers.

743
00:35:57,280 --> 00:35:59,019
Okay. Probably you
can just write

744
00:35:59,019 --> 00:36:00,879
all the numbers in white paper.

745
00:36:00,879 --> 00:36:03,940
Yeah, that's all the numbers
this IP eight can represent.

746
00:36:03,940 --> 00:36:05,889
Okay. And you can think this.

747
00:36:05,889 --> 00:36:07,110
This is pretty amazing

748
00:36:07,110 --> 00:36:08,750
because essentially you
only have these numbers,

749
00:36:08,750 --> 00:36:10,589
you can you have,

750
00:36:10,589 --> 00:36:13,669
say, 70,700 billion number.

751
00:36:13,669 --> 00:36:14,769
You basically need
to choose each

752
00:36:14,769 --> 00:36:17,309
lumber from this range, right?

753
00:36:17,309 --> 00:36:19,290
Design value to each parameter,

754
00:36:19,290 --> 00:36:20,469
and you will get
a model that can

755
00:36:20,469 --> 00:36:21,869
basically answer
many many questions.

756
00:36:21,869 --> 00:36:24,405
Yeah. Yeah, it's
pretty interesting.

757
00:36:24,405 --> 00:36:27,879
So for P eight, this one is like

758
00:36:27,879 --> 00:36:29,359
a major development by

759
00:36:29,359 --> 00:36:33,679
VDa B NVDA wants to push
for lower precision, right?

760
00:36:33,679 --> 00:36:35,759
They have this drive
because they want to

761
00:36:35,759 --> 00:36:37,439
maintain their more law, right?

762
00:36:37,439 --> 00:36:38,820
The only way that
you can maintain

763
00:36:38,820 --> 00:36:40,500
this law is basically
you use lower precision.

764
00:36:40,500 --> 00:36:42,500
Okay? And there
are two standards.

765
00:36:42,500 --> 00:36:44,760
One is e4m3.

766
00:36:44,760 --> 00:36:46,899
The other is e5m2.

767
00:36:46,899 --> 00:36:48,899
So basically, e43 stands for

768
00:36:48,899 --> 00:36:50,519
exponen equals to four bits

769
00:36:50,519 --> 00:36:51,939
and the Mntisa equal to three.

770
00:36:51,939 --> 00:36:55,704
And the R is basically exponent
five Mantisa two, okay?

771
00:36:55,704 --> 00:36:58,909
And apparently not
already you can

772
00:36:58,909 --> 00:37:00,329
already reason between

773
00:37:00,329 --> 00:37:02,209
the pros and cons
between these two.

774
00:37:02,209 --> 00:37:04,789
So in this e4m3, uh,

775
00:37:04,789 --> 00:37:08,390
I have four bits of explon
and three bits of fraction.

776
00:37:08,390 --> 00:37:10,009
And in this one, can I have

777
00:37:10,009 --> 00:37:13,225
five explot two of
fraction, okay?

778
00:37:13,225 --> 00:37:16,459
And we also have some artificial
designations that is,

779
00:37:16,459 --> 00:37:18,139
for example, in CLP eight,

780
00:37:18,139 --> 00:37:20,340
EMs, we do not have infinity.

781
00:37:20,340 --> 00:37:26,680
Okay? And we reserve some
one as basically lot number.

782
00:37:26,680 --> 00:37:33,480
Okay. And the largest Em
normal value is basically 448.

783
00:37:33,480 --> 00:37:35,219
Okay? That's the
largest you can.

784
00:37:35,219 --> 00:37:38,800
So in order to make sure
your training is successful,

785
00:37:38,800 --> 00:37:41,060
which means that
during the throat,

786
00:37:41,060 --> 00:37:42,559
the training process,
you cannot produce

787
00:37:42,559 --> 00:37:44,319
a grading that is
bigger than this one.

788
00:37:44,319 --> 00:37:45,700
Otherwise, it will disrupt

789
00:37:45,700 --> 00:37:47,039
your marche learning
training, right?

790
00:37:47,039 --> 00:37:50,509
Okay. And we can also do,

791
00:37:50,509 --> 00:37:54,869
e52, and we have a few
artificial things.

792
00:37:54,869 --> 00:37:57,789
I will leave to you to
check the slides, okay?

793
00:37:57,789 --> 00:38:00,549
And the largest
PightE two value is

794
00:38:00,549 --> 00:38:04,709
basically like almost
100 times larger, right?

795
00:38:04,709 --> 00:38:06,129
More than 100 times larger.

796
00:38:06,129 --> 00:38:08,529
So basically, uh, five,
seven, three, four,

797
00:38:08,529 --> 00:38:11,229
four, okay? Then I
ask you a question.

798
00:38:11,229 --> 00:38:12,789
So now,

799
00:38:12,789 --> 00:38:16,070
assuming that I'm only allowed
to use LP eight, okay?

800
00:38:16,070 --> 00:38:17,709
So which one should I use for

801
00:38:17,709 --> 00:38:20,290
training and which one
should I use for inference?

802
00:38:23,960 --> 00:38:27,219
You know answer
because in training,

803
00:38:27,219 --> 00:38:28,859
we want more dynamic range.

804
00:38:28,859 --> 00:38:32,160
So we would use
e52 for training.

805
00:38:32,160 --> 00:38:34,640
And in inference, we
want more precision.

806
00:38:34,640 --> 00:38:37,900
So we would use E for
three for inference.

807
00:38:37,900 --> 00:38:41,459
That is basically what we are
doing today for LP eight,

808
00:38:41,459 --> 00:38:44,809
training and
inference. Okay, cool.

809
00:38:44,809 --> 00:38:51,110
And the quest for even lower
precision number stops.

810
00:38:51,110 --> 00:38:52,909
So I think Amdia
is trying to push

811
00:38:52,909 --> 00:38:55,849
for Integer four is old thing.

812
00:38:55,849 --> 00:38:57,469
Integer four is
already last year,

813
00:38:57,469 --> 00:38:59,230
I think Ing four is
very successful.

814
00:38:59,230 --> 00:39:02,010
If you ever run any
model on your iPhone,

815
00:39:02,010 --> 00:39:04,829
it's likely being
run in Integer four.

816
00:39:04,829 --> 00:39:07,670
But Amedia has been more
and more aggressive.

817
00:39:07,670 --> 00:39:09,809
In the next generation
of GPUs they want

818
00:39:09,809 --> 00:39:12,269
to push for LP four. Okay.

819
00:39:12,269 --> 00:39:14,449
And FP four is
even great, right?

820
00:39:14,449 --> 00:39:16,749
Because if you check this curve,

821
00:39:16,749 --> 00:39:18,129
let's go one by one, okay.

822
00:39:18,129 --> 00:39:20,370
For integer four, essentially,

823
00:39:20,370 --> 00:39:23,189
I can write down is value
in the right hand side.

824
00:39:23,189 --> 00:39:25,529
So here I use to
complement, right,

825
00:39:25,529 --> 00:39:26,950
you still remember to compliment

826
00:39:26,950 --> 00:39:28,269
the reentation integers, right?

827
00:39:28,269 --> 00:39:31,570
So, um, uh, I write
down is values,

828
00:39:31,570 --> 00:39:35,950
and if I basically put
these values X cess,

829
00:39:35,950 --> 00:39:37,989
you can see, uh,

830
00:39:37,989 --> 00:39:40,609
the distance between any
two values is equal.

831
00:39:40,609 --> 00:39:43,859
So, uh basically integers, okay?

832
00:39:43,859 --> 00:39:46,370
And for P four, media

833
00:39:46,370 --> 00:39:48,850
is trying to push
for three standards.

834
00:39:48,850 --> 00:39:51,589
The first standard is
basically EY M two,

835
00:39:51,589 --> 00:39:54,210
one bit for exponent,

836
00:39:54,210 --> 00:39:55,749
two bits for Mantisa.

837
00:39:55,749 --> 00:39:57,590
Okay? I also write

838
00:39:57,590 --> 00:39:59,989
the value here, and
you can check later.

839
00:39:59,989 --> 00:40:01,930
But if I put all these values,

840
00:40:01,930 --> 00:40:04,589
that is represented by EIM two.

841
00:40:04,589 --> 00:40:06,889
I basically get this range.

842
00:40:06,889 --> 00:40:10,809
And you all find
that EIM two is not

843
00:40:10,809 --> 00:40:15,509
very useful Y because it is
essentially, Integer four.

844
00:40:15,509 --> 00:40:17,289
Because what I do
is, for example,

845
00:40:17,289 --> 00:40:19,269
I can skill it a little bit

846
00:40:19,269 --> 00:40:22,129
and it will basically be
skilled into this range.

847
00:40:22,129 --> 00:40:26,629
Basically, LP four E two is
equivalent to integer four,

848
00:40:26,629 --> 00:40:28,969
we don't use that very much

849
00:40:28,969 --> 00:40:32,949
because we often roll back
just to use integer four.

850
00:40:32,949 --> 00:40:35,190
But we have a few other choices.

851
00:40:35,190 --> 00:40:36,729
One is e2m1.

852
00:40:36,729 --> 00:40:38,889
Okay? If you do e2m1

853
00:40:38,889 --> 00:40:41,210
and you put all the
values in excess,

854
00:40:41,210 --> 00:40:44,549
you'll find that is
slightly different, right?

855
00:40:44,549 --> 00:40:47,029
Because here we have

856
00:40:47,029 --> 00:40:48,789
a slightly larger
dameter range because

857
00:40:48,789 --> 00:40:51,350
we give more bits into Epoing.

858
00:40:51,350 --> 00:40:55,620
Okay. And there's
a third standard.

859
00:40:55,620 --> 00:40:58,679
Basically we only do exp
we don't designate any bis

860
00:40:58,679 --> 00:41:02,119
for Mantisa B it
only hold four bits.

861
00:41:02,119 --> 00:41:04,999
Es 30. And it turns out that

862
00:41:04,999 --> 00:41:07,920
e3m0 is basically
the log values,

863
00:41:07,920 --> 00:41:09,819
you can see, all the values were

864
00:41:09,819 --> 00:41:12,739
reason is basically
the power of two.

865
00:41:12,940 --> 00:41:18,939
And this is possible
values. Cool. Any question?

866
00:41:19,860 --> 00:41:24,419
Yeah, by far, there's

867
00:41:24,419 --> 00:41:29,729
no successful job that
is trained using P four.

868
00:41:29,729 --> 00:41:32,020
This is a pretty good
area of research.

869
00:41:32,020 --> 00:41:34,239
So if you are doing
research in this area,

870
00:41:34,239 --> 00:41:35,919
I strongly recommend
to look into this.

871
00:41:35,919 --> 00:41:37,659
I think you can publish
the first paper,

872
00:41:37,659 --> 00:41:39,820
if you can make this successful.

873
00:41:39,820 --> 00:41:45,260
Cool. That basically covers

874
00:41:45,260 --> 00:41:46,919
digital representation of data.

875
00:41:46,919 --> 00:41:49,040
I think you now have
a deep understanding

876
00:41:49,040 --> 00:41:51,300
of how we represent
data in uni machinery.

877
00:41:51,300 --> 00:41:53,119
Then we start talking
about a bit on

878
00:41:53,119 --> 00:41:57,259
condensation. I'm going
to repeat this slide.

879
00:41:57,259 --> 00:42:00,219
Condensation is a process of
constraining an input from

880
00:42:00,219 --> 00:42:02,139
a continuous or
otherwise large set

881
00:42:02,139 --> 00:42:03,844
of values to a discrete set.

882
00:42:03,844 --> 00:42:06,589
And the left hand
side basically trying

883
00:42:06,589 --> 00:42:08,530
to represent a continuous signal

884
00:42:08,530 --> 00:42:10,369
using a few possible
choices, right?

885
00:42:10,369 --> 00:42:12,110
And the right hand
side is basically

886
00:42:12,110 --> 00:42:14,509
what apples does in
their Apposilicon.

887
00:42:14,509 --> 00:42:19,070
So they have um technical
basically contest the values,

888
00:42:19,070 --> 00:42:21,430
uh, and once you
contact the images,

889
00:42:21,430 --> 00:42:23,990
you can store the image
in lower precision.

890
00:42:23,990 --> 00:42:26,870
You can also compute using
lower precision cores,

891
00:42:26,870 --> 00:42:28,809
and you can see what energy.

892
00:42:28,809 --> 00:42:30,809
You can see, uh,

893
00:42:30,809 --> 00:42:32,609
for human, you can
basically figure

894
00:42:32,609 --> 00:42:34,749
out both images are kept, right?

895
00:42:34,749 --> 00:42:37,750
Okay. So in this lecture,

896
00:42:37,750 --> 00:42:41,270
we are going to talk about
two very, very classic.

897
00:42:41,270 --> 00:42:42,809
In this class, we
are going to talk

898
00:42:42,809 --> 00:42:45,530
about two classic
coding technique.

899
00:42:45,530 --> 00:42:47,910
The first one is basically
means based condition,

900
00:42:47,910 --> 00:42:49,809
and this one is the
most adoptive, Okay.

901
00:42:49,809 --> 00:42:52,769
Uh, so likely all your
phones have this. Okay?

902
00:42:52,769 --> 00:42:55,570
The second one is basically
linear condensation,

903
00:42:55,570 --> 00:42:57,269
and this one is the most
powerful, I would say.

904
00:42:57,269 --> 00:42:59,649
So many, many today's
language model,

905
00:42:59,649 --> 00:43:02,570
uh, they basically follow a
linear condition mechanism.

906
00:43:02,570 --> 00:43:04,449
Okay. I think the thing

907
00:43:04,449 --> 00:43:05,849
that we want to pay
attention is when

908
00:43:05,849 --> 00:43:09,270
we apply this codsation
after the condensation,

909
00:43:09,270 --> 00:43:12,570
so what happens on
storage and compute?

910
00:43:12,570 --> 00:43:15,050
So say if we apply
chemist coniation,

911
00:43:15,050 --> 00:43:17,929
what happens on storage?
How we store data?

912
00:43:17,929 --> 00:43:20,069
Because originally
you store the data

913
00:43:20,069 --> 00:43:22,229
in floating point in
hyp precision and

914
00:43:22,229 --> 00:43:23,950
our goal of condiy

915
00:43:23,950 --> 00:43:27,119
represent hypersen data
using present data.

916
00:43:27,119 --> 00:43:29,470
Okay. Second question
is basically compute.

917
00:43:29,470 --> 00:43:31,229
So after quantization, what

918
00:43:31,229 --> 00:43:32,769
kind of course are we calling?

919
00:43:32,769 --> 00:43:35,670
Uh, say, if the
original data is LP 32,

920
00:43:35,670 --> 00:43:37,210
when you compute FP 32,

921
00:43:37,210 --> 00:43:41,250
you can only call epi 32
course VDA if you remember,

922
00:43:41,250 --> 00:43:44,910
uh, I think we inspected
the product sheet of VDA.

923
00:43:44,910 --> 00:43:47,369
So the flops for Epi 32, uh,

924
00:43:47,369 --> 00:43:49,589
is much lower than lower
precision, for example,

925
00:43:49,589 --> 00:43:52,629
tensor core epis 16 or
tensor core FP eight.

926
00:43:52,629 --> 00:43:54,710
So if we are able to quantize

927
00:43:54,710 --> 00:43:57,310
some values into
lower precision,

928
00:43:57,310 --> 00:44:01,010
that can match high flops
tensor core on VDIa,

929
00:44:01,010 --> 00:44:03,129
we can compute much
faster, right?

930
00:44:03,129 --> 00:44:04,489
Uh, given that we can

931
00:44:04,489 --> 00:44:06,735
preserve machine learning
accuracy and results.

932
00:44:06,735 --> 00:44:09,600
Okay. So every time
in the future,

933
00:44:09,600 --> 00:44:11,359
um, I think whenever you read

934
00:44:11,359 --> 00:44:13,380
a quantity paper, you
want to ask yourself.

935
00:44:13,380 --> 00:44:15,720
So after I apply this
quantity technique,

936
00:44:15,720 --> 00:44:16,899
how I store my data,

937
00:44:16,899 --> 00:44:19,440
in what kind of precision
and how I compute,

938
00:44:19,440 --> 00:44:21,589
in what kind of core, okay?

939
00:44:21,589 --> 00:44:24,560
Let's look at the mins cosition.

940
00:44:24,560 --> 00:44:26,599
Of course, we start with, say,

941
00:44:26,599 --> 00:44:29,020
this four by four matrix, okay?

942
00:44:29,020 --> 00:44:33,779
And here, I assume that
we store data in 32 bits.

943
00:44:33,779 --> 00:44:38,179
Okay? So what Cemens condition
does is basically, um,

944
00:44:38,179 --> 00:44:39,839
so we figure out, there are

945
00:44:39,839 --> 00:44:42,520
a few values from this
four by four matrix,

946
00:44:42,520 --> 00:44:43,920
and they are very close.

947
00:44:43,920 --> 00:44:46,600
Okay? They are close in
the sense that probably

948
00:44:46,600 --> 00:44:49,499
we can just reduce we can
represent them all in,

949
00:44:49,499 --> 00:44:53,059
say a lumber or two. They
are pretty close to two.

950
00:44:53,059 --> 00:44:54,979
And we basically try to

951
00:44:54,979 --> 00:44:57,499
represent the unit just
a single value, okay?

952
00:44:57,499 --> 00:45:00,699
And, assuming that we

953
00:45:00,699 --> 00:45:03,139
can suffer this kind of
condensation error, right?

954
00:45:03,139 --> 00:45:05,359
So in this case, each
value has a small

955
00:45:05,359 --> 00:45:07,780
condension arrow,
after condition.

956
00:45:07,780 --> 00:45:09,739
Okay. So how do we do this?

957
00:45:09,739 --> 00:45:11,480
What do we do is basically,

958
00:45:11,480 --> 00:45:14,920
we use chemis if you guys
are familiar with chemist,

959
00:45:14,920 --> 00:45:17,559
you probably already, figure
out what we are going to do.

960
00:45:17,559 --> 00:45:23,050
So basically, we take this
fob formtris as a as input,

961
00:45:23,050 --> 00:45:25,650
and we just use K means
to run clustering.

962
00:45:25,650 --> 00:45:29,210
Okay? And the number of K
is yet to be determined.

963
00:45:29,210 --> 00:45:31,429
But here, let's assume
that K equal to four.

964
00:45:31,429 --> 00:45:33,570
Okay? That is we
want to represent

965
00:45:33,570 --> 00:45:36,549
all the values all
the flow values here,

966
00:45:36,549 --> 00:45:40,770
four by four or 16 values
using just four codes.

967
00:45:40,770 --> 00:45:44,109
Okay? And we basically
find the code at least

968
00:45:44,109 --> 00:45:45,669
clods to that value and we

969
00:45:45,669 --> 00:45:48,069
change the value into
that code, right?

970
00:45:48,069 --> 00:45:49,489
In order to get that code,

971
00:45:49,489 --> 00:45:51,809
I think we run cluster right?

972
00:45:51,809 --> 00:45:53,689
Clustering we basically
get that code book.

973
00:45:53,689 --> 00:45:56,449
Okay. And here,
after clustering, C,

974
00:45:56,449 --> 00:45:58,719
we get a code book of two,

975
00:45:58,719 --> 00:46:01,119
1.50 and minus one.

976
00:46:01,119 --> 00:46:02,739
And what we do is basically for

977
00:46:02,739 --> 00:46:04,040
each value in the
original matrix,

978
00:46:04,040 --> 00:46:05,439
we are going to find
the one that is

979
00:46:05,439 --> 00:46:08,699
the Closs code on the code
book and we change this value.

980
00:46:08,699 --> 00:46:10,940
That is basically a
process of condition.

981
00:46:10,940 --> 00:46:13,599
We want to represent a much
higher dimensional data,

982
00:46:13,599 --> 00:46:14,919
not higher higher pressing

983
00:46:14,919 --> 00:46:17,839
data using a limited
number of choices.

984
00:46:17,839 --> 00:46:20,099
Okay. So in this case,

985
00:46:20,099 --> 00:46:22,579
after the classing and we

986
00:46:22,579 --> 00:46:25,499
basically do some
nearest neighbor,

987
00:46:25,499 --> 00:46:29,724
and we figure out the
quantity should be like this.

988
00:46:29,724 --> 00:46:32,889
Okay. Makes sense, right?

989
00:46:32,889 --> 00:46:35,329
Cool. And we can
characterize the quantat

990
00:46:35,329 --> 00:46:37,849
arrow by basically minus value
from the original value,

991
00:46:37,849 --> 00:46:39,309
and this is the
quantiting arrow.

992
00:46:39,309 --> 00:46:41,429
And apparently, we want
this qding arrow to

993
00:46:41,429 --> 00:46:43,529
be as small as possible, right?

994
00:46:43,529 --> 00:46:44,969
Because, uh,

995
00:46:44,969 --> 00:46:47,329
intuitively it will influence

996
00:46:47,329 --> 00:46:48,970
your eventual machinery results.

997
00:46:48,970 --> 00:46:53,989
Yeah. Okay. Um no,
let's ask a question.

998
00:46:53,989 --> 00:46:58,709
Uh how are we doing on storage?

999
00:46:58,709 --> 00:47:00,649
So if we do this
kind of quantudon,

1000
00:47:00,649 --> 00:47:03,549
uh, can we actually
save the storage? Okay?

1001
00:47:03,549 --> 00:47:06,429
So originally, um,

1002
00:47:06,429 --> 00:47:09,129
we have this four by
four matrix, right?

1003
00:47:09,129 --> 00:47:12,070
So each entry in this matrix

1004
00:47:12,070 --> 00:47:15,489
is represented using 34
bits floating point, okay?

1005
00:47:15,489 --> 00:47:17,769
We have 16 values, so
ten them together,

1006
00:47:17,769 --> 00:47:20,009
we get 512 bit,

1007
00:47:20,009 --> 00:47:22,869
which is equal to 64 bytes.

1008
00:47:22,869 --> 00:47:26,149
Okay? And after condon,
how we are doing?

1009
00:47:26,149 --> 00:47:31,430
Okay. So now, because we have
a codebook, of four values.

1010
00:47:31,430 --> 00:47:32,969
So in order to
represent four values,

1011
00:47:32,969 --> 00:47:33,794
how many bits we need?

1012
00:47:33,794 --> 00:47:36,339
Two bits, right?

1013
00:47:36,339 --> 00:47:38,759
That's why each entry
we only need two bits.

1014
00:47:38,759 --> 00:47:40,960
And we have 16 entries,

1015
00:47:40,960 --> 00:47:43,119
so we have 32 bits.

1016
00:47:43,119 --> 00:47:45,379
In total, we only
need four bytes.

1017
00:47:45,379 --> 00:47:46,599
You can see this is a pretty

1018
00:47:46,599 --> 00:47:48,559
aggressive reduction of stories.

1019
00:47:48,559 --> 00:47:50,979
Meanwhile, we also need to

1020
00:47:50,979 --> 00:47:53,079
represent our code book because

1021
00:47:53,079 --> 00:47:55,179
our codebook is still
a flowing point.

1022
00:47:55,179 --> 00:47:57,379
So in this example,

1023
00:47:57,379 --> 00:47:59,439
we have four codes, Basically

1024
00:47:59,439 --> 00:48:02,019
each code is still 32
bits of flowing point.

1025
00:48:02,019 --> 00:48:03,820
So in order to
store the codebook,

1026
00:48:03,820 --> 00:48:05,779
we have 16 bytes.

1027
00:48:05,779 --> 00:48:07,959
Every time we try to
figure out value,

1028
00:48:07,959 --> 00:48:10,220
we basically index
on the codebook.

1029
00:48:10,619 --> 00:48:13,340
So if we do a comparison,

1030
00:48:13,340 --> 00:48:16,079
we can see that we get
a 3.2 X reduction,

1031
00:48:16,079 --> 00:48:17,820
which is pretty good on storage.

1032
00:48:17,820 --> 00:48:20,879
Okay? This is the power
of condensation, right?

1033
00:48:20,879 --> 00:48:22,619
Originally,

1034
00:48:22,619 --> 00:48:24,999
you cannot store matures on
your phone, but now you can.

1035
00:48:24,999 --> 00:48:30,129
Yeah. Okay. Then let's
generalize a little bit.

1036
00:48:30,129 --> 00:48:33,529
So assuming that we are
going to do bed quantuation.

1037
00:48:33,529 --> 00:48:35,630
Okay? So this determines

1038
00:48:35,630 --> 00:48:37,169
the number of codes
in your codebook.

1039
00:48:37,169 --> 00:48:39,229
Right. In our previous example,

1040
00:48:39,229 --> 00:48:41,749
we do two bit qdon
and now we do BD and.

1041
00:48:41,749 --> 00:48:43,509
That is why we are going
to have more code.

1042
00:48:43,509 --> 00:48:46,169
So more Cos means more
pending power, right.

1043
00:48:46,169 --> 00:48:48,950
So therefore, there will be
less quantitation error.

1044
00:48:48,950 --> 00:48:51,369
Okay. But that
intuitive also means

1045
00:48:51,369 --> 00:48:53,969
that you are going
to use more storage.

1046
00:48:53,969 --> 00:48:56,109
Right. So we are

1047
00:48:56,109 --> 00:48:58,649
also going to assume
that for big matrix,

1048
00:48:58,649 --> 00:49:01,009
we have a lot of entries
and we have entries.

1049
00:49:01,009 --> 00:49:02,889
And we're going to assume that

1050
00:49:02,889 --> 00:49:05,009
uh the number of entries
in the matrix you

1051
00:49:05,009 --> 00:49:08,929
are going to quantize is much
greater than two power one,

1052
00:49:08,929 --> 00:49:11,169
that is the size
of the code book.

1053
00:49:11,169 --> 00:49:15,869
This is pretty reasonable
because in reality, this case.

1054
00:49:15,869 --> 00:49:18,509
So for storage on
the left hand side,

1055
00:49:18,509 --> 00:49:23,489
will be to get 32 bits times
M equals to 32 bts, right?

1056
00:49:23,489 --> 00:49:25,389
On right hand side,

1057
00:49:25,389 --> 00:49:28,869
will get the B times
M equal to MMbts,

1058
00:49:29,230 --> 00:49:31,369
for the codebook,
we are going to

1059
00:49:31,369 --> 00:49:34,549
have 32 bits times
two power one,

1060
00:49:34,549 --> 00:49:36,669
which is two power
N plus five bits.

1061
00:49:36,669 --> 00:49:39,509
But like I said, this is much
greater than two power one,

1062
00:49:39,509 --> 00:49:41,389
so we are going to basically

1063
00:49:41,389 --> 00:49:43,329
think we are going to

1064
00:49:43,329 --> 00:49:45,389
not count this because
this is a small value.

1065
00:49:45,389 --> 00:49:48,199
Okay. Then the reduction rate

1066
00:49:48,199 --> 00:49:50,720
is essentially 32 divided by NM,

1067
00:49:50,720 --> 00:49:54,219
which is equal to 32 divided
by N times the reduction.

1068
00:49:54,219 --> 00:49:55,799
This is basically the power

1069
00:49:55,799 --> 00:49:59,380
of KMs condensation effective.

1070
00:49:59,380 --> 00:50:02,419
I directly give you a
reduction by factor of N,

1071
00:50:02,419 --> 00:50:05,299
and this N is basically, um,

1072
00:50:05,299 --> 00:50:10,039
the code book that you want
to choose. Any question?

1073
00:50:10,559 --> 00:50:15,179
Okay. Um, uh, this

1074
00:50:15,179 --> 00:50:16,659
basically give you a
sense like how way

1075
00:50:16,659 --> 00:50:19,320
to basic condition using Kemins.

1076
00:50:19,320 --> 00:50:21,459
Okay. But you probably

1077
00:50:21,459 --> 00:50:23,219
ask if I do this
kind of condition,

1078
00:50:23,219 --> 00:50:25,679
what I do for neur
network training?

1079
00:50:25,679 --> 00:50:26,880
Because in neuronal training,

1080
00:50:26,880 --> 00:50:28,719
we are going to do
forward and backward.

1081
00:50:28,719 --> 00:50:30,919
And during backward,
what happens is,

1082
00:50:30,919 --> 00:50:33,639
uh, my values on the
met Mol, for example,

1083
00:50:33,639 --> 00:50:36,019
for metamol XM Y,

1084
00:50:36,019 --> 00:50:37,579
my values on X and Y is going

1085
00:50:37,579 --> 00:50:39,939
to update it by the
grading distance.

1086
00:50:39,939 --> 00:50:42,994
So what I do. So this is
pretty straightforward, right?

1087
00:50:42,994 --> 00:50:45,229
So what we do is,

1088
00:50:45,229 --> 00:50:47,669
we first apply
quantisation on the waves.

1089
00:50:47,669 --> 00:50:50,069
And then, uh, for the was,

1090
00:50:50,069 --> 00:50:51,209
assuming we are going to get the

1091
00:50:51,209 --> 00:50:53,069
gradients and their
gradients are here,

1092
00:50:53,069 --> 00:50:55,329
and we are going to
basically follow in

1093
00:50:55,329 --> 00:50:58,729
the um clustering
results on the was,

1094
00:50:58,729 --> 00:51:03,049
we are going to designate
the same color, um,

1095
00:51:03,049 --> 00:51:07,389
for each each each parameters
we gradient, okay?

1096
00:51:07,389 --> 00:51:10,049
And then we group
them together, okay?

1097
00:51:10,049 --> 00:51:12,070
And we generate a vector

1098
00:51:12,070 --> 00:51:15,049
that's trying to
update the codebook.

1099
00:51:15,049 --> 00:51:20,029
Okay. And we basically
reduce all these, um,

1100
00:51:20,029 --> 00:51:22,329
small values, produced
on each entry of

1101
00:51:22,329 --> 00:51:25,790
the original weight using
the emis castering results.

1102
00:51:25,790 --> 00:51:28,609
And, uh, this is basically
the update that we are

1103
00:51:28,609 --> 00:51:31,689
going to apply to the
codebook and same thing.

1104
00:51:31,689 --> 00:51:33,969
As as we update the parameters,

1105
00:51:33,969 --> 00:51:34,809
what do we do is we are going

1106
00:51:34,809 --> 00:51:35,949
to apply a learning rate and

1107
00:51:35,949 --> 00:51:38,310
we use this to update our
centro in the codebook.

1108
00:51:38,310 --> 00:51:40,509
So this centre is
basically the results of

1109
00:51:40,509 --> 00:51:42,869
the emis clattering.
Basically our codes.

1110
00:51:42,869 --> 00:51:44,469
We are going to update
codes a little bit.

1111
00:51:44,469 --> 00:51:46,430
So the how intuition

1112
00:51:46,430 --> 00:51:48,110
is basically as our
parameter being updated,

1113
00:51:48,110 --> 00:51:50,414
we are also updating
our cluster centros.

1114
00:51:50,414 --> 00:51:53,619
Okay. So basically, we'll

1115
00:51:53,619 --> 00:51:56,679
make sure that our condensation
still makes sense.

1116
00:51:56,679 --> 00:51:59,439
The key means we
still makes sense.

1117
00:51:59,659 --> 00:52:04,139
Okay. So how do this
work in practice.

1118
00:52:04,139 --> 00:52:08,039
Okay? And uh, people have
been applying this into,

1119
00:52:08,039 --> 00:52:09,659
say, conversion neural networks

1120
00:52:09,659 --> 00:52:11,579
and all kinds of
neural networks.

1121
00:52:11,579 --> 00:52:14,859
And as we can see, uh,

1122
00:52:14,859 --> 00:52:16,839
we can effectively
choose a lumber bits to

1123
00:52:16,839 --> 00:52:18,559
quantize and that will give

1124
00:52:18,559 --> 00:52:20,699
us a compression rate
for model weights.

1125
00:52:20,699 --> 00:52:23,779
And as we can see, this
is pretty powerful, okay?

1126
00:52:23,779 --> 00:52:27,379
So if we reduce the
lumber weights by

1127
00:52:27,379 --> 00:52:29,319
five times we roughly

1128
00:52:29,319 --> 00:52:32,360
we do not lose any
accuracy at inference.

1129
00:52:32,360 --> 00:52:35,279
Which is pretty good.
And if we're going

1130
00:52:35,279 --> 00:52:38,619
to reduce the lumber
Bs in the code book,

1131
00:52:38,619 --> 00:52:42,359
our accuracy is going to
decrease, at some point,

1132
00:52:42,359 --> 00:52:46,619
the accuracy will basically
decrease, very drastically.

1133
00:52:46,619 --> 00:52:48,159
But we don't want to we want

1134
00:52:48,159 --> 00:52:49,819
to basically touch this regime.

1135
00:52:49,819 --> 00:52:52,399
This becomes another
hyper primary tuning job,

1136
00:52:52,399 --> 00:52:53,659
basically in practice,

1137
00:52:53,659 --> 00:52:55,439
you want to tune this
number of codebooks to make

1138
00:52:55,439 --> 00:52:57,640
sure you find the largest,

1139
00:52:57,640 --> 00:52:59,239
uh, conversion rate, which

1140
00:52:59,239 --> 00:53:01,359
does not decrease your accuracy.

1141
00:53:01,359 --> 00:53:04,039
This also means that in
today's neural networks,

1142
00:53:04,039 --> 00:53:07,879
there are a lot of
space for compression.

1143
00:53:07,879 --> 00:53:10,639
Mony parameters are
quite repetive.

1144
00:53:10,639 --> 00:53:15,519
So that's why quantity works
pretty well. Any question?

1145
00:53:15,639 --> 00:53:19,619
Cool. We can also visualize

1146
00:53:19,619 --> 00:53:24,299
basically weight values
their histogram.

1147
00:53:24,299 --> 00:53:26,039
Before qdenation you can see

1148
00:53:26,039 --> 00:53:27,879
our weight value is
looking like this.

1149
00:53:27,879 --> 00:53:31,979
Xs is weight values
and the way discount.

1150
00:53:31,979 --> 00:53:34,419
There are a few things
you want to notice.

1151
00:53:34,419 --> 00:53:36,019
One is, before quantuation,

1152
00:53:36,019 --> 00:53:37,799
our values are
pretty continuous.

1153
00:53:37,799 --> 00:53:42,179
It's basically spread
across continuous space.

1154
00:53:42,179 --> 00:53:43,719
A second thing you
want to notice

1155
00:53:43,719 --> 00:53:47,119
that there are very
few values as zero.

1156
00:53:47,480 --> 00:53:51,159
Uh, this is basically
on new works.

1157
00:53:51,159 --> 00:53:53,299
After condon, like I said,

1158
00:53:53,299 --> 00:53:55,619
still remember my
teacher figure,

1159
00:53:55,619 --> 00:53:57,639
what we do is
basically we represent

1160
00:53:57,639 --> 00:54:01,379
all the continuous values
using the discrete values.

1161
00:54:01,379 --> 00:54:04,159
You can see there are
just a few values that

1162
00:54:04,159 --> 00:54:07,439
is used, compared
to a previous one.

1163
00:54:08,520 --> 00:54:11,539
If we use this kind of
trick to do training,

1164
00:54:11,539 --> 00:54:15,219
that is we update our code a
little bit, um we can see.

1165
00:54:15,219 --> 00:54:18,559
I'm going to rotate
between these two, right?

1166
00:54:18,559 --> 00:54:20,899
You can see the codes.

1167
00:54:20,899 --> 00:54:23,120
So basically the central
is slightly shift

1168
00:54:23,120 --> 00:54:25,499
after every iteration of update.

1169
00:54:25,499 --> 00:54:33,029
Okay. Cool. Okay. And like I
said, how many bits you use?

1170
00:54:33,029 --> 00:54:35,029
This is a hypergrimary
tuning, question.

1171
00:54:35,029 --> 00:54:37,469
So you have to figure
out the sweet spot

1172
00:54:37,469 --> 00:54:39,489
that best trade off between,

1173
00:54:39,489 --> 00:54:41,169
uh, compression rate and

1174
00:54:41,169 --> 00:54:42,870
your march learning
model accuracy.

1175
00:54:42,870 --> 00:54:47,129
So this basically is a result
from a very famous paper,

1176
00:54:47,129 --> 00:54:49,349
deep compression, which I
give to you as a reading.

1177
00:54:49,349 --> 00:54:52,669
Okay. That is the paper that
starts all this denation.

1178
00:54:52,669 --> 00:54:54,349
Okay. And I think the author of

1179
00:54:54,349 --> 00:54:56,329
the paper is basically
a professor at MIT.

1180
00:54:56,329 --> 00:54:59,329
He's very famous in
doing denation and he

1181
00:54:59,329 --> 00:55:02,309
has a company being acquired
by NVDA, by the way.

1182
00:55:02,309 --> 00:55:03,729
NVDA really likes him.

1183
00:55:03,729 --> 00:55:07,469
Yeah. And that deal is
pretty good, by the way.

1184
00:55:07,469 --> 00:55:11,019
Remember I told you that Okay.

1185
00:55:11,019 --> 00:55:14,019
The AutoML, basically the
guest speakers company

1186
00:55:14,019 --> 00:55:15,979
next week in search area.

1187
00:55:15,979 --> 00:55:17,679
Their company is called AutoML.

1188
00:55:17,679 --> 00:55:20,580
They were also
acquired by N media,

1189
00:55:20,580 --> 00:55:23,159
the creator of this graph,

1190
00:55:23,159 --> 00:55:26,639
condensation, his company
is also acquired by NMDia.

1191
00:55:26,639 --> 00:55:29,679
But this guy's company
acquisition deal

1192
00:55:29,679 --> 00:55:31,419
is much better than
Octo Mails deal.

1193
00:55:31,419 --> 00:55:33,359
Yeah, you can do some
Google search later.

1194
00:55:33,359 --> 00:55:37,539
Okay. Cool. The figure

1195
00:55:37,539 --> 00:55:38,399
here basically tells you

1196
00:55:38,399 --> 00:55:40,060
that you want to
trade off between,

1197
00:55:40,060 --> 00:55:42,379
uh, number of bits
you use and accuracy.

1198
00:55:42,379 --> 00:55:44,619
You want to tune
this a little bit.

1199
00:55:44,619 --> 00:55:46,979
Okay, so in practice,

1200
00:55:46,979 --> 00:55:51,000
how does this screen condidon
works, um in runtime?

1201
00:55:51,000 --> 00:55:53,799
What do we do is basically, um,

1202
00:55:53,799 --> 00:55:57,769
we start with tact
with R code book.

1203
00:55:57,769 --> 00:56:01,180
And every time when we
try to do computation,

1204
00:56:01,180 --> 00:56:04,679
uh, we still need to
decode the width.

1205
00:56:04,679 --> 00:56:07,399
Using our code book
and context like

1206
00:56:07,399 --> 00:56:10,819
a lookup table to get
the width, real width.

1207
00:56:10,819 --> 00:56:14,119
So remember this width is
a value from the code book

1208
00:56:14,119 --> 00:56:15,699
a code from the code
book and the code

1209
00:56:15,699 --> 00:56:17,719
is still in flowing point.

1210
00:56:17,719 --> 00:56:19,719
Which means that
we have to decode

1211
00:56:19,719 --> 00:56:23,799
our uh index into flowing
point values in the code book,

1212
00:56:23,799 --> 00:56:24,999
and then we perform

1213
00:56:24,999 --> 00:56:29,139
arithmetics in floating
point arithmetics.

1214
00:56:29,139 --> 00:56:30,579
That means that even if you

1215
00:56:30,579 --> 00:56:32,499
apply this kind of
chemit condensation,

1216
00:56:32,499 --> 00:56:35,234
you are still using your
floating point core.

1217
00:56:35,234 --> 00:56:40,209
Okay. Yeah. So to
summarize little bit,

1218
00:56:40,209 --> 00:56:42,189
so basically for
storage, we save a lot.

1219
00:56:42,189 --> 00:56:44,829
We basically we do

1220
00:56:44,829 --> 00:56:47,269
not have to save all the
original floating point values.

1221
00:56:47,269 --> 00:56:51,989
We basically save the
index, the index.

1222
00:56:51,989 --> 00:56:54,169
But for computer,
we still need to

1223
00:56:54,169 --> 00:56:55,909
decode at runtime
on the fly and we

1224
00:56:55,909 --> 00:56:57,409
try to call our floting point

1225
00:56:57,409 --> 00:57:00,330
course to calculate
results, okay?

1226
00:57:01,010 --> 00:57:03,829
Okay. That basically
fills this question.

1227
00:57:03,829 --> 00:57:06,070
So for storage, we are
going to do integer

1228
00:57:06,070 --> 00:57:11,189
with also we have a very small
floating point code book.

1229
00:57:11,189 --> 00:57:12,969
And for computer, we roll back

1230
00:57:12,969 --> 00:57:15,029
to use floating
point arithmetics.

1231
00:57:15,029 --> 00:57:20,449
Okay. Any question? Yeah.

1232
00:57:26,210 --> 00:57:28,849
Yeah. Very good point.

1233
00:57:28,849 --> 00:57:30,510
So in Kemin conversation,

1234
00:57:30,510 --> 00:57:31,910
what you face is basically

1235
00:57:31,910 --> 00:57:34,150
your compute will
be even slower.

1236
00:57:34,150 --> 00:57:39,710
Why? Because compared to the
original form of computer,

1237
00:57:39,710 --> 00:57:41,329
you add a lookup table,

1238
00:57:41,329 --> 00:57:42,669
and the lookup table will

1239
00:57:42,669 --> 00:57:44,310
basically slow down
your computer.

1240
00:57:44,310 --> 00:57:46,569
But the meaning is
basically, like I said,

1241
00:57:46,569 --> 00:57:49,290
the compression ratio for
storage is very aggressive.

1242
00:57:49,290 --> 00:57:52,370
So in many many devices,

1243
00:57:52,370 --> 00:57:54,690
if you have limited storage,

1244
00:57:54,690 --> 00:57:58,349
but you have okay compute,
you want to do this.

1245
00:57:58,349 --> 00:58:00,410
One example is on your iPhone.

1246
00:58:00,410 --> 00:58:02,329
Your iPhone has very
limited memory.

1247
00:58:02,329 --> 00:58:03,209
You want to write your model,

1248
00:58:03,209 --> 00:58:05,529
use a very limited memory, um,

1249
00:58:05,529 --> 00:58:07,629
uh but you're probably
okay with the power

1250
00:58:07,629 --> 00:58:10,109
and compute. So use
this one. Yeah.

1251
00:58:10,109 --> 00:58:11,690
You basically try to compress

1252
00:58:11,690 --> 00:58:13,819
your model very aggressively
into smaller model.

1253
00:58:13,819 --> 00:58:18,250
Yeah. Okay. Okay, that
is Kemi condition,

1254
00:58:18,250 --> 00:58:20,349
and this chems codon
is most powerful on

1255
00:58:20,349 --> 00:58:21,669
the last generation of

1256
00:58:21,669 --> 00:58:24,229
neural networks. So
basically cob ont.

1257
00:58:24,229 --> 00:58:27,050
And today, people play
with language models,

1258
00:58:27,050 --> 00:58:30,510
and that brings us into
this linear codinon.

1259
00:58:30,510 --> 00:58:32,490
And this near condition
is very powerful

1260
00:58:32,490 --> 00:58:33,469
because there are many ways that

1261
00:58:33,469 --> 00:58:34,770
we can do linear codiation.

1262
00:58:34,770 --> 00:58:37,589
But I'm not going
to dew too deep.

1263
00:58:37,589 --> 00:58:40,030
I'm going to give you a high
le intuition how this works.

1264
00:58:40,030 --> 00:58:41,710
And maybe in the next lecture,

1265
00:58:41,710 --> 00:58:43,329
I'm going to tell you what are

1266
00:58:43,329 --> 00:58:46,589
the common condition measures
we use for language model.

1267
00:58:46,589 --> 00:58:50,170
But essentially, they all
fall into linear codation.

1268
00:58:50,170 --> 00:58:53,569
Okay. So again, um,

1269
00:58:53,569 --> 00:58:56,369
we start with our
four by four matrix,

1270
00:58:56,369 --> 00:58:59,109
represented using 32 bits.

1271
00:58:59,109 --> 00:59:03,489
Okay. So the intuition we
need condition is this.

1272
00:59:03,489 --> 00:59:07,920
So So here, I'm trying to
apply linear quantitation,

1273
00:59:07,920 --> 00:59:09,200
and I try to quantize

1274
00:59:09,200 --> 00:59:11,779
my original weights in 32
bits into integer values.

1275
00:59:11,779 --> 00:59:15,380
My target output
value is integers.

1276
00:59:15,380 --> 00:59:17,759
But like I said, inar
quantity is pretty general.

1277
00:59:17,759 --> 00:59:19,519
You can also choose
different target values.

1278
00:59:19,519 --> 00:59:20,700
For example, you can quantize

1279
00:59:20,700 --> 00:59:23,820
from 32 bits to eight
bits of integer,

1280
00:59:23,820 --> 00:59:26,979
four bits of integer, or
you can quantize it into,

1281
00:59:26,979 --> 00:59:31,339
say, from LP 32 in LP 16
or P 32 into FV eight.

1282
00:59:31,339 --> 00:59:33,080
It depends on how you
choose your target.

1283
00:59:33,080 --> 00:59:35,399
But here, I give you
the most aggressive one

1284
00:59:35,399 --> 00:59:37,820
that is I want to quantize
it into integers.

1285
00:59:37,820 --> 00:59:40,620
And here, I want to
quantize it into basically,

1286
00:59:40,620 --> 00:59:42,859
uh the integer I
represented here,

1287
00:59:42,859 --> 00:59:45,319
two bit integers, four values.

1288
00:59:45,319 --> 00:59:48,290
Okay. So what I do is basically,

1289
00:59:48,290 --> 00:59:50,909
um, um, I try to

1290
00:59:50,909 --> 00:59:53,589
scale my original floating
point value into a new range.

1291
00:59:53,589 --> 00:59:55,249
And this new range
is determined by

1292
00:59:55,249 --> 00:59:57,690
my choose target range,

1293
00:59:57,690 --> 00:59:59,810
for example, in this example,

1294
00:59:59,810 --> 01:00:01,489
I just have these many values,

1295
01:00:01,489 --> 01:00:03,270
02 bits of integer.

1296
01:00:03,270 --> 01:00:05,670
And in order to scale
it, the equation

1297
01:00:05,670 --> 01:00:07,709
we try to follow is
basically a linear mapping.

1298
01:00:07,709 --> 01:00:11,489
That is, we try to basically
minus it by some value,

1299
01:00:11,489 --> 01:00:13,549
and we call this
value zero point.

1300
01:00:13,549 --> 01:00:17,329
Okay. And then we scale it
by a floating point value.

1301
01:00:17,329 --> 01:00:19,670
Okay. So basically,
we are skinning

1302
01:00:19,670 --> 01:00:21,789
original range into a new range.

1303
01:00:21,789 --> 01:00:24,370
And in the new range,
numbers are represented.

1304
01:00:24,370 --> 01:00:26,290
You need fewer choices.

1305
01:00:26,290 --> 01:00:30,030
Okay. You are probably
wondering how we determine,

1306
01:00:30,030 --> 01:00:32,929
um, this zero point
and also this scale,

1307
01:00:32,929 --> 01:00:35,269
we are going to figure
out pretty soon.

1308
01:00:35,269 --> 01:00:37,029
But before we figure
out the mass,

1309
01:00:37,029 --> 01:00:39,070
I want to properly characterize

1310
01:00:39,070 --> 01:00:42,349
this zero point and
scale a little bit.

1311
01:00:42,349 --> 01:00:45,710
Okay. So here, we can actually

1312
01:00:45,710 --> 01:00:50,130
express this skinning
process as this equation.

1313
01:00:50,130 --> 01:00:52,689
Um, I hold my original
value which is R,

1314
01:00:52,689 --> 01:00:54,390
R is at a high precision.

1315
01:00:54,390 --> 01:00:56,589
I hold my target value
which is quantat value,

1316
01:00:56,589 --> 01:00:59,580
which is Q. Q
stands for quantas.

1317
01:00:59,580 --> 01:01:03,809
And so my intuition that,

1318
01:01:03,809 --> 01:01:05,869
if I antact my
original value into Q,

1319
01:01:05,869 --> 01:01:08,189
I can still recover
the quantat value

1320
01:01:08,189 --> 01:01:09,289
back into the original value

1321
01:01:09,289 --> 01:01:10,910
so I can preserve the accuracy.

1322
01:01:10,910 --> 01:01:13,649
Okay. And the way

1323
01:01:13,649 --> 01:01:15,869
I recover is basically
following this uh,

1324
01:01:15,869 --> 01:01:18,549
linear transformation
that is I'm going to

1325
01:01:18,549 --> 01:01:22,409
first minus u bias term Z.

1326
01:01:22,409 --> 01:01:26,069
And basically represents
zero point in the new range.

1327
01:01:26,069 --> 01:01:27,889
Okay. And then once I

1328
01:01:27,889 --> 01:01:30,849
basically minus the
value from the Q,

1329
01:01:30,849 --> 01:01:34,034
I'm going to scale it back
to my original range.

1330
01:01:34,034 --> 01:01:40,039
Okay. Apparently, if you

1331
01:01:40,039 --> 01:01:41,900
want to apply this
near condition,

1332
01:01:41,900 --> 01:01:46,339
you have two critical parameters
to determine one is Z,

1333
01:01:46,339 --> 01:01:49,259
or what is zero point in
the new target range?

1334
01:01:49,259 --> 01:01:51,360
The other is the
skinning factor,

1335
01:01:51,360 --> 01:01:54,935
S. Which means that
for near condition,

1336
01:01:54,935 --> 01:01:58,190
Uh, you always carry
two parameters.

1337
01:01:58,190 --> 01:02:01,569
Okay? Because if you want to
apply this kind of air unit,

1338
01:02:01,569 --> 01:02:04,589
you have to carry
these two values.

1339
01:02:04,589 --> 01:02:08,269
So if you go into Hugging
phase and you check those, uh,

1340
01:02:08,269 --> 01:02:11,170
see, language models,
especially context version,

1341
01:02:11,170 --> 01:02:12,830
and you check their config.

1342
01:02:12,830 --> 01:02:15,049
Uh, in addition to
all those kind of,

1343
01:02:15,049 --> 01:02:16,609
like, lumbar heads, uh,

1344
01:02:16,609 --> 01:02:18,929
hidden dimensions, et cetera,

1345
01:02:18,929 --> 01:02:20,909
standard language
model configurations,

1346
01:02:20,909 --> 01:02:22,709
you probably will observe
these two values.

1347
01:02:22,709 --> 01:02:27,429
So contact point
and, um, and skill.

1348
01:02:27,429 --> 01:02:29,030
And for yourself, in the future,

1349
01:02:29,030 --> 01:02:31,509
you want to release your
model, in contact working,

1350
01:02:31,509 --> 01:02:32,809
you also need to release
these two values.

1351
01:02:32,809 --> 01:02:34,510
Otherwise, people
cannot reproduce

1352
01:02:34,510 --> 01:02:37,429
the results. Okay. Cool.

1353
01:02:37,429 --> 01:02:39,570
There are two
critical parameters.

1354
01:02:39,570 --> 01:02:41,669
One is zero point?
The other scale.

1355
01:02:41,669 --> 01:02:45,709
Okay. Like I said,

1356
01:02:45,709 --> 01:02:49,310
Q is integer here I choose
my target value as integer.

1357
01:02:49,310 --> 01:02:52,389
My original point, R
is floating point.

1358
01:02:52,389 --> 01:02:54,470
For the Z, this is basically

1359
01:02:54,470 --> 01:02:56,749
one quantity parameter because

1360
01:02:56,749 --> 01:02:58,469
it's a zero point
of a target range,

1361
01:02:58,469 --> 01:03:00,310
so it must be integer,

1362
01:03:00,310 --> 01:03:02,530
the meaning of this
Z is basically

1363
01:03:02,530 --> 01:03:04,329
we allow real number R,

1364
01:03:04,329 --> 01:03:06,169
which is on the left hand side,

1365
01:03:06,169 --> 01:03:08,909
which is equal to
zero be represented

1366
01:03:08,909 --> 01:03:11,989
by a antat integer Z
in the target range.

1367
01:03:11,989 --> 01:03:15,429
Okay. And then we screw it back,

1368
01:03:15,429 --> 01:03:18,449
and that gives us S.
S is a flowing point,

1369
01:03:18,449 --> 01:03:19,630
and that is the quantaing

1370
01:03:19,630 --> 01:03:21,689
parameter that we
screw the value back.

1371
01:03:21,689 --> 01:03:25,209
Okay. And geometrically
is basically this.

1372
01:03:25,209 --> 01:03:28,930
I'll let you look at
this for a few seconds.

1373
01:03:37,920 --> 01:03:40,160
So once you see this figure,

1374
01:03:40,160 --> 01:03:41,479
you will feel you

1375
01:03:41,479 --> 01:03:43,159
basically understand what
we are doing, right?

1376
01:03:43,159 --> 01:03:45,759
So basically, we are doing
some sort of normalization.

1377
01:03:45,759 --> 01:03:47,879
We try to normalize
value that is

1378
01:03:47,879 --> 01:03:50,499
in original range into a
smaller range target range.

1379
01:03:50,499 --> 01:03:52,939
Yeah. We try to
align a few things.

1380
01:03:52,939 --> 01:03:53,979
For example, we try to align

1381
01:03:53,979 --> 01:03:56,319
the mean value in
my original range,

1382
01:03:56,319 --> 01:03:58,099
the maximum value in
my original range

1383
01:03:58,099 --> 01:04:00,339
and the max in my target range.

1384
01:04:00,339 --> 01:04:02,359
We also try to align a
few things, for example,

1385
01:04:02,359 --> 01:04:05,140
zero point, between
these two ranges.

1386
01:04:05,140 --> 01:04:07,879
Okay. And for example,

1387
01:04:07,879 --> 01:04:09,480
if we do integer condensation,

1388
01:04:09,480 --> 01:04:12,540
um and if we choose
different bit of integers,

1389
01:04:12,540 --> 01:04:15,659
we will have different Qmax
in the target range, right?

1390
01:04:15,659 --> 01:04:18,080
This is to complement, okay?

1391
01:04:18,280 --> 01:04:20,680
Then you probably start
asking questions.

1392
01:04:20,680 --> 01:04:22,280
So if I have a tensor,

1393
01:04:22,280 --> 01:04:24,959
my tensor, how many
many different values?

1394
01:04:24,959 --> 01:04:28,800
How can I basically figure
out the parameters,

1395
01:04:28,800 --> 01:04:32,479
Z and S so that I can quantize
it into this target range.

1396
01:04:32,479 --> 01:04:35,720
For example, if you choose
probably four bit integer.

1397
01:04:35,720 --> 01:04:37,959
Okay. Let's do that. Okay.

1398
01:04:37,959 --> 01:04:40,839
So how do you determine S and Z?

1399
01:04:40,960 --> 01:04:43,659
So this is pretty simple, right?

1400
01:04:43,659 --> 01:04:45,599
We just need to play a
little bit in mathematics.

1401
01:04:45,599 --> 01:04:49,219
Okay. So we have
our original value,

1402
01:04:49,219 --> 01:04:51,700
which is, matrix,

1403
01:04:51,700 --> 01:04:53,839
and we try to figure out the
maximum value in a matrix,

1404
01:04:53,839 --> 01:04:56,294
for example, which is
RMAx and Rmin, right?

1405
01:04:56,294 --> 01:04:58,730
And we follow this
linear transformation,

1406
01:04:58,730 --> 01:05:04,649
we try to test it into the
new integer range and we

1407
01:05:04,649 --> 01:05:07,309
try to use the new value
in the new text value to

1408
01:05:07,309 --> 01:05:09,569
recover following this
linear transformation

1409
01:05:09,569 --> 01:05:10,889
back into the original value.

1410
01:05:10,889 --> 01:05:12,689
So that gives two
equations, right?

1411
01:05:12,689 --> 01:05:15,189
So RMAX in the original range

1412
01:05:15,189 --> 01:05:18,049
equals to S times
Qx minus Z, right?

1413
01:05:18,049 --> 01:05:20,649
That we try to align
the max value,

1414
01:05:20,649 --> 01:05:23,349
RM equals to times

1415
01:05:23,349 --> 01:05:25,109
Q Man min Z that is what

1416
01:05:25,109 --> 01:05:27,674
we try to align the
mean value. Okay.

1417
01:05:27,674 --> 01:05:31,099
And this is like a system
of equations, right?

1418
01:05:31,099 --> 01:05:33,480
You have two unknown

1419
01:05:33,480 --> 01:05:36,559
and you also have two
equations, so you can solve it.

1420
01:05:36,559 --> 01:05:38,839
And we just play a little bit.

1421
01:05:38,839 --> 01:05:40,560
We minus these two equations,

1422
01:05:40,560 --> 01:05:42,220
and we do some transformation,

1423
01:05:42,220 --> 01:05:45,040
we'll figure out S.
S is equal basically

1424
01:05:45,040 --> 01:05:48,339
RMAXRM divided by QMS, M's Cmin.

1425
01:05:48,339 --> 01:05:50,339
Basically from this
equation, you get it.

1426
01:05:50,339 --> 01:05:52,280
This is man addi, kind
of roman addition.

1427
01:05:52,280 --> 01:05:57,670
Okay. Yeah. Let's do a practice.

1428
01:05:57,670 --> 01:06:00,109
So, I have a matrix four by four

1429
01:06:00,109 --> 01:06:03,690
and my target range
is two bit integer.

1430
01:06:03,690 --> 01:06:06,550
In two bit integer, I
only have four values,

1431
01:06:06,550 --> 01:06:08,669
which I represent here,

1432
01:06:08,669 --> 01:06:10,729
minus two is mean value

1433
01:06:10,729 --> 01:06:13,849
and one is positive
one is the max value.

1434
01:06:13,849 --> 01:06:16,449
I want to use linear
condensation to

1435
01:06:16,449 --> 01:06:19,909
quantize this matrix into
this range, what I do?

1436
01:06:21,410 --> 01:06:25,950
So I first figure out
the R max and min,

1437
01:06:25,950 --> 01:06:28,010
right? From this matrix.

1438
01:06:28,010 --> 01:06:31,129
Okay? This matrix, um,

1439
01:06:31,129 --> 01:06:33,889
the maximum value
is 2.12, right?

1440
01:06:33,889 --> 01:06:37,670
The mean value is -1.08.

1441
01:06:37,670 --> 01:06:41,370
Okay. Then what I do is
basically I use this equation.

1442
01:06:41,370 --> 01:06:44,110
Okay. I substitute the values,

1443
01:06:44,110 --> 01:06:47,489
and also I figure out
and the mean value here,

1444
01:06:47,489 --> 01:06:52,970
and I get my S. My skinning
actor is essentially 1.07.

1445
01:06:52,970 --> 01:06:58,799
Okay. Pretty simple.
And same thing,

1446
01:06:58,799 --> 01:07:01,339
once we have this value,

1447
01:07:01,339 --> 01:07:03,019
we can figure out the Z value,

1448
01:07:03,019 --> 01:07:05,059
which is Z zero point.

1449
01:07:05,059 --> 01:07:06,739
What do we do is
basically we play

1450
01:07:06,739 --> 01:07:07,999
around this equation a little

1451
01:07:07,999 --> 01:07:11,999
bit and do some transformation
and we can figure out Z.

1452
01:07:11,999 --> 01:07:13,639
Because like I said, the Z

1453
01:07:13,639 --> 01:07:15,279
is zero point in
the target range.

1454
01:07:15,279 --> 01:07:18,580
And in my example, I
I choose the integer.

1455
01:07:18,580 --> 01:07:20,520
In my target range,
I only have integer,

1456
01:07:20,520 --> 01:07:22,119
so I have to take around here.

1457
01:07:22,119 --> 01:07:26,359
Okay. Cool. Similarly, we

1458
01:07:26,359 --> 01:07:28,960
can do exercise
for this example.

1459
01:07:28,960 --> 01:07:33,119
Um, I basically use this
equation where I try to figure

1460
01:07:33,119 --> 01:07:34,840
out Q mean and I minus

1461
01:07:34,840 --> 01:07:37,079
M divided by S from it
and I take a wrong.

1462
01:07:37,079 --> 01:07:40,439
Okay. So cumin is
minus two, right?

1463
01:07:40,439 --> 01:07:44,079
And I already know my
army is that value.

1464
01:07:44,079 --> 01:07:47,039
In my previous slide, I
figured out the value of

1465
01:07:47,039 --> 01:07:50,439
S. I basically substitute
them and I get my zero point,

1466
01:07:50,439 --> 01:07:52,419
which is minus one. Okay.

1467
01:07:52,419 --> 01:07:55,420
So therefore, here,
when we try to quantize

1468
01:07:55,420 --> 01:07:58,760
this given matrix into this
target range to be integer,

1469
01:07:58,760 --> 01:08:01,760
we basically use a
value where S equals

1470
01:08:01,760 --> 01:08:05,579
to 1.07 and Z equals
to minus one.

1471
01:08:05,579 --> 01:08:08,060
Okay? And if you quantize
your model parameters,

1472
01:08:08,060 --> 01:08:09,659
you also want to figure
out the values and you

1473
01:08:09,659 --> 01:08:11,899
write down these values
into your model.

1474
01:08:11,899 --> 01:08:15,239
Every time you recover
that from those values.

1475
01:08:16,060 --> 01:08:21,439
Okay. Then let's come to our
last part of this lecture.

1476
01:08:21,439 --> 01:08:27,379
So we want to apply near
condition to matma please.

1477
01:08:29,380 --> 01:08:32,939
If Yeah, that's possible.

1478
01:08:32,939 --> 01:08:35,459
Yeah. Yeah.

1479
01:08:38,100 --> 01:08:40,659
That's a really good
question. So that

1480
01:08:40,659 --> 01:08:43,599
is a part that we are going
to cover in the next lecture.

1481
01:08:43,599 --> 01:08:46,039
So that is called
posttraining condensation.

1482
01:08:46,039 --> 01:08:47,399
You have a few choices.

1483
01:08:47,399 --> 01:08:51,079
Okay? You can choose a
different SNZ for each tensor.

1484
01:08:51,079 --> 01:08:53,959
You can also choose a
different you can just choose

1485
01:08:53,959 --> 01:08:56,979
one constant SNZ
for entire model.

1486
01:08:56,979 --> 01:08:59,459
So this is called
calibration conation

1487
01:08:59,459 --> 01:09:01,219
You want to calibrate
the values of S

1488
01:09:01,219 --> 01:09:03,019
andZ depending on
how much you lose

1489
01:09:03,019 --> 01:09:05,599
on the codaon accuracy.

1490
01:09:05,599 --> 01:09:07,979
Okay? So apparently,

1491
01:09:07,979 --> 01:09:10,979
you can know that if you
do pretensal condensation,

1492
01:09:10,979 --> 01:09:12,899
you are going to have a
higher accuracy, right?

1493
01:09:12,899 --> 01:09:15,159
If you do one single SN Z

1494
01:09:15,159 --> 01:09:16,720
for all the ways and activations

1495
01:09:16,720 --> 01:09:18,619
for the entire network and

1496
01:09:18,619 --> 01:09:21,279
the entire input data, you are
going to lose more, right?

1497
01:09:21,279 --> 01:09:23,519
So that is a trade off,

1498
01:09:23,519 --> 01:09:26,539
so people can use
pretensior conduon or

1499
01:09:26,539 --> 01:09:28,839
entire model codenonO they

1500
01:09:28,839 --> 01:09:30,459
can figure out a
calibration dataset.

1501
01:09:30,459 --> 01:09:33,819
They just quantize depending
on the calibration data set.

1502
01:09:33,819 --> 01:09:35,569
Or they can use,

1503
01:09:35,569 --> 01:09:38,099
some aggressive ones, for
example, per channel.

1504
01:09:38,099 --> 01:09:41,539
I have a matrix where when
I'm meeting the channel,

1505
01:09:41,539 --> 01:09:44,859
I just try to figure out
S and Z for each channel.

1506
01:09:44,859 --> 01:09:47,139
I can even use per
group conidation.

1507
01:09:47,139 --> 01:09:48,579
That, I figure out, okay,

1508
01:09:48,579 --> 01:09:50,139
this group of value
they are pretty close,

1509
01:09:50,139 --> 01:09:53,359
so I should designate
SNZ for this group.

1510
01:09:53,359 --> 01:09:55,579
But for that group, they
are quite different.

1511
01:09:55,579 --> 01:09:59,519
So I disign different SNZ.
Does that make sense?

1512
01:09:59,519 --> 01:10:01,159
Apparently, when you do

1513
01:10:01,159 --> 01:10:02,759
more fine grain
condation you are

1514
01:10:02,759 --> 01:10:04,859
going to it's going
to be more complex.

1515
01:10:04,859 --> 01:10:08,829
Yeah. Okay. I think

1516
01:10:08,829 --> 01:10:11,149
that we'll cover that
briefly in the next lecture.

1517
01:10:11,149 --> 01:10:14,029
The thing I can tell
you is Fama models,

1518
01:10:14,029 --> 01:10:16,409
what do we do with
many popular condi

1519
01:10:16,409 --> 01:10:17,889
basically po tensor.

1520
01:10:17,889 --> 01:10:24,449
Yeah. Okay. Cool. Here,
we do la X that's okay.

1521
01:10:24,449 --> 01:10:27,069
We have Y equals to W and X,

1522
01:10:27,069 --> 01:10:29,549
and we want to apply
our condensation.

1523
01:10:29,549 --> 01:10:31,809
Remember, at the beginning
of denting lecture,

1524
01:10:31,809 --> 01:10:33,689
I said, you want to figure
out two things, right?

1525
01:10:33,689 --> 01:10:36,789
One is how you store data.
This one is pretty clear.

1526
01:10:36,789 --> 01:10:39,049
The way you store data is you

1527
01:10:39,049 --> 01:10:43,389
store the target range values,
which is to be integer.

1528
01:10:43,389 --> 01:10:48,164
You also store off SNZ depending
on which scheme you use.

1529
01:10:48,164 --> 01:10:50,419
The question is, what kind of

1530
01:10:50,419 --> 01:10:54,559
arithmetic course we use
to compute after conation.

1531
01:10:54,559 --> 01:10:56,879
So what do we do is, uh,

1532
01:10:56,879 --> 01:11:01,239
in the original Y equals to
X, we use floating point.

1533
01:11:01,239 --> 01:11:03,799
And now we apply condenation.

1534
01:11:03,799 --> 01:11:07,859
Now, assuming we do some kind
of a potential coddation

1535
01:11:07,859 --> 01:11:12,879
and we are going to get
SW and SX and w and ZX.

1536
01:11:12,879 --> 01:11:15,129
We basically subsue them.

1537
01:11:15,129 --> 01:11:18,600
Okay. And we do a little
bit transformation.

1538
01:11:18,600 --> 01:11:22,219
We find that our computation
based becomes this.

1539
01:11:22,219 --> 01:11:27,999
Here, QW and QX is the
quantized weight and input X,

1540
01:11:27,999 --> 01:11:30,459
and QY is the quantized output.

1541
01:11:30,459 --> 01:11:31,919
The reason we need quantize

1542
01:11:31,919 --> 01:11:33,039
output because this is the layer

1543
01:11:33,039 --> 01:11:35,459
of neural work and that
proceed to next layer.

1544
01:11:35,459 --> 01:11:38,739
Okay? And we do

1545
01:11:38,739 --> 01:11:41,799
a little bit like manipulation
and we get this equation.

1546
01:11:41,799 --> 01:11:44,279
Okay? Let's try to
inspect this equation

1547
01:11:44,279 --> 01:11:46,859
a little bit. Okay.

1548
01:11:46,859 --> 01:11:49,219
So the first part we
notice that for this part,

1549
01:11:49,219 --> 01:11:50,839
we can pre compute it because

1550
01:11:50,839 --> 01:11:53,399
the weight is fixed,
spit inference.

1551
01:11:53,399 --> 01:11:56,679
So we can pre compute
how we tact the weight.

1552
01:11:56,679 --> 01:11:59,159
And for X, we can
also, for example,

1553
01:11:59,159 --> 01:12:01,839
if we do calibration data
set, we can take a dataset.

1554
01:12:01,839 --> 01:12:04,559
Right. We assume we
are going to use

1555
01:12:04,559 --> 01:12:05,979
the same skill and zero point

1556
01:12:05,979 --> 01:12:08,199
for all the data in the dataset.

1557
01:12:08,199 --> 01:12:10,759
Okay. Then using
calibration data set,

1558
01:12:10,759 --> 01:12:13,619
we can basically
figure out a value

1559
01:12:13,619 --> 01:12:16,339
or the basically ZX Right.

1560
01:12:16,339 --> 01:12:19,159
Then with calibrating data
set and with fixed weights,

1561
01:12:19,159 --> 01:12:21,379
we basically figure out that all

1562
01:12:21,379 --> 01:12:25,559
the values in the
orange box, y constant.

1563
01:12:25,559 --> 01:12:28,739
They are not dependent
on the input.

1564
01:12:28,739 --> 01:12:31,119
So we basically
pre compute them.

1565
01:12:31,119 --> 01:12:34,479
Okay. So this is not trouble.

1566
01:12:34,479 --> 01:12:36,679
Okay, they are pretty computed.

1567
01:12:36,679 --> 01:12:38,619
And we ask the question.

1568
01:12:38,619 --> 01:12:42,979
So basically, what it takes
to compute these values.

1569
01:12:43,740 --> 01:12:46,299
So here, si remember,

1570
01:12:46,299 --> 01:12:47,859
it is a zero value
in the target range,

1571
01:12:47,859 --> 01:12:49,439
so it's integer and

1572
01:12:49,439 --> 01:12:53,259
QW is the quantitive value,
so it's also integer.

1573
01:12:53,259 --> 01:12:55,719
In order to perform
the computing here,

1574
01:12:55,719 --> 01:12:57,919
we only need integer course.

1575
01:12:57,919 --> 01:13:00,339
Which is good
because like I said,

1576
01:13:00,339 --> 01:13:02,559
integer course are
pretty high flops

1577
01:13:02,559 --> 01:13:04,779
compared to floating
point course, okay?

1578
01:13:04,779 --> 01:13:07,259
Meanwhile, we are going to
accumulate these values.

1579
01:13:07,259 --> 01:13:09,379
We have addition
subtraction, right?

1580
01:13:09,379 --> 01:13:11,499
And this addition subtraction

1581
01:13:11,499 --> 01:13:13,459
are also performed
only integer range.

1582
01:13:13,459 --> 01:13:16,119
So we can still use integer
course, which is good.

1583
01:13:16,119 --> 01:13:20,219
Okay. And in practice,
usually, uh, here,

1584
01:13:20,219 --> 01:13:22,559
we use a bit integer to
represent the value,

1585
01:13:22,559 --> 01:13:24,579
but when we do
additional subtraction,

1586
01:13:24,579 --> 01:13:27,779
what we do is we use 32
bit to avoid overflow.

1587
01:13:27,779 --> 01:13:35,739
Okay. Any question? Yeah,
QW is the weight, right?

1588
01:13:35,739 --> 01:13:37,559
Here we talk about inference.

1589
01:13:37,559 --> 01:13:39,659
In inference, we don't
change the weights,

1590
01:13:39,659 --> 01:13:42,399
we can pre compute the weight
value. Contact with value.

1591
01:13:42,399 --> 01:13:47,139
Okay. Okay. Then we
look at this value,

1592
01:13:47,139 --> 01:13:49,599
SW time X divided by Y.

1593
01:13:49,599 --> 01:13:53,779
You still remember, this
value is a floating point,

1594
01:13:53,779 --> 01:13:55,899
It's a scale in the
original range.

1595
01:13:55,899 --> 01:13:57,439
And this is bad

1596
01:13:57,439 --> 01:14:01,979
because if we directly
computer this value,

1597
01:14:01,979 --> 01:14:03,639
we are going to call
floating point course.

1598
01:14:03,639 --> 01:14:05,399
That defeats the
purpose of denation

1599
01:14:05,399 --> 01:14:07,339
because in some scenario,

1600
01:14:07,339 --> 01:14:10,159
my codiation is that I don't
have floating point course,

1601
01:14:10,159 --> 01:14:12,559
I just want to do
everything integal course.

1602
01:14:12,559 --> 01:14:17,639
How I do this.
Empirical many nerNtw

1603
01:14:17,639 --> 01:14:20,659
this value is in a
range of zero and one,

1604
01:14:20,659 --> 01:14:22,659
because it's a skinning factor.

1605
01:14:22,659 --> 01:14:25,739
What people do is a
very dirty solution.

1606
01:14:25,739 --> 01:14:27,559
So instead of directly

1607
01:14:27,559 --> 01:14:29,559
calling floating point course
to calculate this value,

1608
01:14:29,559 --> 01:14:31,409
what they do is basically uh,

1609
01:14:31,409 --> 01:14:34,669
they use some fixed point
multiplication plus bit shift.

1610
01:14:34,669 --> 01:14:39,149
This price represents
value using power two.

1611
01:14:39,149 --> 01:14:40,909
Of course, the power
is going to be

1612
01:14:40,909 --> 01:14:45,049
a super negative value
times another fixed point,

1613
01:14:45,049 --> 01:14:47,049
um, like a fraction,

1614
01:14:47,049 --> 01:14:49,309
which is around
you half and one.

1615
01:14:49,309 --> 01:14:52,309
And the reason they do this
is because fixed point is

1616
01:14:52,309 --> 01:14:55,489
computation is much much
cheaper than floating point.

1617
01:14:55,489 --> 01:14:57,529
You don't have to touch
floating point course.

1618
01:14:57,529 --> 01:14:59,029
Okay? And also bit shift

1619
01:14:59,029 --> 01:15:00,749
is much cheaper, you
just ship the bits.

1620
01:15:00,749 --> 01:15:02,469
Yeah. Uh, you ship the bits

1621
01:15:02,469 --> 01:15:04,489
in fixed point, which
is pretty easy.

1622
01:15:04,489 --> 01:15:06,129
So they use this kind of like

1623
01:15:06,129 --> 01:15:08,584
a trick to approximate
the value.

1624
01:15:08,584 --> 01:15:14,359
Okay. Then we look at
this value, right.

1625
01:15:14,359 --> 01:15:16,819
So this value is W times QX,

1626
01:15:16,819 --> 01:15:20,459
and the QX is apparently
dependent on input X, right?

1627
01:15:20,459 --> 01:15:22,519
It has to be computed, uh,

1628
01:15:22,519 --> 01:15:24,799
just on a fly.

1629
01:15:24,799 --> 01:15:30,299
W is the zero point
of, uh, the weight.

1630
01:15:30,299 --> 01:15:33,879
Okay? So what we do is, um, uh,

1631
01:15:33,879 --> 01:15:36,834
we observe that many many
neural networks, um, uh,

1632
01:15:36,834 --> 01:15:38,409
they are basically
their value are

1633
01:15:38,409 --> 01:15:40,749
basically following
Gaussian distribution.

1634
01:15:40,749 --> 01:15:42,789
That is, they have a zero me,

1635
01:15:42,789 --> 01:15:46,489
especially a training
raw inference.

1636
01:15:46,489 --> 01:15:48,429
So instead of
calculating this value,

1637
01:15:48,429 --> 01:15:49,409
how about we just describe

1638
01:15:49,409 --> 01:15:51,309
this value because
they have a zero me,

1639
01:15:51,309 --> 01:15:53,729
which means that when
we try to quantat it,

1640
01:15:53,729 --> 01:15:56,909
we can also quantat in a range
where we align zero point.

1641
01:15:56,909 --> 01:15:59,209
So the zero point at the
target range should also

1642
01:15:59,209 --> 01:16:01,809
be zero in the original range.

1643
01:16:01,809 --> 01:16:03,769
So here we can assume that

1644
01:16:03,769 --> 01:16:07,269
DW equals to zero, so
we can basically im.

1645
01:16:07,269 --> 01:16:10,509
Okay. I per, is because of zero,

1646
01:16:10,509 --> 01:16:12,309
we try to simplify
it a little bit.

1647
01:16:12,309 --> 01:16:14,249
Of course, this is not
accurate, by the way,

1648
01:16:14,249 --> 01:16:15,949
because tuning is between
not accurate, right?

1649
01:16:15,949 --> 01:16:17,169
You're going to lose accuracy,

1650
01:16:17,169 --> 01:16:19,909
but it's fine as long as you
don't lose too much, yeah.

1651
01:16:19,909 --> 01:16:24,669
Yeah. And then if we substute
this EW equal to zero,

1652
01:16:24,669 --> 01:16:26,229
and we basically get
this equation, right.

1653
01:16:26,229 --> 01:16:28,309
So you can see,

1654
01:16:28,309 --> 01:16:29,949
all the other parts
have been figured out.

1655
01:16:29,949 --> 01:16:31,569
The only heavy lifting
part is basically

1656
01:16:31,569 --> 01:16:34,049
this part, QW times QX.

1657
01:16:34,049 --> 01:16:37,829
So what is this? Remember Q and

1658
01:16:37,829 --> 01:16:39,869
QX are basically the repent of

1659
01:16:39,869 --> 01:16:42,249
the original W and X
at my target range.

1660
01:16:42,249 --> 01:16:44,089
And in my example is integers.

1661
01:16:44,089 --> 01:16:46,029
So basically this QW and X are

1662
01:16:46,029 --> 01:16:48,629
the quantit integer reptation
of the weight and X.

1663
01:16:48,629 --> 01:16:50,729
And if we tape these two
together is basically

1664
01:16:50,729 --> 01:16:54,669
performing met mol
in integer course.

1665
01:16:54,669 --> 01:16:56,289
So here we are using integer

1666
01:16:56,289 --> 01:16:58,509
courts to do the
highway lifting.

1667
01:16:58,509 --> 01:17:02,369
That basically
satisfies our goal.

1668
01:17:02,369 --> 01:17:04,409
Our goal is to reduce,

1669
01:17:04,409 --> 01:17:06,729
on one hand, we try to
reduce the storage.

1670
01:17:06,729 --> 01:17:08,469
On the other hand,
we try to reduce

1671
01:17:08,469 --> 01:17:10,389
the previous floating
point arithmetics

1672
01:17:10,389 --> 01:17:12,569
into integer arithmetics.

1673
01:17:12,569 --> 01:17:15,349
Because like I said, integer
arithmetic is much faster,

1674
01:17:15,349 --> 01:17:17,089
much cheaper, save
a lot of energy.

1675
01:17:17,089 --> 01:17:19,999
Okay. Cool. That basically

1676
01:17:19,999 --> 01:17:22,439
give you a high level intuition,

1677
01:17:22,439 --> 01:17:26,119
what basically this kind
of near quarantine does.

1678
01:17:26,119 --> 01:17:27,999
Like I said at the
beginning, uh,

1679
01:17:27,999 --> 01:17:29,879
you can choose different
target ranges.

1680
01:17:29,879 --> 01:17:33,139
So here, if you chose
target range as LP eight,

1681
01:17:33,139 --> 01:17:35,479
you basically reduced to

1682
01:17:35,479 --> 01:17:38,499
the equation where this QW
and Qx are in LP eight.

1683
01:17:38,499 --> 01:17:40,139
Which means that you can reduce

1684
01:17:40,139 --> 01:17:44,519
the original LP 16 arithmetics
into LP eight arithmetics,

1685
01:17:44,519 --> 01:17:47,019
and that is basically
what DP sig does.

1686
01:17:47,019 --> 01:17:49,854
Target range being P eight.

1687
01:17:49,854 --> 01:17:54,929
Cool. Okay, this slides

1688
01:17:54,929 --> 01:17:57,709
give you a sense how this
performs in practice.

1689
01:17:57,709 --> 01:18:00,529
Indeed, if you do this
kind of near condition,

1690
01:18:00,529 --> 01:18:02,649
you can still preserve
much accuracy and

1691
01:18:02,649 --> 01:18:06,089
because compared to Kems codon,

1692
01:18:06,089 --> 01:18:07,789
remember in Kemi Cnation you are

1693
01:18:07,789 --> 01:18:09,829
slowing down the compute, right?

1694
01:18:09,829 --> 01:18:12,069
But in near connection,
not necessarily.

1695
01:18:12,069 --> 01:18:14,049
Sometimes you can
accelerate the computer

1696
01:18:14,049 --> 01:18:17,059
by leveraging lower
pricing course, okay?

1697
01:18:17,059 --> 01:18:20,949
Okay, that basically comes
to end of this lecture.

1698
01:18:20,949 --> 01:18:22,389
Let's do a quick summary.

1699
01:18:22,389 --> 01:18:25,209
For Nina condition, uh, uh,

1700
01:18:25,209 --> 01:18:26,769
if our target range
are integers,

1701
01:18:26,769 --> 01:18:27,969
we are basically represent

1702
01:18:27,969 --> 01:18:29,989
the data using integers, right.

1703
01:18:29,989 --> 01:18:31,589
We try to use our skill and

1704
01:18:31,589 --> 01:18:34,449
point to recover integer
into the target range.

1705
01:18:34,449 --> 01:18:36,569
And for the arithmetics,

1706
01:18:36,569 --> 01:18:39,389
we are able to
simplify the little

1707
01:18:39,389 --> 01:18:42,189
bit and eventually we'll
be able to integer course,

1708
01:18:42,189 --> 01:18:44,789
and integer course are much
better than phone course.

1709
01:18:44,789 --> 01:18:46,589
Okay. Cool. Thank you.

1710
01:18:46,589 --> 01:18:48,029
I'll see you on hasday on Zoom.

1711
01:18:48,029 --> 01:18:50,229
You don't have to be
here, go to Zoom, okay?

1712
01:18:50,229 --> 01:18:51,509
Yeah.

1713
01:21:14,939 --> 01:21:16,979
H
