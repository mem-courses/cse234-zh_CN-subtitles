1
00:00:04,920 --> 00:00:08,719
Okay, uh, yeah,
thanks for coming.

2
00:00:08,719 --> 00:00:11,540
Let's Let's get
started, but today.

3
00:00:11,540 --> 00:00:18,060
Yeah. Yeah, so let's do
a recap of last lecture.

4
00:00:18,500 --> 00:00:21,839
So last lecture, we
talk about GPUs,

5
00:00:21,839 --> 00:00:24,520
right, and Olympic Coda.

6
00:00:24,520 --> 00:00:28,660
So what we learned is
the GPU execution model,

7
00:00:28,660 --> 00:00:31,999
where we know GPU has
thread hierarchy,

8
00:00:31,999 --> 00:00:34,899
from grade to block to threads.

9
00:00:34,899 --> 00:00:37,700
And when you write the GPO code,

10
00:00:37,700 --> 00:00:40,259
we call it a GPO
kernel and the way we

11
00:00:40,259 --> 00:00:42,179
basically write that
code is basically we

12
00:00:42,179 --> 00:00:44,599
launch that kernel to many
many threads through this,

13
00:00:44,599 --> 00:00:48,220
uh thread hierarchy, okay?

14
00:00:48,640 --> 00:00:52,840
And we know GPU has a
distinct address space

15
00:00:52,840 --> 00:00:54,340
compared to the host memory,

16
00:00:54,340 --> 00:00:58,120
and we call that GPU
memory or device memory

17
00:00:58,120 --> 00:01:00,460
or HBM, high bandwidth memory.

18
00:01:00,460 --> 00:01:02,839
And in order for

19
00:01:02,839 --> 00:01:04,880
your program to access
this part of memory,

20
00:01:04,880 --> 00:01:07,580
Coda provides you some APS,

21
00:01:07,580 --> 00:01:12,300
like Mock Coda memo copy,
something like that. Okay.

22
00:01:12,300 --> 00:01:16,579
And there's still remember
the definition of pin memory,

23
00:01:16,579 --> 00:01:18,000
pin memory is a part of

24
00:01:18,000 --> 00:01:21,679
the host memory that
is reserved for GPUs.

25
00:01:25,020 --> 00:01:29,419
On the inside of GPO,
besides the HBM,

26
00:01:29,419 --> 00:01:31,699
we also have other
memory hierarchy built

27
00:01:31,699 --> 00:01:34,360
into the GPO itself.

28
00:01:34,360 --> 00:01:37,060
We have per thread,
private memory.

29
00:01:37,060 --> 00:01:39,459
For example, when we
write a GPO kernel,

30
00:01:39,459 --> 00:01:41,879
when we try to ask for
temporal variable,

31
00:01:41,879 --> 00:01:43,519
that variable was
basically created

32
00:01:43,519 --> 00:01:45,959
in thread local memory.

33
00:01:45,959 --> 00:01:47,859
And we have per block memory,

34
00:01:47,859 --> 00:01:49,720
which we call SRAM, right?

35
00:01:49,720 --> 00:01:52,619
All the threads in one
block can access memory.

36
00:01:52,619 --> 00:01:54,600
And the way we
access that memory

37
00:01:54,600 --> 00:01:56,979
is we use prefix called shared,

38
00:01:56,979 --> 00:01:59,520
under square shared, okay?

39
00:01:59,520 --> 00:02:01,439
And like I said, we have HBM,

40
00:02:01,439 --> 00:02:04,579
which is the global
device level memory.

41
00:02:04,580 --> 00:02:09,539
And we also touch base on two
synchronization primitives.

42
00:02:09,539 --> 00:02:12,399
One is the synchronization
between CPU and GPU.

43
00:02:12,399 --> 00:02:14,640
The other is the
synchronization between

44
00:02:14,640 --> 00:02:17,940
all the threads in
one block, okay?

45
00:02:18,300 --> 00:02:22,300
And we also finish our
first GPU program,

46
00:02:22,300 --> 00:02:24,099
which is the Window average or

47
00:02:24,099 --> 00:02:26,750
equivalent com one D, okay?

48
00:02:26,750 --> 00:02:30,079
Cool. Okay. Today's
learning goal,

49
00:02:30,079 --> 00:02:32,499
we are going to grind
the math mo on GPU.

50
00:02:32,499 --> 00:02:35,359
We'll probably spend half
of the class on that.

51
00:02:35,920 --> 00:02:39,620
Then we basically finish
our GPU programming class.

52
00:02:39,620 --> 00:02:42,599
Okay. And we're going to
talk about how we can there

53
00:02:42,599 --> 00:02:44,379
are some other
alternative methods

54
00:02:44,379 --> 00:02:45,820
to make operator faster,

55
00:02:45,820 --> 00:02:47,340
which is basically compiler.

56
00:02:47,340 --> 00:02:48,920
So we are going to
talk about compiler

57
00:02:48,920 --> 00:02:50,540
a little bit, okay.

58
00:02:50,540 --> 00:02:53,640
Um, yeah. And if time permits,

59
00:02:53,640 --> 00:02:55,639
we are going to basically wrap

60
00:02:55,639 --> 00:02:57,879
up this operator
augmentation layer,

61
00:02:57,879 --> 00:02:59,479
and we are going to
touch another layer

62
00:02:59,479 --> 00:03:01,065
which is graph ogenation.

63
00:03:01,065 --> 00:03:05,430
Cool. Okay. So before

64
00:03:05,430 --> 00:03:09,309
we get deep into Koda
programming on met,

65
00:03:09,309 --> 00:03:11,549
we try to basically, uh,

66
00:03:11,549 --> 00:03:15,729
change your way your normal
way of programming, right?

67
00:03:15,729 --> 00:03:18,489
So before we try to
program some Koda,

68
00:03:18,489 --> 00:03:20,409
we need to do a few things.

69
00:03:20,409 --> 00:03:22,389
So you need to
convert your brain

70
00:03:22,389 --> 00:03:24,990
to SIMD because Koda is SMD.

71
00:03:24,990 --> 00:03:27,690
Okay. So whenever I give

72
00:03:27,690 --> 00:03:31,209
you say operator and I
ask you to write a Koda,

73
00:03:31,209 --> 00:03:33,350
you should think
about three steps.

74
00:03:33,350 --> 00:03:35,449
The first one is
you should identify

75
00:03:35,449 --> 00:03:38,289
the work that can be
performed in parallel, right?

76
00:03:38,289 --> 00:03:40,179
And then you partition the work.

77
00:03:40,179 --> 00:03:41,969
And also you need to partition

78
00:03:41,969 --> 00:03:43,869
the data associated
with our work, right?

79
00:03:43,869 --> 00:03:45,909
And then you map
the partition and

80
00:03:45,909 --> 00:03:49,009
the data partition to
Koda thread, right?

81
00:03:49,009 --> 00:03:51,370
And you implement the
logic in that oda kernel.

82
00:03:51,370 --> 00:03:52,989
And GPU will basically

83
00:03:52,989 --> 00:03:55,030
launch your code onto
many many threads, okay?

84
00:03:55,030 --> 00:03:58,510
That's the way how you think
when you program Coda.

85
00:03:58,520 --> 00:04:01,460
And in order to make
this program fast,

86
00:04:01,460 --> 00:04:04,060
I hope you still
remember how we improve

87
00:04:04,060 --> 00:04:07,479
our CdI programming for
the C D last class, right?

88
00:04:07,479 --> 00:04:11,599
So first, we need to make sure
we oversubscribe the GPUs.

89
00:04:11,599 --> 00:04:14,159
We create a lot of
partition work,

90
00:04:14,159 --> 00:04:15,759
and we try to push all of them

91
00:04:15,759 --> 00:04:19,139
into the limited number
of blocks on GPU,

92
00:04:19,139 --> 00:04:21,340
That is called oversubscription.

93
00:04:21,340 --> 00:04:23,440
We also need to
mitigate stragglers.

94
00:04:23,440 --> 00:04:25,160
We don't want to we want to

95
00:04:25,160 --> 00:04:27,139
part workload as
equal as possible.

96
00:04:27,139 --> 00:04:29,620
So every thread roughly get
the same workload, right?

97
00:04:29,620 --> 00:04:32,100
So we don't want to write
those kind of ELs, right,

98
00:04:32,100 --> 00:04:34,140
because some thread
will get work,

99
00:04:34,140 --> 00:04:36,314
but other threads are not, okay?

100
00:04:36,314 --> 00:04:38,430
And we need to minimize
communication.

101
00:04:38,430 --> 00:04:40,930
And here, communication
means we need to

102
00:04:40,930 --> 00:04:44,450
minimize the data movement
between memory hierarchies,

103
00:04:44,450 --> 00:04:46,770
we should use the shared
memory as much as possible.

104
00:04:46,770 --> 00:04:49,489
Okay? But subject to
their constraints, okay?

105
00:04:49,489 --> 00:04:52,370
Cool. Then let's get started.

106
00:04:52,370 --> 00:04:54,510
So we're going to do this,

107
00:04:54,510 --> 00:04:55,790
right, S equals to A,

108
00:04:55,790 --> 00:04:57,950
Mtmo B and this

109
00:04:57,950 --> 00:04:59,369
basically give you visualizing

110
00:04:59,369 --> 00:05:01,210
what is Mtmois doing, right?

111
00:05:01,210 --> 00:05:05,190
And let's follow our
three steps, right?

112
00:05:05,190 --> 00:05:06,609
Let's try to first identify

113
00:05:06,609 --> 00:05:08,429
work that can be
performed in parallel.

114
00:05:08,429 --> 00:05:11,790
So what is the work that can
be performed in parallel?

115
00:05:14,010 --> 00:05:18,310
So basically, every time
we can grab a row on A and

116
00:05:18,310 --> 00:05:20,190
a column on B right and we

117
00:05:20,190 --> 00:05:22,449
just do a dot product
between row and columns.

118
00:05:22,449 --> 00:05:25,550
So we basically can get
one result entry in C,

119
00:05:25,550 --> 00:05:28,449
and all this row
and column product

120
00:05:28,449 --> 00:05:30,030
can be done in parallel.

121
00:05:30,030 --> 00:05:32,750
So the most straightforward
implementation for

122
00:05:32,750 --> 00:05:35,710
this kernel is basically we
do what I just described.

123
00:05:35,710 --> 00:05:39,370
Each GPU thread to grab one
row and one column, right?

124
00:05:39,370 --> 00:05:42,570
And then we launch them to
many many blocks with threads.

125
00:05:42,570 --> 00:05:46,499
Okay? And in order to
write that kernel, uh,

126
00:05:46,499 --> 00:05:48,580
what do we do is, we

127
00:05:48,580 --> 00:05:50,980
basically each thread
compte one element,

128
00:05:50,980 --> 00:05:53,819
and we ended up
with this kernel.

129
00:05:53,819 --> 00:05:58,759
And please look at that
for maybe 15 seconds.

130
00:06:14,400 --> 00:06:17,840
Okay, yeah, let's try
to go through it.

131
00:06:19,640 --> 00:06:22,220
So this one is
three, four, right?

132
00:06:22,220 --> 00:06:25,260
We have matrix we have to

133
00:06:25,260 --> 00:06:29,859
matrix which is the dimension
N here N is roughly 1024.

134
00:06:29,859 --> 00:06:33,399
And we are going to
ask for a few blocks.

135
00:06:33,399 --> 00:06:38,520
Here we ask blocks of the
shape of 32 by 32, right?

136
00:06:38,520 --> 00:06:41,819
Two dimensional blocks because
this one is trivial, okay?

137
00:06:41,819 --> 00:06:43,640
And because we are going to let

138
00:06:43,640 --> 00:06:45,579
each thread to computer
one element, so in total,

139
00:06:45,579 --> 00:06:49,279
we need to ask for 1024 threads.

140
00:06:49,279 --> 00:06:52,480
That's why thread per block
is also this shape. Okay.

141
00:06:52,480 --> 00:06:56,180
And this is our
kernel launch, right?

142
00:06:56,180 --> 00:06:59,499
This is CPU code. We ask CPU
to launch the kernel to GPS.

143
00:06:59,499 --> 00:07:02,720
And then we go into
this kernel, um,

144
00:07:02,720 --> 00:07:05,459
and what do we do is
we use the block ID,

145
00:07:05,459 --> 00:07:10,239
block D and thread ID to
get index for each thread.

146
00:07:10,239 --> 00:07:12,640
Here, we let the
X to take care of

147
00:07:12,640 --> 00:07:17,339
the row from the A left matrix,

148
00:07:17,339 --> 00:07:20,600
and we use Y to index
the column of B, right?

149
00:07:20,600 --> 00:07:23,660
And then what do we do is we

150
00:07:23,660 --> 00:07:25,679
create one variable per thread.

151
00:07:25,679 --> 00:07:29,629
So this variable we'll use
the the thread local memory,

152
00:07:29,629 --> 00:07:32,029
right, because we don't
use the shared prefix.

153
00:07:32,029 --> 00:07:34,529
So this will be allocated
on thread local memory,

154
00:07:34,529 --> 00:07:37,289
which is essentially
registers, okay?

155
00:07:37,289 --> 00:07:40,169
And then we loop over, we board

156
00:07:40,169 --> 00:07:43,210
this dimension, right? Okay.

157
00:07:43,210 --> 00:07:45,589
And what do we do is we
will grab one element,

158
00:07:45,589 --> 00:07:48,450
it them together, and we

159
00:07:48,450 --> 00:07:52,150
will accumulate results
into this result, right.

160
00:07:52,150 --> 00:07:54,130
And eventually we this thread

161
00:07:54,130 --> 00:07:56,289
to write the result back into

162
00:07:56,289 --> 00:08:01,550
our result array C. Okay?
Makes sense, right?

163
00:08:01,550 --> 00:08:04,870
So basically, we launch
one K threadach thread

164
00:08:04,870 --> 00:08:06,669
taking care of one element.

165
00:08:06,669 --> 00:08:15,100
Okay. Okay. So let's try
to analyze the memory,

166
00:08:15,100 --> 00:08:16,940
the IO, right, because
we care about that.

167
00:08:16,940 --> 00:08:19,980
Okay. So the global
memory rate per thread is

168
00:08:19,980 --> 00:08:24,040
basically each thread need to
read one row and one colom.

169
00:08:24,040 --> 00:08:26,320
So it's basically two N, right?

170
00:08:26,320 --> 00:08:29,479
So so how many threads,

171
00:08:29,479 --> 00:08:32,339
I already said is
unsquare threads, right?

172
00:08:32,339 --> 00:08:33,760
Because that thread should be

173
00:08:33,760 --> 00:08:36,700
corresponding to the
shape of the result,

174
00:08:36,700 --> 00:08:39,240
number of elements
in the array, okay?

175
00:08:39,240 --> 00:08:41,639
So the total memory access is

176
00:08:41,639 --> 00:08:44,639
essentially unsquare
times two, right?

177
00:08:44,639 --> 00:08:46,700
Is cube.

178
00:08:46,700 --> 00:08:49,540
Okay, that means in order
to launch this kerdle,

179
00:08:49,540 --> 00:08:51,019
we need to read
the global memory,

180
00:08:51,019 --> 00:08:52,660
read the content from HBM

181
00:08:52,660 --> 00:08:56,420
to register this
many times, okay?

182
00:08:56,500 --> 00:09:01,520
And how many memory will
use thread local memory?

183
00:09:01,520 --> 00:09:05,799
So basically, we only use
one float per thread, right.

184
00:09:05,799 --> 00:09:09,060
Okay. That is the basic
version of Mtmo kernel.

185
00:09:09,060 --> 00:09:11,200
I hope this makes sense
to you, straightforward.

186
00:09:11,200 --> 00:09:13,299
And you know what we are
going to do next, right?

187
00:09:13,299 --> 00:09:14,219
We are going to improve this

188
00:09:14,219 --> 00:09:16,139
because this one is not good.

189
00:09:16,139 --> 00:09:20,839
According to the class we
did for CPU MTM, you know,

190
00:09:20,839 --> 00:09:22,579
we can actually use
more memory from

191
00:09:22,579 --> 00:09:26,119
either thread local memory
or from the shared memory.

192
00:09:26,119 --> 00:09:27,639
And we can do telling, right.

193
00:09:27,639 --> 00:09:29,099
So we are going to
apply telling to

194
00:09:29,099 --> 00:09:31,540
this CPU kernel, okay?

195
00:09:31,670 --> 00:09:35,229
Another intuition, I don't
think I need to repeat.

196
00:09:35,229 --> 00:09:37,789
That is, by looking at this,

197
00:09:37,789 --> 00:09:39,709
P memory hierarchy, we know

198
00:09:39,709 --> 00:09:41,210
we have some additional
memory we can use.

199
00:09:41,210 --> 00:09:43,050
One is the register,
that is local memory,

200
00:09:43,050 --> 00:09:44,530
the other is shared
memory as well.

201
00:09:44,530 --> 00:09:47,070
Okay, so we'll try
to utilize them.

202
00:09:47,310 --> 00:09:50,590
Okay. So how do
you utilize them.

203
00:09:50,590 --> 00:09:54,125
So instead of letting,
yeah, please.

204
00:09:54,125 --> 00:09:56,959
Is the amount of registers,

205
00:09:56,959 --> 00:09:58,940
similar to how much registers

206
00:09:58,940 --> 00:10:01,439
CPU? It's slightly different.

207
00:10:01,439 --> 00:10:03,360
So in GPU,

208
00:10:03,360 --> 00:10:05,419
usually one streaming
multiprocessor

209
00:10:05,419 --> 00:10:07,580
have a fixed number of threads.

210
00:10:07,580 --> 00:10:09,279
And those threads are shared by

211
00:10:09,279 --> 00:10:11,399
all those registers are

212
00:10:11,399 --> 00:10:14,380
shared among all the
threads in that block.

213
00:10:14,380 --> 00:10:16,939
Yeah, yeah. Does that
make sense? Okay.

214
00:10:16,939 --> 00:10:18,600
Which means that if you allocate

215
00:10:18,600 --> 00:10:20,479
more register per thread,

216
00:10:20,479 --> 00:10:22,960
and you are going to
have less threads.

217
00:10:22,960 --> 00:10:25,660
If, so then you need to share

218
00:10:25,660 --> 00:10:27,039
some register built into

219
00:10:27,039 --> 00:10:28,705
that stream
multiprocessor, okay?

220
00:10:28,705 --> 00:10:30,189
Oh.

221
00:10:36,950 --> 00:10:40,789
No, register is
definitely much faster.

222
00:10:40,789 --> 00:10:45,530
Yeah. Okay. Okay, cool.

223
00:10:45,530 --> 00:10:48,169
So our first step
to improve this we

224
00:10:48,169 --> 00:10:51,270
try to utilize a little
bit more on the register.

225
00:10:51,270 --> 00:10:52,629
So in the previous program,

226
00:10:52,629 --> 00:10:53,890
we only use one register per

227
00:10:53,890 --> 00:10:56,010
spread now we are going
to use a little bit more.

228
00:10:56,010 --> 00:11:00,750
And if you recall the telling
register telling in CPU,

229
00:11:00,750 --> 00:11:04,330
what we do is we
let u in a loop,

230
00:11:04,330 --> 00:11:05,909
we do some memory reuse, right.

231
00:11:05,909 --> 00:11:08,010
We let each, each

232
00:11:08,010 --> 00:11:10,290
loop we read a little bit
more than one element,

233
00:11:10,290 --> 00:11:11,710
so we can do more memo.

234
00:11:11,710 --> 00:11:13,109
So here, the high level idea is,

235
00:11:13,109 --> 00:11:15,130
um instead of let

236
00:11:15,130 --> 00:11:18,089
each thread compute just
one element in the result.

237
00:11:18,089 --> 00:11:23,070
We can let E thread to
compute a V by V sub matrix.

238
00:11:23,070 --> 00:11:26,729
And this V is apparently
our telling factor.

239
00:11:26,729 --> 00:11:29,269
It's very similar
to the CPU telling.

240
00:11:29,269 --> 00:11:32,709
This figure basically give
you a high level idea.

241
00:11:32,709 --> 00:11:35,450
So instead of computing
that one element,

242
00:11:35,450 --> 00:11:37,849
and I'm going to extend
it a bit into V by

243
00:11:37,849 --> 00:11:41,550
V. I'm going to let E
thread to grab a few rows

244
00:11:41,550 --> 00:11:43,209
instead of one row from A

245
00:11:43,209 --> 00:11:45,589
and a few columns inside
of one column from

246
00:11:45,589 --> 00:11:48,849
B. I perform some sort of

247
00:11:48,849 --> 00:11:52,890
submetrix multiply and I
write the result spec.

248
00:11:52,890 --> 00:11:56,690
So why this can reduce memory?

249
00:11:57,970 --> 00:12:01,629
Because if you look at B,
from the perspective of B,

250
00:12:01,629 --> 00:12:02,829
every time I read the B,

251
00:12:02,829 --> 00:12:05,929
I can reuse the previous read
from the ad of A, right?

252
00:12:05,929 --> 00:12:07,530
So there are some memory reuse.

253
00:12:07,530 --> 00:12:10,610
Okay. Let's try to
implement this kernel.

254
00:12:10,610 --> 00:12:15,330
Okay. Please take a look at
this kernel for 30 seconds.

255
00:12:15,330 --> 00:12:16,569
Okay.

256
00:12:53,450 --> 00:12:56,289
Okay. Yeah, let's
pass this program.

257
00:12:56,289 --> 00:12:59,570
Okay. So from the
previous program,

258
00:12:59,570 --> 00:13:01,329
I think the main difference

259
00:13:01,329 --> 00:13:03,469
here, in the previous program,

260
00:13:03,469 --> 00:13:05,569
we directly index, the

261
00:13:05,569 --> 00:13:08,669
thread index one
element from A and B.

262
00:13:08,669 --> 00:13:12,209
But here we are trying
to index a block, okay.

263
00:13:12,209 --> 00:13:14,609
Another difference is we are

264
00:13:14,609 --> 00:13:18,169
allocating array
in the register.

265
00:13:18,169 --> 00:13:20,210
So which means that we are using

266
00:13:20,210 --> 00:13:23,369
we square registers, compared
to the previous one.

267
00:13:23,369 --> 00:13:27,969
And we are also allocating
random array, okay?

268
00:13:27,969 --> 00:13:32,289
And what do we do is
basically, uh, every time,

269
00:13:32,289 --> 00:13:34,989
I'm going to read this
row and read this column,

270
00:13:34,989 --> 00:13:37,170
I'm going to type them
together and write

271
00:13:37,170 --> 00:13:38,610
the results accumulated
results and

272
00:13:38,610 --> 00:13:40,290
then write it back to here.

273
00:13:40,290 --> 00:13:42,370
Okay? And I'm going to
read a little bit more

274
00:13:42,370 --> 00:13:44,589
because I'm going to read
all this into the register.

275
00:13:44,589 --> 00:13:46,670
So when I scan

276
00:13:46,670 --> 00:13:49,709
through this from this
column to this column,

277
00:13:49,709 --> 00:13:53,890
I can reuse my previous
read on this area, okay?

278
00:13:53,890 --> 00:13:58,399
So that we ended up with
this loop that is, um,

279
00:13:58,399 --> 00:14:02,269
I'm going to first index
for this current thread,

280
00:14:02,269 --> 00:14:03,890
I'm going to first
index which block

281
00:14:03,890 --> 00:14:05,330
I'm going to read, okay?

282
00:14:05,330 --> 00:14:06,930
And this is basically the index,

283
00:14:06,930 --> 00:14:09,349
X space times V plus X. Okay.

284
00:14:09,349 --> 00:14:11,269
I use my thread index to

285
00:14:11,269 --> 00:14:14,029
index the area I
need to read from A.

286
00:14:14,029 --> 00:14:16,810
And then I'm going to

287
00:14:16,810 --> 00:14:19,590
do the similar thing for
this column here, okay?

288
00:14:19,590 --> 00:14:21,350
And once I get to
this row and column,

289
00:14:21,350 --> 00:14:24,649
I'm going to perform
another inside of, like,

290
00:14:24,649 --> 00:14:28,090
a reduction loop where
I do a dot product

291
00:14:28,090 --> 00:14:29,529
between those elements
of the row and

292
00:14:29,529 --> 00:14:32,289
columns and cumulal
results, right.

293
00:14:32,289 --> 00:14:34,789
And eventually, I will
write the results back

294
00:14:34,789 --> 00:14:37,734
into this particular
block for this thread.

295
00:14:37,734 --> 00:14:39,659
Okay. So here, compared

296
00:14:39,659 --> 00:14:41,180
to the previous version
of the program,

297
00:14:41,180 --> 00:14:43,279
uh, this program did a
little bit more work, right?

298
00:14:43,279 --> 00:14:45,419
So it computes the results of

299
00:14:45,419 --> 00:14:48,260
the entire BV instead of
just a single element.

300
00:14:48,260 --> 00:14:51,559
So you can also get a high
level sense that is we

301
00:14:51,559 --> 00:14:54,619
are going to if the matrix
shape is unchanged,

302
00:14:54,619 --> 00:14:56,559
we are going to
need less threads

303
00:14:56,559 --> 00:14:58,139
because compared to
the previous one,

304
00:14:58,139 --> 00:15:00,239
we need squared
threads, but here,

305
00:15:00,239 --> 00:15:04,399
definitely, by a factor of
divided by a factor of square.

306
00:15:04,399 --> 00:15:10,519
Okay? Let's try to
analyze the memory IO.

307
00:15:11,320 --> 00:15:15,240
So global memory
rate per thread.

308
00:15:18,090 --> 00:15:22,009
So it's V plus U square, right?

309
00:15:22,009 --> 00:15:24,990
So why is NV plus U square?

310
00:15:24,990 --> 00:15:28,929
Because we are going to
basically read this part, right?

311
00:15:28,929 --> 00:15:30,769
And also, we are going
to read this part.

312
00:15:30,769 --> 00:15:32,610
That's all our read.

313
00:15:32,610 --> 00:15:34,009
Okay? And this part is

314
00:15:34,009 --> 00:15:35,729
only tied to this
outside loop, right?

315
00:15:35,729 --> 00:15:39,449
But this part is tied to
these two loops, right?

316
00:15:39,449 --> 00:15:42,950
And every time we are
reading an elements.

317
00:15:42,950 --> 00:15:47,509
Okay? So that is why this
is square on this V, okay?

318
00:15:47,509 --> 00:15:51,610
Cool. And how many threads?

319
00:15:52,680 --> 00:15:55,080
So compared to the
previous version,

320
00:15:55,080 --> 00:15:56,320
we are doing undivided

321
00:15:56,320 --> 00:15:58,160
by times undivided
by wave threads,

322
00:15:58,160 --> 00:16:01,960
now we only need unsquar
divided by w squared threads.

323
00:16:01,960 --> 00:16:06,499
Okay? So the total
global memory access

324
00:16:06,499 --> 00:16:07,799
is basically we market

325
00:16:07,799 --> 00:16:10,079
them together and we
get this element.

326
00:16:10,079 --> 00:16:12,119
Compared to the
previous version we

327
00:16:12,119 --> 00:16:14,579
did we have been doing
a little bit better.

328
00:16:14,579 --> 00:16:18,559
In the previous version, we
basically have two cube.

329
00:16:18,559 --> 00:16:20,080
Here we have one cube,

330
00:16:20,080 --> 00:16:22,539
which is basically divided
by the factor of V,

331
00:16:22,539 --> 00:16:23,980
so we reduce a little bit.

332
00:16:23,980 --> 00:16:27,409
Okay. And for memory,

333
00:16:27,409 --> 00:16:29,050
like how many register we use.

334
00:16:29,050 --> 00:16:30,829
So this is the memory
we use, right?

335
00:16:30,829 --> 00:16:37,009
We use basically Wquare
plus two float per thread.

336
00:16:37,009 --> 00:16:41,949
Okay. Are we good with this one?

337
00:16:41,949 --> 00:16:47,289
Cool. So, I think the
high level idea of

338
00:16:47,289 --> 00:16:49,289
this one is going to
is basically we like

339
00:16:49,289 --> 00:16:52,590
each thread to compute a
region instead of one element.

340
00:16:52,590 --> 00:16:56,649
But from the title of this,

341
00:16:56,649 --> 00:16:58,929
you know, this is just 1.5,

342
00:16:58,929 --> 00:17:01,530
and we can do a little bit
better than this region.

343
00:17:01,530 --> 00:17:04,830
So we all follow the
same idea as, um,

344
00:17:04,830 --> 00:17:08,589
we still like each thread
to compute one region,

345
00:17:08,589 --> 00:17:09,909
one result region in C.

346
00:17:09,909 --> 00:17:12,549
But can we still, improve
this program a little bit,

347
00:17:12,549 --> 00:17:14,509
so we don't have to,

348
00:17:14,509 --> 00:17:18,110
like, read this many times.

349
00:17:19,430 --> 00:17:22,509
Anyone has an idea here?

350
00:17:27,830 --> 00:17:30,830
So one efficient you find

351
00:17:30,830 --> 00:17:33,629
is when I do this
kind of tiling,

352
00:17:33,629 --> 00:17:35,029
every time I basically read

353
00:17:35,029 --> 00:17:37,109
one row from A and
one column from

354
00:17:37,109 --> 00:17:41,869
dira I basically do a
dot product together.

355
00:17:41,869 --> 00:17:45,129
And for each read
of the row here,

356
00:17:45,129 --> 00:17:46,429
we are going to read we are

357
00:17:46,429 --> 00:17:48,690
still going to read
B multiple times,

358
00:17:48,690 --> 00:17:51,789
here the way we
read the elements

359
00:17:51,789 --> 00:17:54,869
from B is still cube, right?

360
00:17:54,869 --> 00:17:58,179
So can we do better
here? We can right.

361
00:17:58,179 --> 00:18:01,940
So there are two ways of doing
multiply multiplication.

362
00:18:01,940 --> 00:18:04,319
You can do a row by column,

363
00:18:04,319 --> 00:18:06,299
um, uh, door product,

364
00:18:06,299 --> 00:18:08,640
and you directly get the
results on one element,

365
00:18:08,640 --> 00:18:12,359
but you can also do column
by row product, right?

366
00:18:12,359 --> 00:18:17,600
You pick you first locate
that small region from here.

367
00:18:17,600 --> 00:18:20,459
And you grab one
column here, right?

368
00:18:20,459 --> 00:18:22,919
And you also grab one row here,

369
00:18:22,919 --> 00:18:25,559
add them you multip
them together and you

370
00:18:25,559 --> 00:18:28,119
get you get one result matrix,

371
00:18:28,119 --> 00:18:31,039
which is the shape of V by

372
00:18:31,039 --> 00:18:33,239
V. But this V by V is not

373
00:18:33,239 --> 00:18:35,899
the eventual results
we need, right?

374
00:18:35,899 --> 00:18:38,659
So in order to get
eventual results,

375
00:18:38,659 --> 00:18:42,619
what we do is we can scan from
here and we scan from here

376
00:18:42,619 --> 00:18:45,220
and we basically read

377
00:18:45,220 --> 00:18:48,139
one small column here,
one small row here.

378
00:18:48,139 --> 00:18:51,339
We basically get one
region of results.

379
00:18:51,339 --> 00:18:55,259
And then we read one column
here and one small row here,

380
00:18:55,259 --> 00:18:57,539
and we get another
region of them.

381
00:18:57,539 --> 00:19:00,600
And this is called partial sum.

382
00:19:00,600 --> 00:19:02,300
So every time we
get a partial sum,

383
00:19:02,300 --> 00:19:03,479
and what we need
to do is basically

384
00:19:03,479 --> 00:19:04,760
we add all the partial sum

385
00:19:04,760 --> 00:19:06,379
together and we get
the eventual results

386
00:19:06,379 --> 00:19:08,199
of this region, right?

387
00:19:08,199 --> 00:19:11,860
And in this pattern, you can Oh,

388
00:19:11,860 --> 00:19:13,999
for A, for each route,

389
00:19:13,999 --> 00:19:15,559
we still read the entire region.

390
00:19:15,559 --> 00:19:18,719
And for B, we are doing
better because we only need

391
00:19:18,719 --> 00:19:21,880
to scan the entire region
once instead of times,

392
00:19:21,880 --> 00:19:24,259
right? Let's see this.

393
00:19:24,259 --> 00:19:28,200
Okay. Mathematical
mathematical intuition

394
00:19:28,200 --> 00:19:29,739
behind this is basically,

395
00:19:29,739 --> 00:19:32,560
in order to get
that result matrix

396
00:19:32,560 --> 00:19:35,940
here, there are two ways.

397
00:19:35,940 --> 00:19:37,999
One is a row by column.

398
00:19:37,999 --> 00:19:39,840
But we can do it differently.

399
00:19:39,840 --> 00:19:41,440
That is we can basically break

400
00:19:41,440 --> 00:19:45,119
this region into sub
matrix like this.

401
00:19:45,119 --> 00:19:47,519
For example, we break
this X into X one,

402
00:19:47,519 --> 00:19:50,199
X two, many many columns.

403
00:19:50,199 --> 00:19:53,779
And we break this Y
into Y one, Y two.

404
00:19:53,779 --> 00:19:55,839
And we find that this region can

405
00:19:55,839 --> 00:19:58,039
actually be expressed as X,

406
00:19:58,039 --> 00:20:01,009
Y one plus x2y2.

407
00:20:01,009 --> 00:20:04,699
Right. And if we break into
many many small regions,

408
00:20:04,699 --> 00:20:07,139
we find that in order to
get this part results,

409
00:20:07,139 --> 00:20:10,540
we only need to read this
time once and read this once.

410
00:20:10,540 --> 00:20:11,860
And in the previous version,

411
00:20:11,860 --> 00:20:13,760
we need to read this once

412
00:20:13,760 --> 00:20:16,139
and read this cube times, right?

413
00:20:16,139 --> 00:20:17,760
So that is the intuition.

414
00:20:17,760 --> 00:20:20,539
And we are going to follow
in this intuition and we try

415
00:20:20,539 --> 00:20:23,959
to modify our previous program,

416
00:20:23,959 --> 00:20:27,479
progress kernel into
this version, okay?

417
00:20:27,479 --> 00:20:31,379
Please take a look at
this kernel little bit.

418
00:20:36,230 --> 00:20:38,269
Yeah.

419
00:20:45,310 --> 00:20:49,469
So you mean,

420
00:20:49,469 --> 00:20:51,229
where I get this?

421
00:20:57,830 --> 00:21:02,949
Uh huh. Yeah, you
get the by, right?

422
00:21:02,949 --> 00:21:04,850
And then you just
scan from left,

423
00:21:04,850 --> 00:21:06,790
right from top bottom
of my asthm altogether.

424
00:21:06,790 --> 00:21:11,869
You get the by V. So that

425
00:21:12,950 --> 00:21:21,349
you multiply two s get one Yeah,

426
00:21:21,349 --> 00:21:24,589
in the previous, I
read this, right?

427
00:21:24,589 --> 00:21:27,389
I read this, this entire row.

428
00:21:27,389 --> 00:21:30,489
In the previous I read
this entire row, right?

429
00:21:30,489 --> 00:21:33,429
And I read this entire column,
I get one element out.

430
00:21:33,429 --> 00:21:35,570
And in order to get
this second element,

431
00:21:35,570 --> 00:21:37,169
I need to read I read this

432
00:21:37,169 --> 00:21:39,249
and I need to read
this one again, right?

433
00:21:39,249 --> 00:21:42,910
Yeah. So basically, for
each read of this row,

434
00:21:42,910 --> 00:21:44,889
I need to read all
this column one.

435
00:21:44,889 --> 00:21:47,129
That's why it is square, right?

436
00:21:47,129 --> 00:21:49,949
But in this one, I just
need to read this and this.

437
00:21:49,949 --> 00:21:51,690
Yeah. Does that make sense?

438
00:21:51,690 --> 00:21:53,469
And that basically
give you intuition

439
00:21:53,469 --> 00:21:55,589
why this memory are
always smaller.

440
00:21:55,589 --> 00:21:59,789
Okay. Yeah, uh, I
think this program,

441
00:21:59,789 --> 00:22:01,969
once you understand this, uh,

442
00:22:01,969 --> 00:22:03,830
block wise, matrix
multiplication

443
00:22:03,830 --> 00:22:05,629
and partial sum, you
understand this program.

444
00:22:05,629 --> 00:22:07,429
So what it does is it still use

445
00:22:07,429 --> 00:22:10,010
that block index and block

446
00:22:10,010 --> 00:22:14,690
D and also thread index to
locate this region, okay?

447
00:22:14,690 --> 00:22:16,429
And once it locates the region,

448
00:22:16,429 --> 00:22:20,669
it's going to create the exact
same amount of registers.

449
00:22:20,669 --> 00:22:22,689
Okay. But instead of like I

450
00:22:22,689 --> 00:22:26,549
loop I loop over the rows
of A and the column of B,

451
00:22:26,549 --> 00:22:30,229
I'm going to loop over the
columns of A and rows of B.

452
00:22:30,229 --> 00:22:32,649
Okay? That's why there's
outer loop K, right?

453
00:22:32,649 --> 00:22:35,484
This K basically scan
following this direction.

454
00:22:35,484 --> 00:22:41,499
And every time I'm going
to grab a small area,

455
00:22:41,499 --> 00:22:46,060
right here, is this
area and this area.

456
00:22:46,060 --> 00:22:49,599
And I do dot product. Okay?

457
00:22:49,599 --> 00:22:52,759
And then I add all the results
together into this region.

458
00:22:52,759 --> 00:22:57,294
And I add all the partial sum
together and I get the by.

459
00:22:57,294 --> 00:23:02,410
Okay. And let's try to
analyze the memory IO.

460
00:23:02,410 --> 00:23:04,809
So the global memory
rate per thread

461
00:23:04,809 --> 00:23:07,489
is noise and squared, right?

462
00:23:07,489 --> 00:23:09,410
So it's much better, okay?

463
00:23:09,410 --> 00:23:11,449
And the number of
threads we need

464
00:23:11,449 --> 00:23:13,369
is basically, uh, same, right?

465
00:23:13,369 --> 00:23:15,070
Divided by w and nib.

466
00:23:15,070 --> 00:23:17,810
And squared divided
by way square.

467
00:23:17,810 --> 00:23:20,209
The total memory access is,

468
00:23:20,209 --> 00:23:21,989
we multiply this together,

469
00:23:21,989 --> 00:23:23,869
and we find that for

470
00:23:23,869 --> 00:23:27,529
both the cubic term,
we can divide them by.

471
00:23:27,529 --> 00:23:30,709
Okay? And the number

472
00:23:30,709 --> 00:23:33,169
register we need is also
we square plus two,

473
00:23:33,169 --> 00:23:35,869
essentially this
part we allocate.

474
00:23:35,940 --> 00:23:39,659
Okay. Cool. Are we
good with this one?

475
00:23:39,659 --> 00:23:40,619
Yeah.

476
00:23:40,619 --> 00:23:51,199
So you mean,

477
00:23:51,199 --> 00:23:53,779
yeah, you can swap them.

478
00:23:53,779 --> 00:23:55,740
It's fine. Yeah, yeah,
yeah. Doesn't matter.

479
00:23:55,740 --> 00:23:58,659
It's just like messes
make like this.

480
00:23:58,659 --> 00:24:02,160
Yeah. Okay? Okay, cool.

481
00:24:02,160 --> 00:24:04,819
This is already a pretty
good version of Mtmo.

482
00:24:04,819 --> 00:24:08,220
I think in many today's
machinery frameworks,

483
00:24:08,220 --> 00:24:09,659
if you only have

484
00:24:09,659 --> 00:24:12,019
one layer of memory hierarchy,
you end up with this one.

485
00:24:12,019 --> 00:24:16,509
Okay? But we are not
ending here, right?

486
00:24:16,509 --> 00:24:20,409
So we know we have another
layer of memory hierarchy.

487
00:24:20,409 --> 00:24:22,809
That is we want to use
our shared memory.

488
00:24:22,809 --> 00:24:25,969
Okay. So our initial
content was here, right?

489
00:24:25,969 --> 00:24:28,490
And we have to go through
this memory iarchy we read

490
00:24:28,490 --> 00:24:31,369
it from here to shared
memory then to registers.

491
00:24:31,369 --> 00:24:33,169
Okay? Now, we are going to

492
00:24:33,169 --> 00:24:36,030
make things more and
more complicated.

493
00:24:36,030 --> 00:24:39,830
So how to basically leverage
shared memory and try

494
00:24:39,830 --> 00:24:44,409
to implement this two level
telling or math on GPS.

495
00:24:44,409 --> 00:24:50,789
So here, the intuition is
still quite straightforward,

496
00:24:50,789 --> 00:24:52,710
but the coding is a
little bit difficult.

497
00:24:52,710 --> 00:24:54,649
Okay, uh, in the lecture,

498
00:24:54,649 --> 00:24:56,190
I'm going to focus on intuition.

499
00:24:56,190 --> 00:24:58,969
I think it's very hard to
if you are new to this,

500
00:24:58,969 --> 00:25:01,429
it's very hard to explain
this very clearly to you.

501
00:25:01,429 --> 00:25:04,269
So please put time on
this on the program.

502
00:25:04,269 --> 00:25:05,870
But I'm going to
give you intuition.

503
00:25:05,870 --> 00:25:09,649
So here, if you still
remember the uh,

504
00:25:09,649 --> 00:25:11,809
met M in CPU right,

505
00:25:11,809 --> 00:25:14,149
remember that program,
we do two level telling.

506
00:25:14,149 --> 00:25:15,669
The thing is basically we keep

507
00:25:15,669 --> 00:25:17,029
telling and telling
telling, right.

508
00:25:17,029 --> 00:25:18,349
We do a recursive telling.

509
00:25:18,349 --> 00:25:20,930
We first tell it at some
shape and then inside

510
00:25:20,930 --> 00:25:22,069
of that small shape
we're going to

511
00:25:22,069 --> 00:25:23,929
tell human smaller ship matrix.

512
00:25:23,929 --> 00:25:26,790
So here, what we do
is we try to let

513
00:25:26,790 --> 00:25:30,470
know we are using the SM,
which is shared memory.

514
00:25:30,470 --> 00:25:32,450
Remember shared memory
is at the block level.

515
00:25:32,450 --> 00:25:34,409
So all the threads in
one block is going

516
00:25:34,409 --> 00:25:37,189
to access to that shared memory.

517
00:25:37,189 --> 00:25:39,150
So what we do is
we let each block

518
00:25:39,150 --> 00:25:42,109
to compute a region, right?

519
00:25:42,109 --> 00:25:45,609
And this region is
denoted here as L by L.

520
00:25:45,609 --> 00:25:48,610
And then we know that
inside of the block,

521
00:25:48,610 --> 00:25:51,249
all the threads can
access that shared memory

522
00:25:51,249 --> 00:25:55,129
so they can work together
to calculate by, right?

523
00:25:55,129 --> 00:25:56,850
And then for each thread,

524
00:25:56,850 --> 00:25:58,569
we are going to let
them to compute

525
00:25:58,569 --> 00:25:59,649
a smaller sub region

526
00:25:59,649 --> 00:26:02,999
from this by L, which
is essentially by.

527
00:26:02,999 --> 00:26:05,869
Okay. And white telling

528
00:26:05,869 --> 00:26:08,290
can help because
of memory we use,

529
00:26:08,290 --> 00:26:09,789
right? So think about this.

530
00:26:09,789 --> 00:26:13,469
So when we compute
this small regime by,

531
00:26:13,469 --> 00:26:17,330
right, we essentially
need this row.

532
00:26:17,530 --> 00:26:19,689
We need to read this row, right?

533
00:26:19,689 --> 00:26:21,410
And these are many roles,

534
00:26:21,410 --> 00:26:23,950
not just a single, this
block of rows, okay?

535
00:26:23,950 --> 00:26:27,509
And we need to read this
part of column, right?

536
00:26:27,509 --> 00:26:30,889
But when we compute this
regime, what do we need?

537
00:26:31,650 --> 00:26:34,690
So we still need the many rules,

538
00:26:34,690 --> 00:26:37,089
right? But we need to read this.

539
00:26:37,089 --> 00:26:39,249
This is the hoteling
works because

540
00:26:39,249 --> 00:26:42,969
if we basically perform
some reading ahead of time,

541
00:26:42,969 --> 00:26:44,530
that is we read this part into

542
00:26:44,530 --> 00:26:46,850
the shared memory and
keep it there, okay?

543
00:26:46,850 --> 00:26:48,270
And then at the thread level,

544
00:26:48,270 --> 00:26:49,490
we are going to iterate through

545
00:26:49,490 --> 00:26:51,149
the many many columns so we

546
00:26:51,149 --> 00:26:54,389
can basically have some memory
reuse for this reading.

547
00:26:54,389 --> 00:26:57,010
Therefore, we save
some memory, okay?

548
00:26:57,010 --> 00:27:00,059
And same thing, right? And
you can basically do this.

549
00:27:00,059 --> 00:27:02,519
Okay? This is tally. And I'm

550
00:27:02,519 --> 00:27:04,499
going to show this
program a little bit,

551
00:27:04,499 --> 00:27:06,179
maybe give you 30 seconds.

552
00:27:06,179 --> 00:27:08,460
And I think this one is hard.

553
00:27:08,460 --> 00:27:10,979
Yeah. Please spend some time

554
00:27:10,979 --> 00:27:13,179
later after the class
to understand this one.

555
00:27:13,179 --> 00:27:14,339
Yeah.

556
00:27:41,620 --> 00:27:43,459
Okay, I'm going

557
00:27:43,459 --> 00:27:45,359
to give you some
high level things.

558
00:27:45,359 --> 00:27:48,800
Okay, so you can think
about sync either way.

559
00:27:48,800 --> 00:27:52,559
First the difference you
noted is this line, right,

560
00:27:52,559 --> 00:27:54,660
where I started using

561
00:27:54,660 --> 00:27:58,100
this prefix to allocate
memory from SRM.

562
00:27:58,100 --> 00:28:00,959
A? So I allocate two
chunks of memory,

563
00:28:00,959 --> 00:28:03,500
one is SA, the other SB.

564
00:28:03,500 --> 00:28:06,919
Here, I simplify a little bit
because we should allocate

565
00:28:06,919 --> 00:28:08,299
one of the shape

566
00:28:08,299 --> 00:28:11,999
of S by L and the other
is the shape of L by S.

567
00:28:11,999 --> 00:28:14,399
But I basically unify
them a little bit,

568
00:28:14,399 --> 00:28:15,940
because you can think
that I transpose

569
00:28:15,940 --> 00:28:17,620
the matrix a little bit, okay?

570
00:28:17,620 --> 00:28:19,919
So I'm going to allocate memory

571
00:28:19,919 --> 00:28:22,744
for this part and for this
part in shared memory.

572
00:28:22,744 --> 00:28:25,810
Okay. And what I'm going
to do is I'm going to scan

573
00:28:25,810 --> 00:28:29,350
from here to here and here
to here at the block level.

574
00:28:29,350 --> 00:28:31,650
Okay? And I like each block

575
00:28:31,650 --> 00:28:33,189
to basically scan
from here to here,

576
00:28:33,189 --> 00:28:35,269
here to here and
compute this part.

577
00:28:35,269 --> 00:28:36,769
And inside of this block, what

578
00:28:36,769 --> 00:28:38,429
I'm going to do is I will like

579
00:28:38,429 --> 00:28:41,009
each thread to read
a little region

580
00:28:41,009 --> 00:28:42,729
of this and a little
region of this.

581
00:28:42,729 --> 00:28:45,310
Right. I have another inner loop

582
00:28:45,310 --> 00:28:48,829
that basically let me
clean this a little bit.

583
00:28:49,560 --> 00:28:52,780
I have another inner loop
which basically reads

584
00:28:52,780 --> 00:28:56,139
a small area of this and
I scan from here to here.

585
00:28:56,139 --> 00:28:59,940
Sorry, from here to
here and here to here.

586
00:28:59,940 --> 00:29:03,759
This is recursive, one
is basically telling at

587
00:29:03,759 --> 00:29:05,800
the block level and the
other is basically telling

588
00:29:05,800 --> 00:29:08,140
at uh the thread level.

589
00:29:08,140 --> 00:29:11,799
Okay. I'm going to

590
00:29:11,799 --> 00:29:15,359
first use my block index to
index this region, right?

591
00:29:15,359 --> 00:29:18,780
And this is what I'm doing.
I'm indexing it regi.

592
00:29:18,780 --> 00:29:20,800
I basically start reading

593
00:29:20,800 --> 00:29:23,399
things from HBM
because it's A and B,

594
00:29:23,399 --> 00:29:25,119
allocated on HBM, I

595
00:29:25,119 --> 00:29:27,779
reading things from GBM
to my shared memory.

596
00:29:27,779 --> 00:29:32,139
Okay? And the and
this line of code is

597
00:29:32,139 --> 00:29:36,540
problematic because if you
still remember the window sum,

598
00:29:36,540 --> 00:29:38,259
in the Window sum example,

599
00:29:38,259 --> 00:29:40,820
when I try to let
all threads to read

600
00:29:40,820 --> 00:29:43,259
things from GPM to share memory,

601
00:29:43,259 --> 00:29:46,180
what I do is I need to
also partition read,

602
00:29:46,180 --> 00:29:48,999
I only let each thread
to read one part of it.

603
00:29:48,999 --> 00:29:50,959
And this is called
cooperative fetching.

604
00:29:50,959 --> 00:29:54,754
I think we covered a little
bit. Still remember?

605
00:29:54,754 --> 00:29:57,769
Okay. And here I simplify
this code a little bit

606
00:29:57,769 --> 00:29:59,129
because we are going to

607
00:29:59,129 --> 00:30:01,450
cover how we implement
this chunk later.

608
00:30:01,450 --> 00:30:02,989
But the high level idea is you

609
00:30:02,989 --> 00:30:06,169
cannot just let all threads
to read all the regions

610
00:30:06,169 --> 00:30:08,029
into shared memory because

611
00:30:08,029 --> 00:30:09,509
this shared memory
is going to be

612
00:30:09,509 --> 00:30:11,190
accessed by all the
threads in the block.

613
00:30:11,190 --> 00:30:12,789
So each thread only is

614
00:30:12,789 --> 00:30:15,809
responsible for
reading its own part.

615
00:30:15,809 --> 00:30:17,209
Yeah, I hope to write some

616
00:30:17,209 --> 00:30:19,009
code part in I
read a little bit.

617
00:30:19,009 --> 00:30:23,269
Okay. And if you still
remember the two sync threads,

618
00:30:23,269 --> 00:30:26,189
why we need a syn thread here?

619
00:30:26,620 --> 00:30:28,660
Yeah, it's shared memory.

620
00:30:28,660 --> 00:30:30,499
I need to wait for this
to finish the job,

621
00:30:30,499 --> 00:30:32,519
and then I can start access
that part of memory, right?

622
00:30:32,519 --> 00:30:33,699
Otherwise, there will be

623
00:30:33,699 --> 00:30:37,079
undetermined area,
undetermined behavior.

624
00:30:37,079 --> 00:30:39,819
Why I need this single stress?

625
00:30:46,110 --> 00:30:50,530
Because I want to avoid that
one thread properly finish

626
00:30:50,530 --> 00:30:52,409
the computation here and

627
00:30:52,409 --> 00:30:54,190
come back to the next
iteration of that loop,

628
00:30:54,190 --> 00:30:57,129
I want to avoid that
thread to start reading.

629
00:30:57,129 --> 00:30:59,069
I want to make sure
all threads finish

630
00:30:59,069 --> 00:31:02,349
the computation and basically
accumulated results,

631
00:31:02,349 --> 00:31:03,570
and then I add

632
00:31:03,570 --> 00:31:05,869
a barrier and I start
reading new contents.

633
00:31:05,869 --> 00:31:10,524
That is the next columns here.

634
00:31:10,524 --> 00:31:13,800
So that's why I need to add
another barrier here, okay?

635
00:31:13,800 --> 00:31:16,780
So with this barrier, you
understand that this loop

636
00:31:16,780 --> 00:31:18,359
is doing is basically reading

637
00:31:18,359 --> 00:31:20,080
this area, reading this area.

638
00:31:20,080 --> 00:31:22,399
Okay? And then what we do is

639
00:31:22,399 --> 00:31:24,239
basically this is something that

640
00:31:24,239 --> 00:31:26,459
we have been talking about
in the last slide, right.

641
00:31:26,459 --> 00:31:28,700
We basically use ress level

642
00:31:28,700 --> 00:31:32,299
telling to handle the
multiplication of this.

643
00:31:32,299 --> 00:31:35,260
Okay? We each thread to
take a smaller region

644
00:31:35,260 --> 00:31:38,060
from that and we have
another three layer loop,

645
00:31:38,060 --> 00:31:41,299
and you can see,
this is like we are

646
00:31:41,299 --> 00:31:43,739
accumulating results
from all the way

647
00:31:43,739 --> 00:31:45,659
from here to here
and inside of this.

648
00:31:45,659 --> 00:31:47,159
So we are adding all
the results together

649
00:31:47,159 --> 00:31:49,259
into the result entry, okay?

650
00:31:49,259 --> 00:31:55,599
Yeah. Like to report
about the control and

651
00:31:55,599 --> 00:32:05,879
the here in this program,

652
00:32:05,879 --> 00:32:08,179
I don't think there's
a control flow, right?

653
00:32:12,660 --> 00:32:16,960
It's very unlikely. It's very
unlikely, but just in case,

654
00:32:16,960 --> 00:32:19,699
because sometimes
maybe one core get

655
00:32:19,699 --> 00:32:22,479
a little bit higher
power than the other.

656
00:32:22,479 --> 00:32:25,059
Yeah, because of physics.

657
00:32:27,540 --> 00:32:30,019
Yeah, it really is.

658
00:32:30,019 --> 00:32:32,940
But I spell this is for
the safety of the code.

659
00:32:32,940 --> 00:32:35,299
Because you don't
know if this happens,

660
00:32:35,299 --> 00:32:37,080
it's very hard to debate. Yeah.

661
00:32:37,080 --> 00:32:41,219
Okay. Okay, spend some time.

662
00:32:41,219 --> 00:32:42,739
Basically, what do we do

663
00:32:42,739 --> 00:32:44,259
compared to the
previous versions,

664
00:32:44,259 --> 00:32:46,440
basically at this area?

665
00:32:46,440 --> 00:32:47,959
Yeah, this regional code that

666
00:32:47,959 --> 00:32:50,780
basically telling
at the block level.

667
00:32:51,420 --> 00:32:58,360
Similarly, we do we can do
calculation on memory IO,

668
00:32:58,360 --> 00:33:00,259
and I'm going to
present results to you.

669
00:33:00,259 --> 00:33:02,079
I'm going to leave
this to yourself.

670
00:33:02,079 --> 00:33:03,759
But I think this
part is easier than

671
00:33:03,759 --> 00:33:05,699
understanding the code
because what you do is,

672
00:33:05,699 --> 00:33:07,620
uh, you basically read the loop,

673
00:33:07,620 --> 00:33:10,219
and you count how
many registers and

674
00:33:10,219 --> 00:33:11,899
shared memory allocated and

675
00:33:11,899 --> 00:33:15,099
how many IO is happening
between memory hierarchies.

676
00:33:15,920 --> 00:33:21,099
Okay. Looking at
this part, right?

677
00:33:21,099 --> 00:33:23,539
I said this part of code is

678
00:33:23,539 --> 00:33:27,600
problematic because if I
read my code in this way,

679
00:33:27,600 --> 00:33:29,720
that means that all my
thress is going to read

680
00:33:29,720 --> 00:33:32,260
the entire region into the
shared memory, which is wrong.

681
00:33:32,260 --> 00:33:36,249
First, it is it is slow. I
don't how to do that, right?

682
00:33:36,249 --> 00:33:40,289
Second, um, there will be
some conflicts, for example,

683
00:33:40,289 --> 00:33:43,490
some threads reading things
into that register, sorry,

684
00:33:43,490 --> 00:33:44,630
that shared memory entry,

685
00:33:44,630 --> 00:33:46,069
another thread is doing
the same thing and

686
00:33:46,069 --> 00:33:47,809
they are going to
have some conflict.

687
00:33:47,809 --> 00:33:50,530
So what do we do is we need
to change this protocol,

688
00:33:50,530 --> 00:33:53,429
and this is a very
very common approach

689
00:33:53,429 --> 00:33:55,509
in GPU called
cooperative fetching.

690
00:33:55,509 --> 00:33:59,890
That is all fetching memory
from one source area,

691
00:33:59,890 --> 00:34:02,230
for example, GPM into
a destination area.

692
00:34:02,230 --> 00:34:04,389
And this cooperative
fetching also need to be

693
00:34:04,389 --> 00:34:07,070
implemented in a
way that is SMD.

694
00:34:07,070 --> 00:34:10,489
So we need to it's the
developer's responsibility

695
00:34:10,489 --> 00:34:11,969
to think about which region,

696
00:34:11,969 --> 00:34:13,509
which is separating
that should be

697
00:34:13,509 --> 00:34:15,770
responsible for and
try to calculate

698
00:34:15,770 --> 00:34:17,649
the offset a little
bit and let us

699
00:34:17,649 --> 00:34:20,329
write to fresh things and
write things into SM,

700
00:34:20,329 --> 00:34:23,729
into into into the
uh, shared memory.

701
00:34:23,729 --> 00:34:26,609
And so the true implementation

702
00:34:26,609 --> 00:34:28,749
of this line should
be looking like this.

703
00:34:28,749 --> 00:34:30,449
And I will let you guys look at

704
00:34:30,449 --> 00:34:31,849
this for 15 seconds

705
00:34:31,849 --> 00:34:34,410
because I think this
one is easy, okay?

706
00:34:48,260 --> 00:34:49,579
Okay.

707
00:34:49,579 --> 00:34:51,439
Um, so if you are

708
00:34:51,439 --> 00:34:54,000
familiar with C plus pass
programming and indexing,

709
00:34:54,000 --> 00:34:55,239
you know what you've
been doing, right?

710
00:34:55,239 --> 00:34:57,960
So basically, for this
block of threads,

711
00:34:57,960 --> 00:34:59,879
I'm going to get a thread ID,

712
00:34:59,879 --> 00:35:02,320
and then I use thread
ID to calculate

713
00:35:02,320 --> 00:35:05,920
which entry are responsible
for this current thread.

714
00:35:05,920 --> 00:35:08,059
And for this current thread,

715
00:35:08,059 --> 00:35:11,239
I'm going to only I'm going
to index that element I'm

716
00:35:11,239 --> 00:35:12,639
responsible for reading and

717
00:35:12,639 --> 00:35:14,879
read it into shared
memory, right?

718
00:35:14,879 --> 00:35:18,940
Yeah, very standardized
offsetting indexing code,

719
00:35:18,940 --> 00:35:22,439
in CPlusPas Okay, cool.

720
00:35:22,439 --> 00:35:25,400
That basically is,
in my opinion,

721
00:35:25,400 --> 00:35:28,239
the second most difficult
code in this class.

722
00:35:28,239 --> 00:35:29,779
Okay. And we are going to

723
00:35:29,779 --> 00:35:31,159
touch base with flash
teaching later,

724
00:35:31,159 --> 00:35:33,059
and that one will be more
complicated than this one.

725
00:35:33,059 --> 00:35:34,660
But this is the
second difficult.

726
00:35:34,660 --> 00:35:38,259
So as long as you get a good
understanding of this code,

727
00:35:38,259 --> 00:35:39,460
I think you already finished

728
00:35:39,460 --> 00:35:41,479
half of flashing teaching. Okay?

729
00:35:41,479 --> 00:35:47,879
A good? Cool. Okay,
let's wrap up this part,

730
00:35:47,879 --> 00:35:49,639
okay, because we have
been doing a lot of

731
00:35:49,639 --> 00:35:51,940
GPU programming in the
recent two lectures.

732
00:35:51,940 --> 00:35:54,179
Um, so there are

733
00:35:54,179 --> 00:35:56,259
many more GP organization
I didn't cover,

734
00:35:56,259 --> 00:35:59,159
but I give you some readings,
in this week, right?

735
00:35:59,159 --> 00:36:00,979
There are some readings on how

736
00:36:00,979 --> 00:36:02,900
to understand the
GPU performance.

737
00:36:02,900 --> 00:36:04,919
And I think in our
documentation,

738
00:36:04,919 --> 00:36:07,300
media provides a
lot of readings,

739
00:36:07,300 --> 00:36:08,859
and someday if you
need to use that,

740
00:36:08,859 --> 00:36:10,899
you know where to
find pointers, okay?

741
00:36:10,899 --> 00:36:13,330
But in general, um,

742
00:36:13,330 --> 00:36:15,450
there are a few other
important aspects

743
00:36:15,450 --> 00:36:17,089
to make TPO kernel fast.

744
00:36:17,089 --> 00:36:19,909
The first one is global
memory continuous read.

745
00:36:19,909 --> 00:36:21,709
That is when I
myths write to read

746
00:36:21,709 --> 00:36:23,850
AHGPM I need to make sure
that they read continuously.

747
00:36:23,850 --> 00:36:28,089
I cannot read one element
and apply offset and it's

748
00:36:28,089 --> 00:36:30,129
going to read another
element because we know

749
00:36:30,129 --> 00:36:32,609
that even in CP memory,

750
00:36:32,609 --> 00:36:34,309
this is slow because we want

751
00:36:34,309 --> 00:36:36,510
to avoid reading with offsets.

752
00:36:36,510 --> 00:36:38,809
We want to always
do continuous read.

753
00:36:38,809 --> 00:36:42,160
Okay uh, shared
memory bank conflict.

754
00:36:42,160 --> 00:36:46,100
I think here I'm just giving
you a fancier name, okay?

755
00:36:46,100 --> 00:36:47,759
But the things that

756
00:36:47,759 --> 00:36:49,779
I think this term you
already understands,

757
00:36:49,779 --> 00:36:51,980
remember in this
cooperative fetching,

758
00:36:51,980 --> 00:36:53,999
if I let all threats to read

759
00:36:53,999 --> 00:36:56,260
GBM without calculating
this offset,

760
00:36:56,260 --> 00:36:57,639
there will be some conflict, and

761
00:36:57,639 --> 00:36:59,040
we call this bank conflict.

762
00:36:59,040 --> 00:37:01,900
Okay. And sometimes
we need to avoid

763
00:37:01,900 --> 00:37:05,244
this kind of shared memory
bank conflict, okay?

764
00:37:05,244 --> 00:37:07,590
Also pipe planning,

765
00:37:07,590 --> 00:37:09,609
and this one is very easy
to understand, right?

766
00:37:09,609 --> 00:37:11,570
So because we have
memory hierarchies,

767
00:37:11,570 --> 00:37:15,010
we have different stress doing
IO and doing competition.

768
00:37:15,010 --> 00:37:16,749
So in order to even save time,

769
00:37:16,749 --> 00:37:18,469
we can add some stress
to doing reading and

770
00:37:18,469 --> 00:37:20,950
another group of thread
doing competition.

771
00:37:20,950 --> 00:37:24,789
And then we can overlap the
memory IO and computing time.

772
00:37:24,789 --> 00:37:28,849
We don't know how to do
it in sequential order.

773
00:37:28,849 --> 00:37:31,669
Another two things,
one is tensor core,

774
00:37:31,669 --> 00:37:34,709
I hope you guys already read
the readings I give to you,

775
00:37:34,709 --> 00:37:35,750
and I think you understand

776
00:37:35,750 --> 00:37:37,029
what is tensor
core doing, right?

777
00:37:37,029 --> 00:37:39,530
It's a more specialized
core for MTM, okay?

778
00:37:39,530 --> 00:37:41,509
So it will be faster in MT

779
00:37:41,509 --> 00:37:44,170
M. And the last one
is lower precision,

780
00:37:44,170 --> 00:37:45,824
which we'll cover next week.

781
00:37:45,824 --> 00:37:49,760
Okay. Cool. That basically

782
00:37:49,760 --> 00:37:53,720
covers all the lecture
contents for GPUs.

783
00:37:53,720 --> 00:37:59,509
Any question? Okay.

784
00:37:59,509 --> 00:38:02,449
Then let's move forward. So if

785
00:38:02,449 --> 00:38:04,230
we come back to
revisit this program,

786
00:38:04,230 --> 00:38:05,929
we find that, yeah,

787
00:38:05,929 --> 00:38:09,669
we have a high level
idea how to do this

788
00:38:09,669 --> 00:38:12,569
uh kernel how to

789
00:38:12,569 --> 00:38:14,910
basically utilize those
memory hierarchy.

790
00:38:14,910 --> 00:38:17,189
But if I ask you to do this,

791
00:38:17,189 --> 00:38:19,409
you are still going to
face a lot of problem.

792
00:38:19,409 --> 00:38:21,649
Because remember in CPU tolling,

793
00:38:21,649 --> 00:38:23,750
there are a few tolling
factors that are critical.

794
00:38:23,750 --> 00:38:26,489
For example, the way you
use shared memory cannot

795
00:38:26,489 --> 00:38:28,029
exceed the limit the

796
00:38:28,029 --> 00:38:29,889
shared memory provided
by the device.

797
00:38:29,889 --> 00:38:34,219
Same thing here, how many
threads should you ask for?

798
00:38:34,219 --> 00:38:38,389
Okay. How many regiers
you should exactly use?

799
00:38:38,389 --> 00:38:40,769
Apparently, it cannot
exceed the number of

800
00:38:40,769 --> 00:38:43,450
regiers in that streaming
microprocessor, okay?

801
00:38:43,450 --> 00:38:45,830
And how many amount of SRAM

802
00:38:45,830 --> 00:38:48,110
should I ask for for my program?

803
00:38:48,110 --> 00:38:50,089
Because different devices have

804
00:38:50,089 --> 00:38:52,829
different number SRAM, okay?

805
00:38:52,829 --> 00:38:55,410
And this is basically
the practical concerns

806
00:38:55,410 --> 00:38:56,689
when you implement this kernel.

807
00:38:56,689 --> 00:38:58,849
And even if you

808
00:38:58,849 --> 00:39:01,370
can write the entire
code of this program,

809
00:39:01,370 --> 00:39:02,630
it's not guaranteed
that this program

810
00:39:02,630 --> 00:39:04,269
is efficient because
you still need to

811
00:39:04,269 --> 00:39:07,730
tune the hyperparameters
configurations for this kernel.

812
00:39:07,730 --> 00:39:10,109
And imagine learning, we
call this kernel tuning,

813
00:39:10,109 --> 00:39:12,430
and there are a
lot of frameworks

814
00:39:12,430 --> 00:39:14,249
like petard and tender flow.

815
00:39:14,249 --> 00:39:15,709
When you just set it
up on your model,

816
00:39:15,709 --> 00:39:18,009
you are with a
pretty long time for

817
00:39:18,009 --> 00:39:20,630
it to profile program and try
to tune those parameters.

818
00:39:20,630 --> 00:39:22,189
That's basically kernel tuning.

819
00:39:22,189 --> 00:39:24,669
And Kert tuning is a very
difficult problem because,

820
00:39:24,669 --> 00:39:27,910
uh we have so many models,

821
00:39:27,910 --> 00:39:31,610
transformers, STRs Net, they
have different operators.

822
00:39:31,610 --> 00:39:33,709
Each operator ready
have different shape.

823
00:39:33,709 --> 00:39:36,189
For example, metam
of one kb one k

824
00:39:36,189 --> 00:39:39,269
and fourk definitely different
than the configurations

825
00:39:39,269 --> 00:39:42,550
that I applied on this
telling because they require

826
00:39:42,550 --> 00:39:44,529
different amount of
processing power

827
00:39:44,529 --> 00:39:47,910
and also the memory
hierarchy capacity.

828
00:39:47,910 --> 00:39:49,950
There are different frameworks,

829
00:39:49,950 --> 00:39:51,489
of course, we already
covered that.

830
00:39:51,489 --> 00:39:54,690
I think the thing that
makes since most complex

831
00:39:54,690 --> 00:39:58,350
is there are many different
devices, different GPUs.

832
00:39:58,350 --> 00:39:59,929
For example, media itself has

833
00:39:59,929 --> 00:40:01,250
so many generations of GPUs

834
00:40:01,250 --> 00:40:03,089
with different
configurations, right?

835
00:40:03,089 --> 00:40:05,090
And there are also CPUs.

836
00:40:05,090 --> 00:40:07,649
There are CDN, uh,

837
00:40:07,649 --> 00:40:09,610
there are like TPU bacons,

838
00:40:09,610 --> 00:40:11,910
we also want to
deploy our kernels

839
00:40:11,910 --> 00:40:14,269
on iPhone MacBook, right?

840
00:40:14,269 --> 00:40:16,370
So which makes this
things complicated.

841
00:40:16,370 --> 00:40:18,729
Uh, so if you ask
developers to do this,

842
00:40:18,729 --> 00:40:21,489
for example, even for a single
Mdm, you have to, like,

843
00:40:21,489 --> 00:40:23,610
at least every company, Apple,

844
00:40:23,610 --> 00:40:25,149
Vd Intel, they need to hire

845
00:40:25,149 --> 00:40:26,789
a bunch of engineers to
tune the kernels, right.

846
00:40:26,789 --> 00:40:29,249
They are basically working
on the same original code,

847
00:40:29,249 --> 00:40:31,790
but they just need to tune
different kernel parameters.

848
00:40:31,790 --> 00:40:36,399
Okay? There are essentially
two solution to this.

849
00:40:36,399 --> 00:40:38,159
One is you just
hire more people,

850
00:40:38,159 --> 00:40:40,459
and like I already mentioned,

851
00:40:40,459 --> 00:40:42,920
you ask the experts to craft

852
00:40:42,920 --> 00:40:44,660
those kernels and tune
their performance

853
00:40:44,660 --> 00:40:46,820
day by day and you
give them salary.

854
00:40:46,820 --> 00:40:50,039
You enumerate all these
conflicts and make sure,

855
00:40:50,039 --> 00:40:53,400
yeah, it is good after profile.

856
00:40:53,400 --> 00:40:55,420
The second solution is basically

857
00:40:55,420 --> 00:40:57,199
a more ambitious
solution that is

858
00:40:57,199 --> 00:40:59,579
basically what machinering
compiler people are doing.

859
00:40:59,579 --> 00:41:01,319
They are saying, like,
how about I give you

860
00:41:01,319 --> 00:41:03,800
a system, which is a compiler,

861
00:41:03,800 --> 00:41:06,480
and you give me your operator,

862
00:41:06,480 --> 00:41:08,640
and you also give
me your devices,

863
00:41:08,640 --> 00:41:10,560
and you through them
to that compiler,

864
00:41:10,560 --> 00:41:11,399
and that compiler will

865
00:41:11,399 --> 00:41:13,519
automatically figure
out everything for you.

866
00:41:13,519 --> 00:41:17,879
Okay. That basically brings
us into the second part of,

867
00:41:17,879 --> 00:41:21,760
uh, and also a very important
topic emergency system.

868
00:41:21,760 --> 00:41:27,939
I think, between the
year 2018 and 2020, uh,

869
00:41:27,939 --> 00:41:29,620
the entire community,

870
00:41:29,620 --> 00:41:31,739
they are doing compilers
because they are

871
00:41:31,739 --> 00:41:34,600
really like this vision

872
00:41:34,600 --> 00:41:36,619
because they can
reduce labors, right?

873
00:41:36,619 --> 00:41:38,560
They can pay salary
to engineers.

874
00:41:38,560 --> 00:41:39,919
They try to automate something.

875
00:41:39,919 --> 00:41:44,040
Okay. So here is
machining compilation.

876
00:41:44,040 --> 00:41:47,059
Obviously the primary goal
of machining compilation is

877
00:41:47,059 --> 00:41:48,440
I try to automatically generate

878
00:41:48,440 --> 00:41:50,759
optimal configurations
on the kernel code,

879
00:41:50,759 --> 00:41:53,259
giving users high level
code, for example,

880
00:41:53,259 --> 00:41:55,480
written in ten flow
Petro or even Python,

881
00:41:55,480 --> 00:41:59,959
and target hardware, if
I can build this layer,

882
00:41:59,959 --> 00:42:03,999
then this is going to be
superpower because it's going to

883
00:42:03,999 --> 00:42:05,899
compare any code into a
highly efficient version

884
00:42:05,899 --> 00:42:08,844
and basically eliminate
human labor, okay?

885
00:42:08,844 --> 00:42:11,129
So let's look at how

886
00:42:11,129 --> 00:42:12,810
machining compiler is different

887
00:42:12,810 --> 00:42:14,190
from traditional compiler.

888
00:42:14,190 --> 00:42:18,409
Okay? So I hope you
guys all took at least,

889
00:42:18,409 --> 00:42:20,589
know what is traditional
compiler, right?

890
00:42:20,589 --> 00:42:22,389
It's basically, for example,

891
00:42:22,389 --> 00:42:24,269
when you write some code,

892
00:42:24,269 --> 00:42:26,029
C plus plus, you can
write in whatever way.

893
00:42:26,029 --> 00:42:28,229
But compiler will
guarantee that they

894
00:42:28,229 --> 00:42:30,689
pass your code and make
it very efficient, right.

895
00:42:30,689 --> 00:42:33,729
It also lower your code into
some machine instructions

896
00:42:33,729 --> 00:42:38,309
and so the instruct the
processors can understand, right?

897
00:42:38,309 --> 00:42:40,490
So this is a
traditional compiler.

898
00:42:40,490 --> 00:42:42,189
Human basically write some code,

899
00:42:42,189 --> 00:42:44,069
for example, CPP or whatever.

900
00:42:44,069 --> 00:42:46,129
And the compiler
will take your code.

901
00:42:46,129 --> 00:42:48,589
It will eliminate all
those efficient code

902
00:42:48,589 --> 00:42:50,489
or repeated code,
unnecessary code.

903
00:42:50,489 --> 00:42:52,829
And then it will try
to lower it into

904
00:42:52,829 --> 00:42:54,469
another format that
is probably not

905
00:42:54,469 --> 00:42:56,430
human readable, but
the process readable.

906
00:42:56,430 --> 00:42:59,769
Instructions. Okay. I need
to give the instruction to,

907
00:42:59,769 --> 00:43:02,309
uh, uh, basically to

908
00:43:02,309 --> 00:43:05,354
the target hardware and hardware
will ask you your code.

909
00:43:05,354 --> 00:43:09,300
E. Imagine learning,
this is very similar.

910
00:43:09,300 --> 00:43:11,819
And here I basically
draw the stack here.

911
00:43:11,819 --> 00:43:15,499
Remember immerse learning
how we write code.

912
00:43:15,499 --> 00:43:18,280
We basically compose
dataflow graphs,

913
00:43:18,280 --> 00:43:21,260
we use Tinder flow
and Patrig API,

914
00:43:21,260 --> 00:43:23,659
and we try to take
care of this layer.

915
00:43:23,659 --> 00:43:25,859
We convert our idea about

916
00:43:25,859 --> 00:43:28,979
our model into
dataflow graph, okay?

917
00:43:28,979 --> 00:43:30,759
And the promise of compiler is

918
00:43:30,759 --> 00:43:32,479
basically, starting from there,

919
00:43:32,479 --> 00:43:35,380
you don't have to optimize
say graph transformation,

920
00:43:35,380 --> 00:43:37,239
you don't have to
optimize your kernel code

921
00:43:37,239 --> 00:43:39,240
or whatever, random scheduling.

922
00:43:39,240 --> 00:43:41,899
And I'm going to take a full
stack solution for you.

923
00:43:41,899 --> 00:43:44,639
I'm going to take over.
I will take your code.

924
00:43:44,639 --> 00:43:47,800
I will automatically transform
it into Dataflograph.

925
00:43:47,800 --> 00:43:49,919
And then I have
some code that will

926
00:43:49,919 --> 00:43:51,360
automatically translate

927
00:43:51,360 --> 00:43:53,819
your data flow graph
into efficient version.

928
00:43:53,819 --> 00:43:55,659
And then once I

929
00:43:55,659 --> 00:43:58,099
have the eventual version of
data flograph what I do is

930
00:43:58,099 --> 00:44:00,099
basically I ask you

931
00:44:00,099 --> 00:44:01,860
from forward and backward

932
00:44:01,860 --> 00:44:03,540
following the
operator definition.

933
00:44:03,540 --> 00:44:05,480
So I will also help
you to generate

934
00:44:05,480 --> 00:44:06,659
efficient kernel code for

935
00:44:06,659 --> 00:44:09,020
each operator you use
in your data flograph.

936
00:44:09,020 --> 00:44:11,299
Okay. And this is

937
00:44:11,299 --> 00:44:13,400
what machining compiler
tries to permiss.

938
00:44:13,400 --> 00:44:15,759
And once you add this layer,

939
00:44:15,759 --> 00:44:17,699
I think we are good because once

940
00:44:17,699 --> 00:44:19,099
we have that efficient
kernel code,

941
00:44:19,099 --> 00:44:20,840
we studied in the previous
several lectures,

942
00:44:20,840 --> 00:44:22,280
we can just give
the code to Quota

943
00:44:22,280 --> 00:44:23,780
compiler or whatever compiler.

944
00:44:23,780 --> 00:44:26,840
And they will basically
they will basically

945
00:44:26,840 --> 00:44:30,559
take over and transform
it into machine code.

946
00:44:30,559 --> 00:44:32,579
You can see machining
compiler is

947
00:44:32,579 --> 00:44:35,300
slightly higher level than
traditional compiler.

948
00:44:35,300 --> 00:44:37,039
It will take your
data flow graph and

949
00:44:37,039 --> 00:44:38,619
compile it all the way down

950
00:44:38,619 --> 00:44:39,859
into kernel code and

951
00:44:39,859 --> 00:44:43,039
then traditional
compiler will take over.

952
00:44:43,039 --> 00:44:46,019
Okay. Any problem here?

953
00:44:46,050 --> 00:44:48,510
Okay, this is a pretty big scope

954
00:44:48,510 --> 00:44:49,850
because if you have a compiler,

955
00:44:49,850 --> 00:44:51,290
that means most of the marchey

956
00:44:51,290 --> 00:44:52,709
system problem will be solved.

957
00:44:52,709 --> 00:44:55,529
Okay. So so what kind of
problem we face here?

958
00:44:55,529 --> 00:44:58,740
So uh, apparently,
there are three layers.

959
00:44:58,740 --> 00:45:01,859
At the first layer,
programming level,

960
00:45:01,859 --> 00:45:04,399
we face very difficult program

961
00:45:04,399 --> 00:45:05,860
that is automatically transform

962
00:45:05,860 --> 00:45:10,400
arbitrary and usually
imperative code by developers,

963
00:45:10,400 --> 00:45:12,079
into a compatible code.

964
00:45:12,079 --> 00:45:14,660
And I think we talked about
this in the second lecture.

965
00:45:14,660 --> 00:45:18,199
This is already pretty
hard because like I said,

966
00:45:18,199 --> 00:45:19,700
dataflow graph is so static,

967
00:45:19,700 --> 00:45:22,020
but user code are so dynamic.

968
00:45:22,020 --> 00:45:24,099
And there's a gap,
how we do that.

969
00:45:24,099 --> 00:45:26,880
We touch based on that. I
hope you still remember.

970
00:45:26,880 --> 00:45:29,879
And you can see this area of
research is basically being

971
00:45:29,879 --> 00:45:33,559
conducted by most of
programming language people.

972
00:45:33,559 --> 00:45:35,799
Programming language people
really like doing this

973
00:45:35,799 --> 00:45:38,359
and they all look at

974
00:45:38,359 --> 00:45:39,839
this area that is taking

975
00:45:39,839 --> 00:45:41,099
your machine learning
code and try

976
00:45:41,099 --> 00:45:44,119
to convert your dynamic
code into study code.

977
00:45:44,119 --> 00:45:45,899
Okay? And this touch base

978
00:45:45,899 --> 00:45:48,009
with programming language, okay?

979
00:45:48,009 --> 00:45:52,020
In a graph level, what the
compiler does is basically,

980
00:45:52,020 --> 00:45:55,120
we try to perform some
automatic graph transformation.

981
00:45:55,120 --> 00:45:56,659
We also talked
about this a little

982
00:45:56,659 --> 00:45:58,080
bit, in our early lectures.

983
00:45:58,080 --> 00:46:01,539
That is, we have a stNt
we can optimize it in

984
00:46:01,539 --> 00:46:03,539
many runs and then
eventually get a graph

985
00:46:03,539 --> 00:46:06,219
that is 30 times
faster or something.

986
00:46:06,219 --> 00:46:07,979
And this is basically being

987
00:46:07,979 --> 00:46:10,139
touchb by some graph
theory people.

988
00:46:10,139 --> 00:46:12,840
Okay, they like to
analyze in graphs,

989
00:46:12,840 --> 00:46:14,139
so they take graphs and try

990
00:46:14,139 --> 00:46:15,620
to find the equivalent graphs.

991
00:46:15,620 --> 00:46:18,459
Okay. And at the last layer,

992
00:46:18,459 --> 00:46:22,179
what do we do is helping
heavily doing this, right?

993
00:46:22,179 --> 00:46:24,919
So how to make operator fast.

994
00:46:24,919 --> 00:46:26,559
So basically, they will take

995
00:46:26,559 --> 00:46:28,060
a high level definition

996
00:46:28,060 --> 00:46:30,060
of your operator, for
example, metamor.

997
00:46:30,060 --> 00:46:31,920
When you write Metamo in Python,

998
00:46:31,920 --> 00:46:34,239
what you do is basically
you do a layer loop.

999
00:46:34,239 --> 00:46:36,480
But they will take that loop
and they try to generate

1000
00:46:36,480 --> 00:46:38,019
a very efficient kernel code

1001
00:46:38,019 --> 00:46:40,880
that we did in the first
part of this tecture.

1002
00:46:40,880 --> 00:46:42,419
Which is auto those
kind of code.

1003
00:46:42,419 --> 00:46:44,719
Okay, coda, on GPS and which

1004
00:46:44,719 --> 00:46:48,259
was automate the
entire process. Okay.

1005
00:46:48,259 --> 00:46:52,299
Um, uh, this is a
pretty big area,

1006
00:46:52,299 --> 00:46:54,160
but in this lecture

1007
00:46:54,160 --> 00:46:56,019
because you still
remember the big picture,

1008
00:46:56,019 --> 00:46:58,299
we are trying to
optimize operator code.

1009
00:46:58,299 --> 00:47:00,339
So I'm going to touch
base a little bit on how

1010
00:47:00,339 --> 00:47:02,600
to do this using compilers.

1011
00:47:02,600 --> 00:47:04,500
Okay. And in later lectures,

1012
00:47:04,500 --> 00:47:07,569
we are going to touch
base other layers, okay?

1013
00:47:07,569 --> 00:47:09,819
So before that, I
want to give you

1014
00:47:09,819 --> 00:47:10,919
a high level picture what is

1015
00:47:10,919 --> 00:47:12,059
going on in the compiler domain.

1016
00:47:12,059 --> 00:47:14,940
So I'm going to lame a
few famous compilers

1017
00:47:14,940 --> 00:47:17,960
and give you an introduction
of what they are cooking.

1018
00:47:17,960 --> 00:47:21,399
And what are the people
behind them, okay?

1019
00:47:21,399 --> 00:47:26,120
So the first one, of
course, from Google,

1020
00:47:26,120 --> 00:47:28,600
right, uh, it's called XLA,

1021
00:47:28,600 --> 00:47:31,299
XLA, if you look at Logo.

1022
00:47:31,299 --> 00:47:33,979
It's very similar
to tens TF. Okay?

1023
00:47:33,979 --> 00:47:36,120
So this SLE was actually

1024
00:47:36,120 --> 00:47:37,479
uh I would say it's

1025
00:47:37,479 --> 00:47:39,299
the first compiler
for machine learning.

1026
00:47:39,299 --> 00:47:41,099
It was released together with,

1027
00:47:41,099 --> 00:47:43,999
um, with Tenerflow in 2016.

1028
00:47:43,999 --> 00:47:45,539
And you can see how Google is,

1029
00:47:45,539 --> 00:47:47,319
uh, visionary on this.

1030
00:47:47,319 --> 00:47:48,439
So basically, when they build

1031
00:47:48,439 --> 00:47:49,539
the first region of tener flow,

1032
00:47:49,539 --> 00:47:50,940
you already think
about the compiler.

1033
00:47:50,940 --> 00:47:54,599
Yeah, because I think when
they build Tener flow,

1034
00:47:54,599 --> 00:47:55,839
they realize that
they have to ask

1035
00:47:55,839 --> 00:47:57,560
their Google
developers to develop

1036
00:47:57,560 --> 00:47:59,200
more than 200 operators

1037
00:47:59,200 --> 00:48:01,420
on at least three
kinds of hardware,

1038
00:48:01,420 --> 00:48:02,860
CPU GPU and TPU.

1039
00:48:02,860 --> 00:48:05,679
And that is already 60
combinations, right?

1040
00:48:05,679 --> 00:48:07,439
And that takes a lot of time.

1041
00:48:07,439 --> 00:48:09,920
So they start thinking about
how to build a compiler,

1042
00:48:09,920 --> 00:48:13,939
this compiler is called
SOE and if you look,

1043
00:48:13,939 --> 00:48:15,979
if you look back,
you can see, uh,

1044
00:48:15,979 --> 00:48:17,879
this is so good that today,

1045
00:48:17,879 --> 00:48:21,739
most of tenser flows back
is based on this one.

1046
00:48:21,739 --> 00:48:23,479
So basically what I mean is

1047
00:48:23,479 --> 00:48:25,240
the throw away all the operators

1048
00:48:25,240 --> 00:48:27,580
written by their own developers

1049
00:48:27,580 --> 00:48:30,659
and use SLA to
generate code. Okay?

1050
00:48:30,659 --> 00:48:33,559
And SLAs also
extending its scope to

1051
00:48:33,559 --> 00:48:37,189
support petarg and any
other frameworks, okay?

1052
00:48:37,189 --> 00:48:40,519
Second compiler TBM, right,
Tensor virtual machine.

1053
00:48:40,519 --> 00:48:42,760
This is a very, very famous
project from academia.

1054
00:48:42,760 --> 00:48:45,179
And after two weeks,

1055
00:48:45,179 --> 00:48:46,700
the leader of this project,

1056
00:48:46,700 --> 00:48:50,600
TQ Chen, was he's a
professor at CMU.

1057
00:48:50,600 --> 00:48:52,359
He's going to give
us guest talk.

1058
00:48:52,359 --> 00:48:54,819
Okay? And this one is one of

1059
00:48:54,819 --> 00:48:58,639
the most successful
compiler in academia, okay?

1060
00:48:58,639 --> 00:49:01,199
And it mostly focus on
it's also open source,

1061
00:49:01,199 --> 00:49:03,299
and it mostly focus on,

1062
00:49:03,340 --> 00:49:06,920
like, compiling code for
inference, not for training.

1063
00:49:06,920 --> 00:49:08,559
Okay. That means there's

1064
00:49:08,559 --> 00:49:12,539
no backward pass for the
code generated by TVM, okay?

1065
00:49:12,539 --> 00:49:18,500
And I think the researchers
and students who develop TM,

1066
00:49:18,500 --> 00:49:20,540
they went out and do a startup,

1067
00:49:20,540 --> 00:49:24,479
which is octo, they
raise quite a lot.

1068
00:49:24,479 --> 00:49:28,319
I think there is
almost 200 M. Okay.

1069
00:49:28,319 --> 00:49:29,839
But the company was not doing

1070
00:49:29,839 --> 00:49:31,459
pretty well and it
was acquired by

1071
00:49:31,459 --> 00:49:35,239
Amedia last year at a similar
price to what the risk.

1072
00:49:35,239 --> 00:49:36,919
Yeah, that is a set story.

1073
00:49:36,919 --> 00:49:41,439
Okay. Yeah. Okay. The third one,

1074
00:49:41,439 --> 00:49:43,059
you probably know,
and this is the one

1075
00:49:43,059 --> 00:49:45,679
that attracts the
most user, right?

1076
00:49:45,679 --> 00:49:51,220
2.0, uh, which is essentially
torch dot compile.

1077
00:49:51,220 --> 00:49:53,239
And like I said, torch is

1078
00:49:53,239 --> 00:49:55,219
really good at doing
something really late,

1079
00:49:55,219 --> 00:49:58,200
but still win over the
previous, players.

1080
00:49:58,200 --> 00:49:59,959
Yeah. So I think the reason

1081
00:49:59,959 --> 00:50:02,089
it wins is because like I said,

1082
00:50:02,089 --> 00:50:03,849
tort starts from imperative

1083
00:50:03,849 --> 00:50:05,789
programming without
compilation, right?

1084
00:50:05,789 --> 00:50:07,689
But they gradually migrate into

1085
00:50:07,689 --> 00:50:09,890
a compiler friendly framework.

1086
00:50:09,890 --> 00:50:12,129
Uh once you have
a deep user base

1087
00:50:12,129 --> 00:50:14,109
that is get addicted to
your programming model,

1088
00:50:14,109 --> 00:50:15,429
they are not going to
switch, and you can

1089
00:50:15,429 --> 00:50:17,329
add whatever feature you
want, and they will use that.

1090
00:50:17,329 --> 00:50:19,269
Okay. Uh in fact,

1091
00:50:19,269 --> 00:50:21,770
Excel is much better
than Toti Compil.

1092
00:50:21,770 --> 00:50:23,689
But I don't think people
like that because

1093
00:50:23,689 --> 00:50:26,610
Tn flow itself is too
hard to program, okay?

1094
00:50:26,610 --> 00:50:28,989
And, and in to two point,

1095
00:50:28,989 --> 00:50:31,650
all what they do
is they add, uh,

1096
00:50:31,650 --> 00:50:33,630
a few like compiler related,

1097
00:50:33,630 --> 00:50:35,309
interfaces and
they are trying to

1098
00:50:35,309 --> 00:50:38,269
advocate to use T
D comple. Yeah.

1099
00:50:41,200 --> 00:50:45,040
No. Today, SLE also
works for Torch.

1100
00:50:45,040 --> 00:50:47,800
Yeah. So I think Meta and
Google they are collaborating.

1101
00:50:47,800 --> 00:50:50,599
Yeah. The reason Torch
wants to support XOI is

1102
00:50:50,599 --> 00:50:54,600
because XLA is the best
backend for running on TPUs.

1103
00:50:54,600 --> 00:50:56,399
And people like users or

1104
00:50:56,399 --> 00:50:58,620
petri they also want to
deploy their job on TPUs,

1105
00:50:58,620 --> 00:51:01,945
so they have to integrate, okay?

1106
00:51:01,945 --> 00:51:05,769
Each of these compilers
are specific?

1107
00:51:08,490 --> 00:51:11,370
They are not
specific frameworks.

1108
00:51:11,370 --> 00:51:13,669
I think today, if you
look at mechanism,

1109
00:51:13,669 --> 00:51:16,250
the frameworks concept
is kind of fitted.

1110
00:51:16,250 --> 00:51:18,089
It's more like a front end

1111
00:51:18,089 --> 00:51:20,809
programming language,
a Bon engine.

1112
00:51:20,809 --> 00:51:22,950
I can essentially combine

1113
00:51:22,950 --> 00:51:25,629
my Petros favorite imperative
programming language or

1114
00:51:25,629 --> 00:51:28,569
programming interfaces to
a bacon that is XOI based.

1115
00:51:28,569 --> 00:51:32,069
That's also fine.
Yeah. But of course,

1116
00:51:32,069 --> 00:51:35,030
I think if you want the
best user experience,

1117
00:51:35,030 --> 00:51:38,529
you should use that
framework. Yeah. Okay.

1118
00:51:38,880 --> 00:51:40,979
And for this one, uh,

1119
00:51:40,979 --> 00:51:43,159
I think you guys use a lot.

1120
00:51:43,159 --> 00:51:44,859
And the last one
is modular, okay?

1121
00:51:44,859 --> 00:51:48,279
This modular is a company
which raised almost 300 today.

1122
00:51:48,279 --> 00:51:51,259
Okay? It was built by a
guy called Chris Lautner.

1123
00:51:51,259 --> 00:51:52,819
And if you guys are
compiler people,

1124
00:51:52,819 --> 00:51:54,019
you probably know his name.

1125
00:51:54,019 --> 00:51:57,189
He was an inventor of LLVM.

1126
00:51:57,189 --> 00:52:00,249
Okay, LLVM is a
default compiler today

1127
00:52:00,249 --> 00:52:01,929
for compiling many C plus

1128
00:52:01,929 --> 00:52:03,789
pass code into
diverse platforms.

1129
00:52:03,789 --> 00:52:06,789
Okay. He's one of the
most successful compiler.

1130
00:52:06,789 --> 00:52:08,789
And this guy, apparently, he

1131
00:52:08,789 --> 00:52:10,049
wants to do machinery as well,

1132
00:52:10,049 --> 00:52:12,569
and he tried to build
under startup that tried

1133
00:52:12,569 --> 00:52:15,509
to build the world's
greatest compilers.

1134
00:52:15,509 --> 00:52:17,429
And this guy, Chris Lateran,

1135
00:52:17,429 --> 00:52:19,669
if you search Google,
you'll find him.

1136
00:52:19,669 --> 00:52:21,589
He's a very famous
people. So he worked for

1137
00:52:21,589 --> 00:52:24,189
Google for a few years.

1138
00:52:24,189 --> 00:52:28,169
Uh, but, and Google gives
him a pretty high title,

1139
00:52:28,169 --> 00:52:30,209
at least director level,

1140
00:52:30,209 --> 00:52:32,469
and, of course, big package.

1141
00:52:32,469 --> 00:52:34,589
But he only worked
there for one year,

1142
00:52:34,589 --> 00:52:37,049
and he was not able to
work with Google people

1143
00:52:37,049 --> 00:52:40,469
because he wants to push
his own idea on compiler,

1144
00:52:40,469 --> 00:52:41,789
but Google is not
going to accept that.

1145
00:52:41,789 --> 00:52:43,409
Google wants to push XOI.

1146
00:52:43,409 --> 00:52:49,489
So, then he basically left
Google and joined Apple, okay?

1147
00:52:49,489 --> 00:52:52,435
And in Apple, he wants to
push a language called Swift.

1148
00:52:52,435 --> 00:52:55,299
So you guys know
many many Apple apps

1149
00:52:55,299 --> 00:52:57,679
are basically program
using Swift, not Python.

1150
00:52:57,679 --> 00:52:59,599
And you can understand Swift as

1151
00:52:59,599 --> 00:53:01,899
a very magical language that

1152
00:53:01,899 --> 00:53:05,419
can interpret between
Python and C plus plus.

1153
00:53:05,419 --> 00:53:09,280
So you can write, it's like
today's machinery framework.

1154
00:53:09,280 --> 00:53:12,579
You can write in a way that
is very similar to Python,

1155
00:53:12,579 --> 00:53:14,739
very easy to understand, but
you can directly convert

1156
00:53:14,739 --> 00:53:17,219
that code into a symbolic
version that can compile.

1157
00:53:17,219 --> 00:53:19,739
So he really likes that,
and he pushed pretty hard

1158
00:53:19,739 --> 00:53:22,399
on Swift on both
Google and Apple.

1159
00:53:22,399 --> 00:53:24,759
I think he was successful
on Apple and many,

1160
00:53:24,759 --> 00:53:26,954
many Apples code was
written in Swift.

1161
00:53:26,954 --> 00:53:28,549
And then at some point, he left

1162
00:53:28,549 --> 00:53:30,350
Apple and started
doing this modular.

1163
00:53:30,350 --> 00:53:33,069
And I think while
or two years ago,

1164
00:53:33,069 --> 00:53:34,909
he start this company,

1165
00:53:34,909 --> 00:53:37,229
posted something like,

1166
00:53:37,229 --> 00:53:40,760
um they are 20 times faster
than priority, okay?

1167
00:53:40,760 --> 00:53:44,919
And that post actually messed
up with priority people,

1168
00:53:44,919 --> 00:53:46,399
uh, because, you know,

1169
00:53:46,399 --> 00:53:47,599
in the world, it's very hard to

1170
00:53:47,599 --> 00:53:49,339
be 20 times another baseline,

1171
00:53:49,339 --> 00:53:53,080
There must be something that
is not correctly evaluated.

1172
00:53:53,080 --> 00:53:55,679
And there was a strong
debate on if modular

1173
00:53:55,679 --> 00:53:58,400
is indeed 20 times
faster than priority,

1174
00:53:58,400 --> 00:54:00,279
and if you do some
search on Google,

1175
00:54:00,279 --> 00:54:01,859
you will find it out. Okay?

1176
00:54:01,859 --> 00:54:04,359
These are two main
compilers, okay?

1177
00:54:04,359 --> 00:54:06,659
Uh I think four years ago,

1178
00:54:06,659 --> 00:54:08,199
there are more compiler
than this, but they all

1179
00:54:08,199 --> 00:54:10,499
died. Only this four survive.

1180
00:54:10,499 --> 00:54:15,919
Okay? That means it's not a
good research to do today.

1181
00:54:15,919 --> 00:54:18,260
Okay. Yeah, I
wouldn't do compiler.

1182
00:54:18,260 --> 00:54:21,199
Okay. So what is,

1183
00:54:21,199 --> 00:54:23,799
let's focus on the
underlying uh,

1184
00:54:23,799 --> 00:54:26,139
um, mechanisms for
compiler, okay?

1185
00:54:26,139 --> 00:54:27,639
So today we are going to focus

1186
00:54:27,639 --> 00:54:28,840
mostly on operator combination.

1187
00:54:28,840 --> 00:54:30,659
That is a low risk layer.
That is Uder give you

1188
00:54:30,659 --> 00:54:32,279
a operator code at a high level

1189
00:54:32,279 --> 00:54:33,699
with a high level definition,

1190
00:54:33,699 --> 00:54:38,639
how we can compile this code
into low level programs,

1191
00:54:38,639 --> 00:54:41,079
for example, Coda kernels

1192
00:54:41,079 --> 00:54:43,679
or tiled matm this kind of code.

1193
00:54:43,679 --> 00:54:45,419
Okay? I think this

1194
00:54:45,419 --> 00:54:46,939
basically give you a
higher idea, right?

1195
00:54:46,939 --> 00:54:48,719
So der write this kind of code,

1196
00:54:48,719 --> 00:54:51,099
which is mostly
just declaration,

1197
00:54:51,099 --> 00:54:53,004
not for efficiency, okay?

1198
00:54:53,004 --> 00:54:55,749
And the mission of the
compiler is basically,

1199
00:54:55,749 --> 00:54:57,689
I'm going to take
this piece of code.

1200
00:54:57,689 --> 00:54:59,949
I'm going to find
a loop that works

1201
00:54:59,949 --> 00:55:01,889
best for your code
on your hardware.

1202
00:55:01,889 --> 00:55:03,929
Because there are so
many ways to loop,

1203
00:55:03,929 --> 00:55:05,550
depending on the looping factor.

1204
00:55:05,550 --> 00:55:08,369
Okay. So eventually it
boils down into finding

1205
00:55:08,369 --> 00:55:09,929
the perfect loop for
your hardware for

1206
00:55:09,929 --> 00:55:12,790
that shape that memo
for the operator.

1207
00:55:12,790 --> 00:55:15,049
Okay? In order to do this,

1208
00:55:15,049 --> 00:55:17,309
I will give you a
workflow, okay?

1209
00:55:17,309 --> 00:55:19,889
So here the user writes a
very simple code that is,

1210
00:55:19,889 --> 00:55:23,809
um this is ready at
A plus B equal to

1211
00:55:23,809 --> 00:55:26,229
C. But depending on

1212
00:55:26,229 --> 00:55:29,729
the shape of ABC and also
the target hardware,

1213
00:55:29,729 --> 00:55:31,949
for example, the
lumber registers,

1214
00:55:31,949 --> 00:55:33,669
the lumber shared memory,

1215
00:55:33,669 --> 00:55:36,689
you can generate many different
versions of this code.

1216
00:55:36,689 --> 00:55:39,569
You can basically
loop in this way.

1217
00:55:39,569 --> 00:55:41,809
You can also loop in this way.

1218
00:55:41,809 --> 00:55:44,349
We probably know the
difference because I can't

1219
00:55:44,349 --> 00:55:45,609
tell this layer tell this layer

1220
00:55:45,609 --> 00:55:46,989
at a different memory hierarchy.

1221
00:55:46,989 --> 00:55:49,349
And also, I can
switch the access.

1222
00:55:49,349 --> 00:55:51,729
I make this for about
this 32 and I will get

1223
00:55:51,729 --> 00:55:53,010
probably a different performance

1224
00:55:53,010 --> 00:55:55,769
depending on the memory
hierarchy capacity.

1225
00:55:56,090 --> 00:55:58,530
And eventually the compiler

1226
00:55:58,530 --> 00:55:59,849
wants to find the perfect loop

1227
00:55:59,849 --> 00:56:03,009
and then basically lower
this kind of loop.

1228
00:56:03,009 --> 00:56:04,269
This loop is in Picon code,

1229
00:56:04,269 --> 00:56:06,549
I want to lower this loop
into some kernel code.

1230
00:56:06,549 --> 00:56:09,089
And all this is being
done automatically.

1231
00:56:09,089 --> 00:56:14,010
Okay? That is the high level
idea of operator compiler.

1232
00:56:14,429 --> 00:56:17,829
So now you understand

1233
00:56:17,829 --> 00:56:19,049
the problem and let's dive

1234
00:56:19,049 --> 00:56:20,909
deeper on the exact problem
we're trying to solve.

1235
00:56:20,909 --> 00:56:22,469
So we are ficing a
lot of problem if

1236
00:56:22,469 --> 00:56:24,030
you want to build this
kind of compiler.

1237
00:56:24,030 --> 00:56:29,049
One is and we need to enumerate
all the possibilities.

1238
00:56:29,049 --> 00:56:30,689
Then the problem is how I can

1239
00:56:30,689 --> 00:56:32,969
represent all the possibilities.

1240
00:56:32,969 --> 00:56:38,599
Or more concretely how I
represent the loops. Right.

1241
00:56:38,599 --> 00:56:40,539
I need to represent
loops. Okay, I

1242
00:56:40,539 --> 00:56:41,719
need to find the repreenton so I

1243
00:56:41,719 --> 00:56:45,339
can basically search over
all the repenting space.

1244
00:56:45,339 --> 00:56:49,139
Second is once I have that
repenting space of loops,

1245
00:56:49,139 --> 00:56:52,219
how to find the close
to optimal value.

1246
00:56:52,219 --> 00:56:54,679
Like should I loop over
the first dimension

1247
00:56:54,679 --> 00:56:56,979
of four second dimensthirty
two or another way?

1248
00:56:56,979 --> 00:57:00,179
I need to This is
optimiing problem.

1249
00:57:00,179 --> 00:57:02,059
Right I have a space.

1250
00:57:02,059 --> 00:57:04,254
I want to optimize
that space, okay?

1251
00:57:04,254 --> 00:57:06,949
And finally, I want to

1252
00:57:06,949 --> 00:57:09,889
basically accelerate this
process because I cannot

1253
00:57:09,889 --> 00:57:12,489
put this as unsolvable
problem that I have

1254
00:57:12,489 --> 00:57:15,829
a program that I search
over for my entire life.

1255
00:57:15,829 --> 00:57:17,429
I want to reduce
the search space,

1256
00:57:17,429 --> 00:57:19,109
and I want to generalize

1257
00:57:19,109 --> 00:57:21,529
this kind of thing from
one device to the other.

1258
00:57:21,529 --> 00:57:25,329
For example, I search one
loop for GPU even hundred.

1259
00:57:25,329 --> 00:57:26,609
I also want this loop to work

1260
00:57:26,609 --> 00:57:29,549
reasonably well on H 100,
something like that.

1261
00:57:29,549 --> 00:57:32,769
Okay? So there are

1262
00:57:32,769 --> 00:57:34,369
many ways to build
this kind of compiler,

1263
00:57:34,369 --> 00:57:37,469
and I'm going to
introduce roughly one way

1264
00:57:37,469 --> 00:57:40,109
that adopted in
today's compiler, TVM.

1265
00:57:40,109 --> 00:57:41,789
Okay? So in TVM,

1266
00:57:41,789 --> 00:57:42,909
in order to solve this problem,

1267
00:57:42,909 --> 00:57:44,249
what they do is, uh,

1268
00:57:44,249 --> 00:57:46,969
they will first define
the three space.

1269
00:57:46,969 --> 00:57:50,010
And then, they have
some algorithm,

1270
00:57:50,010 --> 00:57:52,069
which basically what
navigated this space.

1271
00:57:52,069 --> 00:57:53,570
Okay? And why enumerate

1272
00:57:53,570 --> 00:57:56,449
all the possibility given
your user code and,

1273
00:57:56,449 --> 00:57:58,659
uh, your target hardware.

1274
00:57:58,659 --> 00:58:00,809
And they also have
some code generator

1275
00:58:00,809 --> 00:58:02,909
that once you lock down one
possibility, for example,

1276
00:58:02,909 --> 00:58:04,170
you have a high level repeton

1277
00:58:04,170 --> 00:58:05,929
with that loop and
you figure out,

1278
00:58:05,929 --> 00:58:10,689
the register and the looping
factors you try to choose.

1279
00:58:10,689 --> 00:58:12,849
Another layer, we'll
take that loop

1280
00:58:12,849 --> 00:58:14,929
and convert that code
into kernel code.

1281
00:58:14,929 --> 00:58:17,289
This is a typical
problem in compiler.

1282
00:58:17,289 --> 00:58:18,810
It's called code generation.

1283
00:58:18,810 --> 00:58:22,229
And today, this can be
done by ARMs, by the way,

1284
00:58:22,229 --> 00:58:24,349
but this was done by,

1285
00:58:24,349 --> 00:58:27,214
uh, like handcraft coding, okay?

1286
00:58:27,214 --> 00:58:30,879
And eventually you will
get a lot of data,

1287
00:58:30,879 --> 00:58:36,659
which is a loop and
a compiled kernel,

1288
00:58:36,659 --> 00:58:38,479
you're going to
apply this kernel

1289
00:58:38,479 --> 00:58:40,659
directly into your
target device.

1290
00:58:40,659 --> 00:58:43,219
So you'll get a performance.

1291
00:58:43,219 --> 00:58:47,079
For example, this kernel will
run in 100 milliseconds,

1292
00:58:47,079 --> 00:58:49,019
uh, on this given chip.

1293
00:58:49,019 --> 00:58:53,020
So you're going to get all
these kind of data pairs.

1294
00:58:53,020 --> 00:58:54,599
So you can either stop here.

1295
00:58:54,599 --> 00:58:56,679
That is, you just pick
the best performing

1296
00:58:56,679 --> 00:58:59,639
one and you return the code
to the user, you are good.

1297
00:58:59,639 --> 00:59:01,939
Or if you want to
generatee a little bit,

1298
00:59:01,939 --> 00:59:04,709
you can take this as a
machine and training data.

1299
00:59:04,709 --> 00:59:08,339
Okay. The X is
basically the loop.

1300
00:59:08,339 --> 00:59:10,279
The Y is basically
the performance of

1301
00:59:10,279 --> 00:59:12,460
that loop on the
target hardware.

1302
00:59:12,460 --> 00:59:16,179
And you train these XY pairs
into a neural network,

1303
00:59:16,179 --> 00:59:19,579
and you let this new network
to predict in the future,

1304
00:59:19,579 --> 00:59:21,039
if you want to switch the loop,

1305
00:59:21,039 --> 00:59:22,879
how much performance
I would get.

1306
00:59:22,879 --> 00:59:25,639
And in this way, once I have
this performance model,

1307
00:59:25,639 --> 00:59:27,619
um, I can basically,

1308
00:59:27,619 --> 00:59:29,559
uh, you know, generalize
a little bit.

1309
00:59:29,559 --> 00:59:30,979
Like s I have a different shape

1310
00:59:30,979 --> 00:59:32,400
when I have a
different hardware,

1311
00:59:32,400 --> 00:59:33,699
I can immediately do

1312
00:59:33,699 --> 00:59:36,039
inference and try to get
the perfect loop for me.

1313
00:59:36,039 --> 00:59:38,824
Okay? Does it make sense?

1314
00:59:38,824 --> 00:59:42,189
Cool. And this one is
called Auto TM and, uh,

1315
00:59:42,189 --> 00:59:44,249
it's a very famous layer,

1316
00:59:44,249 --> 00:59:47,549
uh, in TM compiler, y.

1317
00:59:47,549 --> 00:59:51,409
But here, uh, you roughly get
the idea of this workflow,

1318
00:59:51,409 --> 00:59:53,349
but there are still
many many problems.

1319
00:59:53,349 --> 00:59:55,929
Why is actually represent loops.

1320
00:59:55,929 --> 00:59:58,629
Okay? One way to represent
the loop is basically,

1321
00:59:58,629 --> 01:00:03,219
uh, we still need experts to
write a lot of templates.

1322
01:00:03,219 --> 01:00:06,919
Okay? So we just ask some
expert to write this template.

1323
01:00:06,919 --> 01:00:09,699
For example, this is
a loop we did for

1324
01:00:09,699 --> 01:00:11,139
com two D and we are going

1325
01:00:11,139 --> 01:00:13,099
to ask this experts
to generate many,

1326
01:00:13,099 --> 01:00:14,499
many of these versions loops.

1327
01:00:14,499 --> 01:00:18,919
But we leave this looping
factor as a placeholder,

1328
01:00:18,919 --> 01:00:21,799
and we let our compiler
to automatically find out

1329
01:00:21,799 --> 01:00:25,239
the values to fill this
value into looping factor.

1330
01:00:25,239 --> 01:00:28,139
And then I can go back
to this workflow, right?

1331
01:00:28,219 --> 01:00:31,099
Um the other problem
is once I have

1332
01:00:31,099 --> 01:00:33,499
this kind of looping templates,

1333
01:00:33,499 --> 01:00:35,849
how do I basically navigate

1334
01:00:35,849 --> 01:00:38,749
the space and try to search
for the optimal answer,

1335
01:00:38,749 --> 01:00:40,569
right, in the fast possible way.

1336
01:00:40,569 --> 01:00:42,969
There are a few ways. One is,

1337
01:00:42,969 --> 01:00:45,029
this is a typical
search problem, right?

1338
01:00:45,029 --> 01:00:49,829
So say, I have one looping
template, which I want to try.

1339
01:00:49,829 --> 01:00:52,089
What I do is basically
every time I fill

1340
01:00:52,089 --> 01:00:54,709
some proposed values
into this loop,

1341
01:00:54,709 --> 01:00:57,329
and I can go directly evaluate

1342
01:00:57,329 --> 01:00:58,509
this partial loop and try to

1343
01:00:58,509 --> 01:01:00,870
get some performance indicator.

1344
01:01:00,870 --> 01:01:03,149
And if this performance
is already pretty bad,

1345
01:01:03,149 --> 01:01:06,229
I can basically prune
the next possibilities

1346
01:01:06,229 --> 01:01:08,469
build based on top
of this looping.

1347
01:01:08,469 --> 01:01:10,269
I do some backtracking or do

1348
01:01:10,269 --> 01:01:12,329
some whatever search
opplementation,

1349
01:01:12,329 --> 01:01:14,859
and eventually I can, um,

1350
01:01:14,859 --> 01:01:18,909
get a reasonably good loop for
the kernel implementation.

1351
01:01:18,909 --> 01:01:22,689
Okay. And similarly, as
I already explained,

1352
01:01:22,689 --> 01:01:25,369
I can also build some cost model

1353
01:01:25,369 --> 01:01:27,209
using my historical data,

1354
01:01:27,209 --> 01:01:30,369
and I can't predict if this
loop is going to do well

1355
01:01:30,369 --> 01:01:34,149
or bad on this target
hardware or shape.

1356
01:01:34,149 --> 01:01:36,309
Yeah. So it's two ways

1357
01:01:36,309 --> 01:01:39,529
that we can basically build
these kind of competors.

1358
01:01:39,690 --> 01:01:42,329
Okay, any question?

1359
01:01:42,989 --> 01:01:45,229
Yeah.

1360
01:01:50,430 --> 01:01:53,429
It's basically, I don't think

1361
01:01:53,429 --> 01:01:57,169
there's track solver
to solve this.

1362
01:01:57,169 --> 01:02:00,549
This is basically exponentially,
uh, complex, okay?

1363
01:02:00,549 --> 01:02:02,630
And so people in
Compiler computer,

1364
01:02:02,630 --> 01:02:04,269
they just try to
build all kinds of

1365
01:02:04,269 --> 01:02:06,570
searching algorithms to prune

1366
01:02:06,570 --> 01:02:08,269
the possibilities
and try to locate

1367
01:02:08,269 --> 01:02:11,110
the solution as
fast as possible.

1368
01:02:11,110 --> 01:02:12,749
But one thing I want to note is

1369
01:02:12,749 --> 01:02:14,590
once you locate the solution,

1370
01:02:14,590 --> 01:02:17,179
uh, then you are good, right?

1371
01:02:17,179 --> 01:02:20,720
Because for that particular
operator and hardware,

1372
01:02:20,720 --> 01:02:23,500
you already find a pretty
good decent implementation,

1373
01:02:23,500 --> 01:02:26,479
and you can keep that code
forever in the future runs.

1374
01:02:26,479 --> 01:02:28,939
Second is, in most cases,

1375
01:02:28,939 --> 01:02:31,399
this is better than

1376
01:02:31,399 --> 01:02:33,740
letting experts doing
the engineering.

1377
01:02:33,740 --> 01:02:35,439
Because for this kind of search,

1378
01:02:35,439 --> 01:02:37,259
what you do is you

1379
01:02:37,259 --> 01:02:39,599
develop a search algorithm
and you just pull

1380
01:02:39,599 --> 01:02:42,239
it run for one month or one week

1381
01:02:42,239 --> 01:02:44,319
and eventually it will
return something for you.

1382
01:02:44,319 --> 01:02:46,259
But if you the human
to develop that,

1383
01:02:46,259 --> 01:02:48,199
that will take even
longer, right? Yeah.

1384
01:02:48,199 --> 01:02:50,334
One is human error, the
other is machine hour.

1385
01:02:50,334 --> 01:02:56,209
Yeah. Okay. Yeah, that

1386
01:02:56,209 --> 01:02:57,509
is basically a high level idea

1387
01:02:57,509 --> 01:02:59,370
of how we do operate
compilation.

1388
01:02:59,370 --> 01:03:02,949
To summarize a little bit,

1389
01:03:02,949 --> 01:03:05,490
we need to find a
way to represent

1390
01:03:05,490 --> 01:03:08,149
the loop space by possibilities.

1391
01:03:08,149 --> 01:03:10,190
And we need to basically,

1392
01:03:10,190 --> 01:03:12,790
make sure the search space
have a good coverage

1393
01:03:12,790 --> 01:03:16,329
of common ogmentations,
like tailing, whatever.

1394
01:03:16,329 --> 01:03:18,950
And we need to build a
efficient search algorithm

1395
01:03:18,950 --> 01:03:20,429
to navigate all
the entire space.

1396
01:03:20,429 --> 01:03:23,290
And we also need to have
a evaluation metric

1397
01:03:23,290 --> 01:03:25,279
that tell if this
loop is good or not.

1398
01:03:25,279 --> 01:03:28,809
Okay. Okay.

1399
01:03:28,809 --> 01:03:30,909
Then coming back
to this, I'm going

1400
01:03:30,909 --> 01:03:33,149
to give you a question so
you can think about that.

1401
01:03:33,149 --> 01:03:36,029
Okay. So like I said,

1402
01:03:36,029 --> 01:03:39,589
yeah, like I said,

1403
01:03:39,589 --> 01:03:41,169
so the premise of

1404
01:03:41,169 --> 01:03:42,490
machining compilation
is basically

1405
01:03:42,490 --> 01:03:43,310
we try to automatically

1406
01:03:43,310 --> 01:03:45,309
generate optimal
configurations that code

1407
01:03:45,309 --> 01:03:48,190
giving users machining
code and targetware.

1408
01:03:48,190 --> 01:03:50,349
And my question
is, do you think,

1409
01:03:50,349 --> 01:03:53,829
machining compiler is
achieving this goal?

1410
01:03:54,820 --> 01:03:59,759
I will offer opinion,
I don't think so.

1411
01:03:59,759 --> 01:04:01,580
I will give you strong evidence.

1412
01:04:01,580 --> 01:04:04,639
Why? We know that

1413
01:04:04,639 --> 01:04:06,719
most of today's language model

1414
01:04:06,719 --> 01:04:08,900
are trained by flash attention.

1415
01:04:09,140 --> 01:04:12,239
Compiler was invented by
flash attention, right?

1416
01:04:12,239 --> 01:04:14,499
Like I said, people
doing compiler research

1417
01:04:14,499 --> 01:04:17,539
during 2018 and 2021.

1418
01:04:17,539 --> 01:04:19,919
Flash attention was
released in 2022.

1419
01:04:19,919 --> 01:04:22,199
And it was handcrafted
by a guy called

1420
01:04:22,199 --> 01:04:25,519
Trida who is a principal
Princeton professor today.

1421
01:04:25,519 --> 01:04:28,539
Then I ask the question,
I will leave this to you.

1422
01:04:28,539 --> 01:04:32,530
Why compiler didn't discover
the flash attention?

1423
01:04:32,530 --> 01:04:36,579
Right? Because theoretically,
if your compiler is

1424
01:04:36,579 --> 01:04:38,700
smart enough and your compiler

1425
01:04:38,700 --> 01:04:40,719
can search the entire space,

1426
01:04:40,719 --> 01:04:43,260
it should be able to
search for flight engine.

1427
01:04:43,260 --> 01:04:46,340
But eventually, we ended
up with someone actually

1428
01:04:46,340 --> 01:04:47,759
implementing flight engine using

1429
01:04:47,759 --> 01:04:50,500
their own manual engineering.

1430
01:04:50,500 --> 01:04:52,819
I will leave this
question to you.

1431
01:04:52,819 --> 01:04:54,520
But I'm not saying that compiler

1432
01:04:54,520 --> 01:04:55,679
has no contribution because I

1433
01:04:55,679 --> 01:04:58,840
think indeed many many today's
other simpler operations,

1434
01:04:58,840 --> 01:05:01,860
for example, met M convolution,

1435
01:05:01,860 --> 01:05:06,379
compiler people indeed,
build some really good, uh,

1436
01:05:06,379 --> 01:05:09,340
kernels that human probably
is not able to understand,

1437
01:05:09,340 --> 01:05:14,279
but rather performing
on GPUs. Okay.

1438
01:05:14,400 --> 01:05:16,520
Okay, that is a pretty

1439
01:05:16,520 --> 01:05:19,700
fast, introduction
operator compiler.

1440
01:05:19,700 --> 01:05:22,079
I think next week we are
coming back to this again,

1441
01:05:22,079 --> 01:05:24,020
and we are going to talk
about graph compiler.

1442
01:05:24,020 --> 01:05:26,679
That is how to transform
graphs automatically.

1443
01:05:26,679 --> 01:05:30,600
Okay? So let's move to the
third part of this lecture.

1444
01:05:30,600 --> 01:05:32,639
And in the third part, I want to

1445
01:05:32,639 --> 01:05:35,680
introduce a little bit
on a different approach,

1446
01:05:35,680 --> 01:05:37,940
okay, which is
basically treating.

1447
01:05:37,940 --> 01:05:40,860
Very famous one, right?
So which treating?

1448
01:05:40,860 --> 01:05:43,259
So we have many treatings.
Why is this treating, right?

1449
01:05:43,259 --> 01:05:45,809
We are not talking this
treating. Okay, this is us.

1450
01:05:45,809 --> 01:05:49,440
And there's another very
confusing lamin treating.

1451
01:05:49,440 --> 01:05:51,819
It's called NVIDIA
Treating inference server.

1452
01:05:51,819 --> 01:05:53,960
And if you search
this on WDA website,

1453
01:05:53,960 --> 01:05:56,120
it's there's open
source project.

1454
01:05:56,120 --> 01:05:58,019
We are also not talking
about this one.

1455
01:05:58,019 --> 01:06:00,259
This one is for serving
transformers and we

1456
01:06:00,259 --> 01:06:02,439
are going to talk about
this little bit later,

1457
01:06:02,439 --> 01:06:04,239
but this is not the
treatment we'll talk about.

1458
01:06:04,239 --> 01:06:06,020
We are basically talking
about this treating.

1459
01:06:06,020 --> 01:06:07,279
This is open treating,

1460
01:06:07,279 --> 01:06:12,100
okay? So what is treating?

1461
01:06:12,100 --> 01:06:15,779
Um, we introduced Koda we

1462
01:06:15,779 --> 01:06:18,420
introduced the
automatic compiler.

1463
01:06:18,420 --> 01:06:19,939
I think you'll get the idea.

1464
01:06:19,939 --> 01:06:21,999
So we can basically
put them into

1465
01:06:21,999 --> 01:06:24,919
two extremes, Koda on the left.

1466
01:06:24,919 --> 01:06:27,379
And Koda is defined as

1467
01:06:27,379 --> 01:06:31,660
a device specific DSL,
domain specific language.

1468
01:06:31,660 --> 01:06:33,459
Okay? It's basically
the way that

1469
01:06:33,459 --> 01:06:36,480
media provides for you
to program the devices.

1470
01:06:36,480 --> 01:06:39,180
Compiler is other extreme.

1471
01:06:39,180 --> 01:06:42,540
So what is pros
and cons of Koda?

1472
01:06:42,540 --> 01:06:44,600
The pros of Koda is
basically developers

1473
01:06:44,600 --> 01:06:47,259
can do whatever the
heck they want, right?

1474
01:06:47,259 --> 01:06:49,559
Uh, they can squeeze
the last bit of

1475
01:06:49,559 --> 01:06:50,659
performce as long as you are

1476
01:06:50,659 --> 01:06:52,559
a powerful Koda programmer,
you can do that.

1477
01:06:52,559 --> 01:06:56,099
It gives you all the sort
of possibilities, right?

1478
01:06:56,099 --> 01:06:58,380
You can use whatever
data structure

1479
01:06:58,380 --> 01:07:00,599
you want instead of Koda kernel.

1480
01:07:00,599 --> 01:07:03,720
But what is the cons of Koda?

1481
01:07:03,720 --> 01:07:05,719
The const is the same.

1482
01:07:05,719 --> 01:07:08,179
Developer can do
whatever heck they want.

1483
01:07:08,179 --> 01:07:12,855
Which also means that if
I'm not a Koda programmer,

1484
01:07:12,855 --> 01:07:15,670
then it requires deep expertise,

1485
01:07:15,670 --> 01:07:17,090
and the performance optim

1486
01:07:17,090 --> 01:07:18,889
is very time consuming,
as you can see.

1487
01:07:18,889 --> 01:07:20,549
When I give you a
piece of quota code,

1488
01:07:20,549 --> 01:07:21,949
I have to explain a lot.

1489
01:07:21,949 --> 01:07:24,610
You have to convert
your brain to SIMD,

1490
01:07:24,610 --> 01:07:26,529
and you have to be
very good at that in

1491
01:07:26,529 --> 01:07:28,909
order to deliver a
piece of good code.

1492
01:07:28,909 --> 01:07:33,129
Okay? And you can

1493
01:07:33,129 --> 01:07:34,829
imagine the code base with

1494
01:07:34,829 --> 01:07:37,789
Koda is very messy
because you are

1495
01:07:37,789 --> 01:07:41,809
going to add a lot of this
into into CP code and

1496
01:07:41,809 --> 01:07:43,969
you Quota code in

1497
01:07:43,969 --> 01:07:46,909
another file and that CP code
will launch that kernel.

1498
01:07:46,909 --> 01:07:48,470
You need two
different compilers.

1499
01:07:48,470 --> 01:07:51,630
One is the CPP compiler and
the other is Koda compiler.

1500
01:07:51,630 --> 01:07:53,609
Koda compiler is pretty
slow by the way.

1501
01:07:53,609 --> 01:07:57,479
Yeah. Okay. Then let's

1502
01:07:57,479 --> 01:07:58,840
go back to this
automatic compiler.

1503
01:07:58,840 --> 01:08:02,000
So what is the pros and
cons of automatic compiler?

1504
01:08:02,080 --> 01:08:05,160
Very fast iteration
for developers,

1505
01:08:05,160 --> 01:08:07,799
because you just write your
high level code in Python and

1506
01:08:07,799 --> 01:08:11,359
you decorate Toch Compel
and things are good, right?

1507
01:08:11,359 --> 01:08:15,360
You can pull up ideas quickly
and give it to compiler.

1508
01:08:16,000 --> 01:08:18,719
But the problem, it cannot

1509
01:08:18,719 --> 01:08:20,699
represent certain
types of ideas.

1510
01:08:20,699 --> 01:08:22,499
And I already give an example.

1511
01:08:22,499 --> 01:08:24,579
Flash attention
was not discovered

1512
01:08:24,579 --> 01:08:26,999
by compilers because
in a search space,

1513
01:08:26,999 --> 01:08:29,879
there is no such repetition
for flash attention.

1514
01:08:29,879 --> 01:08:35,479
Okay? And also, you cannot
use Cosmo data structures.

1515
01:08:36,280 --> 01:08:39,079
And in compiler will indeed boil

1516
01:08:39,079 --> 01:08:41,059
down to be a problem that
you want to generate code,

1517
01:08:41,059 --> 01:08:42,139
and code generation is

1518
01:08:42,139 --> 01:08:44,959
old problem in the old
compiler traditional compiler,

1519
01:08:44,959 --> 01:08:46,579
which is a very
difficult problem.

1520
01:08:46,579 --> 01:08:48,579
I think programming language
and compiler people they are

1521
01:08:48,579 --> 01:08:51,634
doing this a lot of
many, many years, okay.

1522
01:08:51,634 --> 01:08:54,709
Okay, that basically help you

1523
01:08:54,709 --> 01:08:57,149
understand where treating is.

1524
01:08:57,149 --> 01:09:02,569
Okay. So treating is roughly
in the middle, okay?

1525
01:09:02,569 --> 01:09:04,709
It is a domain spec language,

1526
01:09:04,709 --> 01:09:07,609
but it is Python based.

1527
01:09:07,609 --> 01:09:09,569
On the other hand,
it's not automatic,

1528
01:09:09,569 --> 01:09:11,249
but it also well,

1529
01:09:11,249 --> 01:09:13,009
manually, in the bacon,

1530
01:09:13,009 --> 01:09:14,849
treating will help you
optimize some things.

1531
01:09:14,849 --> 01:09:16,749
For example, telling
it do telling for you.

1532
01:09:16,749 --> 01:09:18,369
You don't have to write
your own telling.

1533
01:09:18,369 --> 01:09:20,829
Okay? So basically it
trying to strike a balance

1534
01:09:20,829 --> 01:09:23,969
between pure domain
speak language,

1535
01:09:23,969 --> 01:09:26,869
and the pure automatic
compaer it's in the middle.

1536
01:09:26,869 --> 01:09:29,649
Okay. And here I

1537
01:09:29,649 --> 01:09:31,849
basically put four
logos and you can see,

1538
01:09:31,849 --> 01:09:33,929
I feel these four
logos are so similar.

1539
01:09:33,929 --> 01:09:36,899
It must come from the same
designer. Yeah. Okay.

1540
01:09:36,899 --> 01:09:41,399
Yeah. Okay. So treating
speech is basically,

1541
01:09:41,399 --> 01:09:44,299
how about we do something
that is simpler than Koda,

1542
01:09:44,299 --> 01:09:48,619
but more expensive than
automatic compilers.

1543
01:09:48,619 --> 01:09:51,339
But compared to Coda,
we are less expensive,

1544
01:09:51,339 --> 01:09:54,219
but we are more complicated
than graph compilers.

1545
01:09:54,219 --> 01:09:56,699
And this is basically
treating market piece.

1546
01:09:56,699 --> 01:10:01,019
Okay? I think this is way too
abstract because it's very

1547
01:10:01,019 --> 01:10:03,279
hard for a person or

1548
01:10:03,279 --> 01:10:04,479
engineer with no experience on

1549
01:10:04,479 --> 01:10:05,839
this, understand this pitch.

1550
01:10:05,839 --> 01:10:08,179
But it turns out this
is super successful.

1551
01:10:08,179 --> 01:10:11,359
And we are going to go
through if time permits,

1552
01:10:11,359 --> 01:10:13,519
we are going to go
through two programs,

1553
01:10:13,519 --> 01:10:14,859
but I don't think we
have time, so we are

1554
01:10:14,859 --> 01:10:16,999
going to go through
one program. Okay.

1555
01:10:16,999 --> 01:10:19,979
So, in treating, uh,

1556
01:10:19,979 --> 01:10:23,299
the user defined tensors in RAM.

1557
01:10:23,299 --> 01:10:27,279
Okay? So treaton gives

1558
01:10:27,279 --> 01:10:30,159
you a language that is
basically Python native.

1559
01:10:30,159 --> 01:10:33,399
It embody its own like a
program interface in PySon.

1560
01:10:33,399 --> 01:10:35,199
So writing treating is

1561
01:10:35,199 --> 01:10:37,134
basically equivalent
to writing Python.

1562
01:10:37,134 --> 01:10:39,429
Okay. And also
treatments give you

1563
01:10:39,429 --> 01:10:42,209
so many primitives that
is basically from torch.

1564
01:10:42,209 --> 01:10:44,369
For example, if you want
to write something,

1565
01:10:44,369 --> 01:10:46,409
you can use torch
indexing operations,

1566
01:10:46,409 --> 01:10:48,409
you can use whatever
torch offers to you,

1567
01:10:48,409 --> 01:10:50,729
which is super intuitive, okay?

1568
01:10:50,729 --> 01:10:52,789
But the treatment also give
you a few limitations.

1569
01:10:52,789 --> 01:10:54,449
For example, when
you allocate arrays,

1570
01:10:54,449 --> 01:10:56,369
it has to be a
shape of power too.

1571
01:10:56,369 --> 01:10:57,829
That is because in the back end,

1572
01:10:57,829 --> 01:10:59,289
the treating will
do some telling,

1573
01:10:59,289 --> 01:11:02,010
do some organization
for you automatically.

1574
01:11:02,010 --> 01:11:04,109
So it has to restrict
in this way.

1575
01:11:04,109 --> 01:11:08,069
Okay? So here, let's go

1576
01:11:08,069 --> 01:11:11,089
say one example treating

1577
01:11:11,089 --> 01:11:12,869
Here in this one, we
are doing array eight.

1578
01:11:12,869 --> 01:11:15,049
They equal to X plus Y.

1579
01:11:15,900 --> 01:11:20,659
Okay, I'm going to let you
look at this for 30 seconds.

1580
01:11:34,990 --> 01:11:37,969
Okay, let's try to
parse this code.

1581
01:11:37,969 --> 01:11:41,109
Um, it's much better to
look at this code, right?

1582
01:11:41,109 --> 01:11:43,729
This is so familiar,
right? Like perch.

1583
01:11:43,729 --> 01:11:47,489
I think the first impression
you had is it is Python.

1584
01:11:47,489 --> 01:11:49,369
Yeah, so much easier to write.

1585
01:11:49,369 --> 01:11:52,369
Okay? The second thing you
noticed is I have this one.

1586
01:11:52,369 --> 01:11:56,109
Treating do JT is very
similar to torch,

1587
01:11:56,109 --> 01:11:59,069
Do compel or any
other Jet competitor.

1588
01:11:59,069 --> 01:12:02,209
So treating is indeed a
Jet competitor, okay?

1589
01:12:02,209 --> 01:12:06,049
And I basically use
the grammar of PySon.

1590
01:12:06,049 --> 01:12:07,969
I define a function.

1591
01:12:07,969 --> 01:12:13,469
Uh, I give it a few,
um, like arrays.

1592
01:12:13,469 --> 01:12:17,449
But in triton, all the arrays
are pointers to a tensor.

1593
01:12:17,449 --> 01:12:19,309
Okay? This is slightly

1594
01:12:19,309 --> 01:12:20,569
different from Python because in

1595
01:12:20,569 --> 01:12:21,969
pyson you get object, right?

1596
01:12:21,969 --> 01:12:24,309
But in triton is a
pointer to array.

1597
01:12:24,309 --> 01:12:27,189
Which means it's a
memory address, okay?

1598
01:12:27,189 --> 01:12:29,169
And the treating kernels

1599
01:12:29,169 --> 01:12:31,454
will basically be mapped
to a single block.

1600
01:12:31,454 --> 01:12:33,339
And this is also why Triton

1601
01:12:33,339 --> 01:12:34,859
can simplify UDA a little bit.

1602
01:12:34,859 --> 01:12:36,799
Because in CUDA, you have to

1603
01:12:36,799 --> 01:12:38,839
define your block shape, right.

1604
01:12:38,839 --> 01:12:40,239
You also you define

1605
01:12:40,239 --> 01:12:42,499
your grid shape that is how
many blocks you want to use.

1606
01:12:42,499 --> 01:12:44,639
And then you define your
thread block shape,

1607
01:12:44,639 --> 01:12:46,199
that is how many threads
you want to use.

1608
01:12:46,199 --> 01:12:47,899
So you have to
think in this kind

1609
01:12:47,899 --> 01:12:49,799
of like a two level hierarchy.

1610
01:12:49,799 --> 01:12:52,119
But in Triton, it was
simplified a little bit.

1611
01:12:52,119 --> 01:12:54,319
That is you only think
about one thing that

1612
01:12:54,319 --> 01:12:56,019
is you think about how to

1613
01:12:56,019 --> 01:12:58,664
write a program that will
be mapped to one block.

1614
01:12:58,664 --> 01:13:01,209
That is many many reads a block.

1615
01:13:01,209 --> 01:13:02,849
And outside of this kernel,

1616
01:13:02,849 --> 01:13:06,549
what do you do you define a
grade with different blocks

1617
01:13:06,549 --> 01:13:08,429
and treating will
automatically map this

1618
01:13:08,429 --> 01:13:10,949
into blocks and threads.

1619
01:13:10,949 --> 01:13:13,070
Okay? This already simplified

1620
01:13:13,070 --> 01:13:15,989
the way we think a little
bit because like I said,

1621
01:13:15,989 --> 01:13:18,569
when you write this
piece of code,

1622
01:13:18,569 --> 01:13:20,929
you only think about
how to make it run

1623
01:13:20,929 --> 01:13:22,899
on many many
threads, not blocks.

1624
01:13:22,899 --> 01:13:25,709
Okay. And here,

1625
01:13:25,709 --> 01:13:28,269
let's pass this and we'll
finish lecture, okay?

1626
01:13:28,269 --> 01:13:33,269
So here, I call a treating
EPA which is range,

1627
01:13:33,269 --> 01:13:34,869
and this is a very similar API

1628
01:13:34,869 --> 01:13:36,649
from tort that is I'm going to

1629
01:13:36,649 --> 01:13:41,829
create rate 0-123, right?

1630
01:13:42,110 --> 01:13:44,589
One thing you'll
notice that whenever

1631
01:13:44,589 --> 01:13:46,129
you create something
in treating,

1632
01:13:46,129 --> 01:13:48,989
using something start
from this package,

1633
01:13:48,989 --> 01:13:51,344
it was created on SRAM.

1634
01:13:51,344 --> 01:13:53,659
Okay. That is a shared memory

1635
01:13:53,659 --> 01:13:56,879
of all the threads in the
streaming i processor.

1636
01:13:56,879 --> 01:13:58,679
You'll remember
this code is going

1637
01:13:58,679 --> 01:14:00,559
to be launched just
one single block.

1638
01:14:00,559 --> 01:14:03,059
So you create this array,

1639
01:14:03,059 --> 01:14:06,479
um SRM This is offset array.

1640
01:14:06,479 --> 01:14:08,820
And then what you do is, uh,

1641
01:14:08,820 --> 01:14:10,200
you try to manipulate

1642
01:14:10,200 --> 01:14:13,119
the pointer semantics
using Python API.

1643
01:14:13,119 --> 01:14:16,804
So here I have the starting
address of my XYZ, right?

1644
01:14:16,804 --> 01:14:19,349
And when I try to
write this program,

1645
01:14:19,349 --> 01:14:21,609
I need to think about how
I make sure each thread is

1646
01:14:21,609 --> 01:14:24,549
skewed a part of X
plus Y equal to zero.

1647
01:14:24,549 --> 01:14:31,429
So here I launch it
into 26 or 24 threads.

1648
01:14:31,429 --> 01:14:33,329
So basically, each
thread in that block,

1649
01:14:33,329 --> 01:14:35,449
we need to take one
element a, right?

1650
01:14:35,449 --> 01:14:37,889
So what I do is
my hello idea is,

1651
01:14:37,889 --> 01:14:40,959
I'm trying to make sure each
thread read one element.

1652
01:14:40,959 --> 01:14:43,789
From HPM to SRM.

1653
01:14:43,789 --> 01:14:46,749
That is cooperative
fetching, right?

1654
01:14:46,749 --> 01:14:48,649
And then I'm going

1655
01:14:48,649 --> 01:14:50,949
to let each road to
take the competition.

1656
01:14:50,949 --> 01:14:54,609
That is I read achrod read
two elements A and B,

1657
01:14:54,609 --> 01:14:58,289
and then add them
together and get together

1658
01:14:58,289 --> 01:15:02,089
results and then write the
results back into C. Okay?

1659
01:15:02,089 --> 01:15:05,029
And this this is
a pure SMD code.

1660
01:15:05,029 --> 01:15:06,829
Okay. So what I do is basically

1661
01:15:06,829 --> 01:15:08,949
I first create this
point of semantics that

1662
01:15:08,949 --> 01:15:12,129
is offset on my SRAM and

1663
01:15:12,129 --> 01:15:16,249
um and one thing you'll
notice that in Cuda,

1664
01:15:16,249 --> 01:15:18,409
you have to do some
cooperative fetching.

1665
01:15:18,409 --> 01:15:20,349
But here, you just go Christ API

1666
01:15:20,349 --> 01:15:21,510
and this will be automatically

1667
01:15:21,510 --> 01:15:24,929
mapped to 1024 threads.

1668
01:15:24,929 --> 01:15:26,489
Which means this
line is going to be

1669
01:15:26,489 --> 01:15:29,569
executed SMD in
that many threads.

1670
01:15:29,569 --> 01:15:33,689
Then I'm going to add this
offset to each pointer.

1671
01:15:33,689 --> 01:15:36,489
This is the add operation.

1672
01:15:36,489 --> 01:15:38,069
And in treating, treating

1673
01:15:38,069 --> 01:15:39,529
will basically take care
of the lines and make

1674
01:15:39,529 --> 01:15:41,589
sure each threading that block

1675
01:15:41,589 --> 01:15:44,529
will perform one
add on the offset.

1676
01:15:44,529 --> 01:15:46,949
And once I have this
offset, I know,

1677
01:15:46,949 --> 01:15:51,849
I need to load the element
from HPM to my SRM,

1678
01:15:51,849 --> 01:15:53,309
treating will basically give you

1679
01:15:53,309 --> 01:15:55,929
another EPL called TL load.

1680
01:15:55,929 --> 01:15:57,829
And this load is also,

1681
01:15:57,829 --> 01:16:00,929
um, coptive fetching, right?

1682
01:16:00,929 --> 01:16:02,809
Because the treatment
will automatically map

1683
01:16:02,809 --> 01:16:06,019
this node into different reads.

1684
01:16:06,019 --> 01:16:09,709
But how we tell each right
to load the exact element.

1685
01:16:09,709 --> 01:16:12,569
The way we tell that is
basically we give them

1686
01:16:12,569 --> 01:16:14,929
the X PTRs Remember

1687
01:16:14,929 --> 01:16:18,409
this X PR was generated in a
way where for each element,

1688
01:16:18,409 --> 01:16:20,309
I apply a different offset.

1689
01:16:20,309 --> 01:16:22,949
So each read will
basically corresponds to

1690
01:16:22,949 --> 01:16:24,489
one element and try to read it

1691
01:16:24,489 --> 01:16:27,269
a corresponding
element from XN Y.

1692
01:16:27,269 --> 01:16:30,589
And this X and Y, basically,

1693
01:16:30,589 --> 01:16:33,749
the uh, RM allocated.

1694
01:16:33,749 --> 01:16:36,470
And eventually, I'm going
to do a competition.

1695
01:16:36,470 --> 01:16:38,189
And this is still
vectorize the code,

1696
01:16:38,189 --> 01:16:39,309
but the treatment
will automatically

1697
01:16:39,309 --> 01:16:40,929
map this into many many threads.

1698
01:16:40,929 --> 01:16:44,134
So each read only compute its
own corresponding element.

1699
01:16:44,134 --> 01:16:45,959
And this result was in

1700
01:16:45,959 --> 01:16:47,999
the register of
that thread, right.

1701
01:16:47,999 --> 01:16:49,959
And eventually,
I'm going to store

1702
01:16:49,959 --> 01:16:52,099
this result back
into the Z pointer.

1703
01:16:52,099 --> 01:16:54,499
And I know why each thread

1704
01:16:54,499 --> 01:16:57,379
knows which position
to write the results?

1705
01:16:57,379 --> 01:17:00,699
Because the PTRs
are basically PTR

1706
01:17:00,699 --> 01:17:02,479
plus offset and I already

1707
01:17:02,479 --> 01:17:04,224
apply the offset
for each thread.

1708
01:17:04,224 --> 01:17:06,669
Okay. Yeah, that's basically

1709
01:17:06,669 --> 01:17:08,229
our first piece
of treating code.

1710
01:17:08,229 --> 01:17:10,649
And you can see, user

1711
01:17:10,649 --> 01:17:13,389
will only be responsible
for mapping.

1712
01:17:13,389 --> 01:17:15,369
At outside of this kernel,

1713
01:17:15,369 --> 01:17:17,009
user is responsible for creating

1714
01:17:17,009 --> 01:17:19,929
different blocks and
passes into the kernel.

1715
01:17:19,929 --> 01:17:21,289
But inside of this kernel,

1716
01:17:21,289 --> 01:17:25,749
the users like salt process
are already being simplified.

1717
01:17:25,749 --> 01:17:28,149
I only need to think
about high rat code that

1718
01:17:28,149 --> 01:17:31,089
will be mapped to all threats
but not blocks anymore.

1719
01:17:31,089 --> 01:17:32,509
And all the code,

1720
01:17:32,509 --> 01:17:34,949
APS are basically Python based,

1721
01:17:34,949 --> 01:17:38,789
and you can co many many
similar APS from torch.

1722
01:17:38,789 --> 01:17:42,229
Okay? Yeah, that's pretty
much all I have today.

1723
01:17:42,229 --> 01:17:43,609
And in the next lecture,

1724
01:17:43,609 --> 01:17:45,049
we are going to continue grand

1725
01:17:45,049 --> 01:17:47,869
this program on triden
a little bit. Yeah.
