1
00:00:02,040 --> 00:00:04,920
Okay, let's get started.

2
00:00:04,920 --> 00:00:09,619
Yeah. So I hope you
enjoy APA two, right?

3
00:00:09,619 --> 00:00:11,759
You got a reference answer

4
00:00:11,759 --> 00:00:13,879
for second question
yesterday, right?

5
00:00:13,879 --> 00:00:16,379
And you get a reference
answer to question one

6
00:00:16,379 --> 00:00:19,319
today from Deepsk, right?

7
00:00:19,319 --> 00:00:21,239
I hope you enjoy. And if

8
00:00:21,239 --> 00:00:22,679
you have no idea
how to optimize,

9
00:00:22,679 --> 00:00:24,194
you can actually
check your code.

10
00:00:24,194 --> 00:00:27,429
Yeah. Okay. And today,

11
00:00:27,429 --> 00:00:30,609
we are going to
finish parallelism.

12
00:00:30,609 --> 00:00:32,189
We have a lot of
deep contents today.

13
00:00:32,189 --> 00:00:34,490
So starting from
the next lecture,

14
00:00:34,490 --> 00:00:36,609
we are going to dive
deeper into Ms.

15
00:00:36,609 --> 00:00:37,709
We are going to use what we

16
00:00:37,709 --> 00:00:39,770
learned in the previous lecture,

17
00:00:39,770 --> 00:00:44,715
and we apply them into Ms.
Let's try to finish today.

18
00:00:44,715 --> 00:00:48,539
Just to recap, right? Um, so

19
00:00:48,539 --> 00:00:50,500
basically, in my last lecture,

20
00:00:50,500 --> 00:00:53,999
we developed a model that
help you analyze the overhead

21
00:00:53,999 --> 00:00:57,780
from Interop or pipeline parts

22
00:00:57,780 --> 00:00:59,599
the overhead of Pipeline
partism, right?

23
00:00:59,599 --> 00:01:02,299
This is the ganchart.
And I think

24
00:01:02,299 --> 00:01:03,739
the core of this model is

25
00:01:03,739 --> 00:01:05,820
basically you care about
the bubble, right?

26
00:01:05,820 --> 00:01:08,050
You want to minimize
the bubble, okay?

27
00:01:08,050 --> 00:01:10,479
And we develop our
first schedule,

28
00:01:10,479 --> 00:01:12,000
which is the GPip schedule,

29
00:01:12,000 --> 00:01:15,680
which is a very structured
and nice looking schedule,

30
00:01:15,680 --> 00:01:16,520
but it doesn't work.

31
00:01:16,520 --> 00:01:17,879
Why? Because the memory

32
00:01:17,879 --> 00:01:19,940
is green with lumbar
microbde size.

33
00:01:19,940 --> 00:01:22,520
Okay? And we fix it, right?

34
00:01:22,520 --> 00:01:24,259
We make the schedule
a little bit messier,

35
00:01:24,259 --> 00:01:26,899
but it works, okay?

36
00:01:26,899 --> 00:01:30,660
And we try to advance
it further and further.

37
00:01:30,660 --> 00:01:32,160
We start developing another

38
00:01:32,160 --> 00:01:34,560
mode schedule, which
is bidirectional.

39
00:01:34,560 --> 00:01:39,399
And we said that this is one
used to train deep, okay?

40
00:01:39,399 --> 00:01:41,459
This one called Chimera.

41
00:01:41,459 --> 00:01:43,840
Okay, and I think
we stopped here.

42
00:01:43,840 --> 00:01:45,999
Okay. Then let's continue.

43
00:01:45,999 --> 00:01:49,599
So for all the schedules I
introduced in my last lecture,

44
00:01:49,599 --> 00:01:51,220
I give them a name that is,

45
00:01:51,220 --> 00:01:53,799
um, they are called

46
00:01:53,799 --> 00:01:55,879
synchronous Pipeline
parallel schedule.

47
00:01:55,879 --> 00:01:57,680
So here, you can understand that

48
00:01:57,680 --> 00:01:59,379
synchronous are
something very similar

49
00:01:59,379 --> 00:02:01,220
to what we discussed
in data parism that

50
00:02:01,220 --> 00:02:03,140
is we always make

51
00:02:03,140 --> 00:02:05,400
sure like all the workers,
they are in the same place.

52
00:02:05,400 --> 00:02:08,399
So what do I mean? So the
main benefits of this kind

53
00:02:08,399 --> 00:02:09,699
of like method is that it

54
00:02:09,699 --> 00:02:11,640
keeps machine
learning semantics.

55
00:02:11,640 --> 00:02:12,939
So in other words,

56
00:02:12,939 --> 00:02:16,004
the whole training competition
will be exactly the same.

57
00:02:16,004 --> 00:02:19,130
Uh, as if you're actually new on

58
00:02:19,130 --> 00:02:22,270
single device because if you
look at the pre schedule,

59
00:02:22,270 --> 00:02:23,929
we always have this barrier.

60
00:02:23,929 --> 00:02:26,450
We update when all the micro
baatches are finished.

61
00:02:26,450 --> 00:02:27,950
Okay. But actually,

62
00:02:27,950 --> 00:02:30,050
there's a little space
that we can play around

63
00:02:30,050 --> 00:02:31,970
that to make this a little bit

64
00:02:31,970 --> 00:02:34,869
more synchronous
but much faster.

65
00:02:34,869 --> 00:02:37,869
So um, like I said,

66
00:02:37,869 --> 00:02:39,370
in this kind of
synchronous schedule,

67
00:02:39,370 --> 00:02:42,350
the main downside
is we have bubbles,

68
00:02:42,350 --> 00:02:45,230
and although the bubble will
diminish as you increase

69
00:02:45,230 --> 00:02:49,969
lumbar microbachs but
we have bubbles, um,

70
00:02:49,969 --> 00:02:51,149
and we have discussed

71
00:02:51,149 --> 00:02:53,389
many algorithms to
reduce the bubble, but,

72
00:02:53,389 --> 00:02:56,389
reducing this pipeline
bubble requires you

73
00:02:56,389 --> 00:03:01,089
basically very radically
increase lamb microbatches.

74
00:03:01,089 --> 00:03:05,369
So for example, if you
have a smaller input,

75
00:03:05,369 --> 00:03:08,509
which you don't have a lot
of space to make many,

76
00:03:08,509 --> 00:03:10,009
many microbtches in that case,

77
00:03:10,009 --> 00:03:12,415
your pipeline parism is not
going to be super efficient.

78
00:03:12,415 --> 00:03:17,039
Okay. Now, let's look at
another line of work.

79
00:03:17,039 --> 00:03:20,479
So another type of pipeline
parallel schedule or what

80
00:03:20,479 --> 00:03:24,280
we call the synchronous
pipeline parallel schedule,

81
00:03:24,280 --> 00:03:25,939
where we basically remove

82
00:03:25,939 --> 00:03:29,920
the synchronization point in
the pipeline schedule, um,

83
00:03:29,920 --> 00:03:31,919
and now we can start
the next round

84
00:03:31,919 --> 00:03:34,500
forward forward pass before

85
00:03:34,500 --> 00:03:37,139
the backward pass of the
previous round finished.

86
00:03:37,139 --> 00:03:40,859
Okay. That is we remove
a barrier update.

87
00:03:40,859 --> 00:03:43,800
Okay. And a synchronous schedule

88
00:03:43,800 --> 00:03:45,820
actually don't have
pipeline bubbles,

89
00:03:45,820 --> 00:03:47,500
but it also breaks

90
00:03:47,500 --> 00:03:50,160
like I said, the
synchronous semantics.

91
00:03:50,160 --> 00:03:52,959
That is, if you use this
kind of pipeline schedule,

92
00:03:52,959 --> 00:03:54,579
your model trained with this

93
00:03:54,579 --> 00:03:56,479
pipeline schedule
is different from,

94
00:03:56,479 --> 00:03:59,359
uh, as if you are training
on a single device. Okay?

95
00:03:59,359 --> 00:04:01,759
Because your grading
becomes steel in some way.

96
00:04:01,759 --> 00:04:05,040
Okay, like what we did
in, um, data parism.

97
00:04:05,040 --> 00:04:08,674
Okay, let's see. The first
one is basically this one.

98
00:04:08,674 --> 00:04:11,129
If we like all the devices

99
00:04:11,129 --> 00:04:14,829
to go free and acute
at their own pace,

100
00:04:14,829 --> 00:04:19,949
we basically get this schedule
which I called AMP net.

101
00:04:19,949 --> 00:04:21,769
And each device
basically performs

102
00:04:21,769 --> 00:04:24,230
forward pass whenever free.

103
00:04:24,230 --> 00:04:27,690
And they update the wise
after every Brod pass.

104
00:04:27,690 --> 00:04:30,089
Okay. And with this schedule,

105
00:04:30,089 --> 00:04:32,809
the different stages in
the forward and Bro pass

106
00:04:32,809 --> 00:04:35,630
can be on different
versions of the WIC, right?

107
00:04:35,630 --> 00:04:37,990
Because each device
update on their own pace.

108
00:04:37,990 --> 00:04:41,109
Okay? For example,
here for the Data 0.1,

109
00:04:41,109 --> 00:04:45,690
for the data one, the forward
pass of stage one and two,

110
00:04:45,690 --> 00:04:47,930
on the initial width, where the

111
00:04:47,930 --> 00:04:50,010
forward pass on
who backward pass

112
00:04:50,010 --> 00:04:54,830
and backward pass on weights
updated by data zero.

113
00:04:54,830 --> 00:04:56,869
Okay? So this discrepancy

114
00:04:56,869 --> 00:04:59,050
brings noise to the
training process, right?

115
00:04:59,050 --> 00:05:00,830
Because in original SED,

116
00:05:00,830 --> 00:05:02,710
we make sure we update and

117
00:05:02,710 --> 00:05:04,890
then dp gradient
and then update.

118
00:05:04,890 --> 00:05:08,595
But here, every device basically
follow a different pace.

119
00:05:08,595 --> 00:05:10,860
And that's why this one

120
00:05:10,860 --> 00:05:12,540
actually doesn't work
pretty well, okay?

121
00:05:12,540 --> 00:05:15,519
On smart that's like
a mist or Cipher

122
00:05:15,519 --> 00:05:18,780
this kind of small um,
image classifier models.

123
00:05:18,780 --> 00:05:21,739
It works, but very likely
that amor language models.

124
00:05:21,739 --> 00:05:23,499
This will lead to divergence.

125
00:05:23,499 --> 00:05:27,420
Okay? Um, and there are
also some work that,

126
00:05:27,420 --> 00:05:31,820
improve the convergence of
this MP net, um, algorithm.

127
00:05:31,820 --> 00:05:33,799
One is called pipe Myer

128
00:05:33,799 --> 00:05:36,280
and it modifies the
grading optimizer

129
00:05:36,280 --> 00:05:37,639
to improve the convergence,

130
00:05:37,639 --> 00:05:39,539
um, but just a
little bit improve.

131
00:05:39,539 --> 00:05:43,104
It cannot fundamentally change
the drawbo this way, okay?

132
00:05:43,104 --> 00:05:46,109
Um, and pipe dream is another,

133
00:05:46,109 --> 00:05:48,789
algorithm that achieves
better convergence

134
00:05:48,789 --> 00:05:50,949
by reducing this synchrony.

135
00:05:50,949 --> 00:05:53,429
So, the timeline
of the pipe dream

136
00:05:53,429 --> 00:05:56,530
algorithm looks very
similar to 1f1b, right.

137
00:05:56,530 --> 00:05:58,070
It looks very similar to 1f1b.

138
00:05:58,070 --> 00:06:00,630
But, um, the main
difference here is,

139
00:06:00,630 --> 00:06:02,770
um, we updated the model

140
00:06:02,770 --> 00:06:04,789
was once the backward
has finished.

141
00:06:04,789 --> 00:06:06,970
Okay. The main idea

142
00:06:06,970 --> 00:06:08,610
here is basically we
try to enforce in

143
00:06:08,610 --> 00:06:10,770
the same version of

144
00:06:10,770 --> 00:06:12,689
weight for the forward
and backward pass

145
00:06:12,689 --> 00:06:14,350
of a single input data point.

146
00:06:14,350 --> 00:06:16,969
It's basically
relaxed constraint.

147
00:06:16,969 --> 00:06:19,270
And let's focus on device one,

148
00:06:19,270 --> 00:06:21,330
okay? So this is device one.

149
00:06:21,330 --> 00:06:23,430
And for the forward and backward

150
00:06:23,430 --> 00:06:25,669
pass of the input batches,

151
00:06:25,669 --> 00:06:27,369
zero to three, we use

152
00:06:27,369 --> 00:06:30,090
the same initial
weight. You can see.

153
00:06:30,090 --> 00:06:33,870
Okay? Um, but for
input batch four.

154
00:06:33,870 --> 00:06:37,650
Um, it uses the width updated
by input battery zero.

155
00:06:37,650 --> 00:06:39,450
Since we still need to use

156
00:06:39,450 --> 00:06:40,870
the initial weights for

157
00:06:40,870 --> 00:06:43,150
the backward pass of
input one to three,

158
00:06:43,150 --> 00:06:44,809
and at this time point,

159
00:06:44,809 --> 00:06:46,949
we will have two
copies of weights.

160
00:06:46,949 --> 00:06:49,370
One initial weight and
one width, that is,

161
00:06:49,370 --> 00:06:53,890
basically updated after we
compute on input zero, right?

162
00:06:53,890 --> 00:06:56,970
Um, and again, for input B five,

163
00:06:56,970 --> 00:06:59,509
we need to update weights, um,

164
00:06:59,509 --> 00:07:02,770
we need to use width updated
by the input zero and one.

165
00:07:02,770 --> 00:07:05,430
Okay? And for input six,

166
00:07:05,430 --> 00:07:07,189
we need to use width updated

167
00:07:07,189 --> 00:07:10,870
by data bat zero, one and two.

168
00:07:10,870 --> 00:07:14,180
Therefore, in total, um,

169
00:07:14,180 --> 00:07:16,420
we need to store
four copies of it.

170
00:07:16,420 --> 00:07:18,420
Okay? But we only divide

171
00:07:18,420 --> 00:07:20,439
the neural network
into four stages.

172
00:07:20,439 --> 00:07:22,840
The biggest downside
of this pipe dream is

173
00:07:22,840 --> 00:07:25,860
that there will be no
memory saving at all.

174
00:07:25,860 --> 00:07:28,500
Right? Remember, when
we do pipeline prism,

175
00:07:28,500 --> 00:07:30,759
we divide the neural
network into four stages.

176
00:07:30,759 --> 00:07:32,220
That's why we consume memory.

177
00:07:32,220 --> 00:07:34,879
But this algorithm basically
defeat our purpose.

178
00:07:34,879 --> 00:07:37,280
You need to store four
copies on each device.

179
00:07:37,280 --> 00:07:41,319
Okay? But this was
just a very initial,

180
00:07:41,319 --> 00:07:43,939
kind of like paper discussing

181
00:07:43,939 --> 00:07:45,799
a synchronous pipeline
barrel schedule

182
00:07:45,799 --> 00:07:47,200
and discussing that possibility.

183
00:07:47,200 --> 00:07:48,840
That's why I bring
this up, okay.

184
00:07:48,840 --> 00:07:50,780
And today, I think people
have been following up

185
00:07:50,780 --> 00:07:52,980
a lot of new work which can
do better than this one.

186
00:07:52,980 --> 00:07:58,520
Yeah. Okay. And to
reduce the memory usage,

187
00:07:58,520 --> 00:08:00,960
um, the pipe dream authors,

188
00:08:00,960 --> 00:08:05,279
they propose a modification
to the original pipeline,

189
00:08:05,279 --> 00:08:08,899
papeream algorithm by updating
the width less frequently,

190
00:08:08,899 --> 00:08:11,299
um, which is called
pipe dream two PW.

191
00:08:11,299 --> 00:08:13,559
For example, here, input patch,

192
00:08:13,559 --> 00:08:16,760
four, five, six, still
use the initial width.

193
00:08:16,760 --> 00:08:19,019
And starting uh, input seven,

194
00:08:19,019 --> 00:08:22,760
we will use the width updated
by inputs zero to three.

195
00:08:22,760 --> 00:08:24,580
Therefore, the greens are always

196
00:08:24,580 --> 00:08:26,979
only stored for one update, y.

197
00:08:26,979 --> 00:08:28,854
And you can reduce the story.

198
00:08:28,854 --> 00:08:32,590
Okay. And this Pablo Author,
he's a very famous guy.

199
00:08:32,590 --> 00:08:34,410
He basically was a pioneer

200
00:08:34,410 --> 00:08:37,750
in doing Pipeline Pism
and he joined Adia,

201
00:08:37,750 --> 00:08:39,809
and the entire Megatron

202
00:08:39,809 --> 00:08:41,489
library was written
mostly by him.

203
00:08:41,489 --> 00:08:44,650
Okay? Megatron is basically
the framework that people use

204
00:08:44,650 --> 00:08:48,169
a lot in community to train
language models. Yeah. Okay.

205
00:08:48,169 --> 00:08:50,750
This is pretty good work, okay?

206
00:08:51,010 --> 00:08:54,349
Uh, there might be
different there

207
00:08:54,349 --> 00:08:55,890
will be more different pipeline

208
00:08:55,890 --> 00:08:57,230
schedules that we
haven't covered,

209
00:08:57,230 --> 00:08:59,770
but one important issue,

210
00:08:59,770 --> 00:09:02,450
we haven't discussed
is basically

211
00:09:02,450 --> 00:09:04,829
for all the pipeline
scheduling algorithm,

212
00:09:04,829 --> 00:09:06,809
um, we showed just now, right,

213
00:09:06,809 --> 00:09:08,889
we assume the running times of

214
00:09:08,889 --> 00:09:11,289
different pipeline stages are

215
00:09:11,289 --> 00:09:14,389
exactly the same as in
this figure above, right?

216
00:09:14,389 --> 00:09:17,969
So basically ABCD they are
of the same lens, right?

217
00:09:17,969 --> 00:09:24,939
Um, But if the latency of
these stages are not the same,

218
00:09:24,939 --> 00:09:26,639
for example, the
timeline will be

219
00:09:26,639 --> 00:09:29,059
looking like this, right?

220
00:09:29,059 --> 00:09:31,120
So basically for a single input,

221
00:09:31,120 --> 00:09:33,379
for each different
input, you will have

222
00:09:33,379 --> 00:09:36,079
different kind of
like executing time.

223
00:09:36,079 --> 00:09:38,319
And you can see this
is much worse, right?

224
00:09:38,319 --> 00:09:39,859
In the upper figure,

225
00:09:39,859 --> 00:09:41,959
when we execute four
batches, we are there.

226
00:09:41,959 --> 00:09:44,780
But here, basically the
time becomes longer.

227
00:09:44,780 --> 00:09:46,299
Okay? So one important factor

228
00:09:46,299 --> 00:09:47,440
when you do pipeline parison,

229
00:09:47,440 --> 00:09:50,399
you need to make sure each
device has the same workload.

230
00:09:50,399 --> 00:09:52,020
Does that make sense?

231
00:09:52,020 --> 00:09:54,660
Okay. We'll discuss
this later, okay?

232
00:09:58,900 --> 00:10:02,580
Okay. There are many studying

233
00:10:02,580 --> 00:10:05,680
how to partition on New
into different stages,

234
00:10:05,680 --> 00:10:07,879
and the main goal is
basically trying to

235
00:10:07,879 --> 00:10:10,599
minimize the maximum
stage latency

236
00:10:10,599 --> 00:10:11,999
to improve pipeline efficiency,

237
00:10:11,999 --> 00:10:14,779
the overall pipeline
efficiency. Um,

238
00:10:14,779 --> 00:10:16,960
And also try to maximize

239
00:10:16,960 --> 00:10:20,099
the pization of different
branches in device placement.

240
00:10:20,099 --> 00:10:22,599
And here I give you an overview

241
00:10:22,599 --> 00:10:23,839
of these two class methods.

242
00:10:23,839 --> 00:10:25,120
I'm not going to
cover all of them,

243
00:10:25,120 --> 00:10:26,740
but if you want to dive deeper,

244
00:10:26,740 --> 00:10:30,259
read these papers.
Essentially two categories.

245
00:10:30,259 --> 00:10:31,820
The first category is basically

246
00:10:31,820 --> 00:10:33,820
the same recipe you
are very familiar

247
00:10:33,820 --> 00:10:35,919
with because we did
this for compilers for,

248
00:10:35,919 --> 00:10:38,639
um Matamo for like a graph

249
00:10:38,639 --> 00:10:42,359
open that is machine
learning for systems.

250
00:10:42,359 --> 00:10:44,099
We try to learn a model

251
00:10:44,099 --> 00:10:45,715
that can give us
the best schedule.

252
00:10:45,715 --> 00:10:47,510
And this model was
usually trained using

253
00:10:47,510 --> 00:10:49,689
inverse learning.
Okay, the left part.

254
00:10:49,689 --> 00:10:51,569
The second part is we explicitly

255
00:10:51,569 --> 00:10:53,889
model this pipeline schedule
as an oppimentation,

256
00:10:53,889 --> 00:10:55,549
and we try to solve
that openation.

257
00:10:55,549 --> 00:10:58,129
But sometimes this opmentation
is probably in trackable,

258
00:10:58,129 --> 00:11:01,529
so it takes forever, because
it is a bit hard and,

259
00:11:01,529 --> 00:11:03,850
um, so, you know, yeah.

260
00:11:03,850 --> 00:11:05,790
Okay, I will basically give

261
00:11:05,790 --> 00:11:08,294
you a brief idea of
the first scheduling.

262
00:11:08,294 --> 00:11:10,479
And this is a work
that basically

263
00:11:10,479 --> 00:11:12,659
trying to optimize the
pipeline for schedule.

264
00:11:12,659 --> 00:11:16,760
So basically, this
work is called Plato.

265
00:11:16,760 --> 00:11:18,760
So basically, is the example

266
00:11:18,760 --> 00:11:20,880
of RO is partition algorithm,

267
00:11:20,880 --> 00:11:22,219
and the environment for

268
00:11:22,219 --> 00:11:25,319
the I algorithm is
defined as in this figure

269
00:11:25,319 --> 00:11:27,480
where the state is a specific

270
00:11:27,480 --> 00:11:30,685
device placement plan
for computing graph.

271
00:11:30,685 --> 00:11:32,630
Um, the action is to modify

272
00:11:32,630 --> 00:11:35,529
the device assignment
of a node in the graph.

273
00:11:35,529 --> 00:11:37,170
And the reward is basically

274
00:11:37,170 --> 00:11:39,109
a latency difference between

275
00:11:39,109 --> 00:11:41,230
the new and the old placement.

276
00:11:41,230 --> 00:11:43,029
And the R agent is

277
00:11:43,029 --> 00:11:45,590
a graph neural network
that's trying to, um,

278
00:11:45,590 --> 00:11:46,950
try to basically perform as

279
00:11:46,950 --> 00:11:48,449
a policy and try to
figure out what is

280
00:11:48,449 --> 00:11:49,769
the right way to
place these notes on

281
00:11:49,769 --> 00:11:52,269
different devices in order
to maximize the reward,

282
00:11:52,269 --> 00:11:56,170
which is equivalent to
minimizing latency, okay?

283
00:11:56,170 --> 00:11:58,249
And people basically, do this,

284
00:11:58,249 --> 00:12:01,250
and, it's basically
like a search.

285
00:12:01,250 --> 00:12:04,149
You keep trying different
placement strategy and,

286
00:12:04,149 --> 00:12:06,790
you put it on a cluster
observe latency,

287
00:12:06,790 --> 00:12:09,110
and then you propagate the
reward back to new network,

288
00:12:09,110 --> 00:12:12,860
something like that, okay? Cool.

289
00:12:12,860 --> 00:12:19,870
Any questions? Yeah. This
is the spars Wire sparse.

290
00:12:19,870 --> 00:12:24,550
How? They just use more
computer to train a model.

291
00:12:24,550 --> 00:12:26,230
Yeah. Keep trying.

292
00:12:26,230 --> 00:12:31,849
Okay. Okay, um I summary,

293
00:12:31,849 --> 00:12:36,570
the idea of interop
parism is to assign

294
00:12:36,570 --> 00:12:39,509
different operators of
the computation graph to

295
00:12:39,509 --> 00:12:40,970
different devices and then

296
00:12:40,970 --> 00:12:43,669
execute in pipeline
parallel fashion, right?

297
00:12:43,669 --> 00:12:45,789
The device placement algorithm

298
00:12:45,789 --> 00:12:48,750
basically paralyze
different graph branches,

299
00:12:48,750 --> 00:12:50,570
but they have a problem they

300
00:12:50,570 --> 00:12:52,250
are not applicable
on all graphs.

301
00:12:52,250 --> 00:12:54,230
That's the first category here.

302
00:12:54,230 --> 00:12:56,149
Um, the second category,

303
00:12:56,149 --> 00:12:58,429
synchronized pipeline
schedule is the one that

304
00:12:58,429 --> 00:13:01,090
actually we used a lot, okay?

305
00:13:01,090 --> 00:13:03,570
And it has the same convergence

306
00:13:03,570 --> 00:13:05,750
as if you are on single device,

307
00:13:05,750 --> 00:13:07,950
and there are many
algorithms on reducing

308
00:13:07,950 --> 00:13:10,929
the pipeline bubbles and
improving hardware condition.

309
00:13:10,929 --> 00:13:13,810
Um, and the third category,

310
00:13:13,810 --> 00:13:15,210
which I just covered
is basically

311
00:13:15,210 --> 00:13:17,089
a synchronous pipeline
Barrel schedule.

312
00:13:17,089 --> 00:13:20,809
It has the highest hardware
utilization because you can

313
00:13:20,809 --> 00:13:24,990
basically reduce you can
basically have zero bubble,

314
00:13:24,990 --> 00:13:26,469
right, because you just
schedule whatever you want.

315
00:13:26,469 --> 00:13:28,230
Yeah, you don't care
about convergence.

316
00:13:28,230 --> 00:13:31,469
And it has no pipe bubble,

317
00:13:31,469 --> 00:13:34,030
but it introduce
samsung synchrony,

318
00:13:34,030 --> 00:13:35,750
which we heard in
machinering convergence.

319
00:13:35,750 --> 00:13:38,630
So it is only applicable
on some problems,

320
00:13:38,630 --> 00:13:39,990
but not all problems.

321
00:13:39,990 --> 00:13:42,949
Okay? So all these
schedules perform the

322
00:13:42,949 --> 00:13:46,290
best when you have
a balanced stage,

323
00:13:46,290 --> 00:13:48,049
that is each stage need to

324
00:13:48,049 --> 00:13:50,030
have roughly the same workload.

325
00:13:50,030 --> 00:13:51,809
Otherwise, you observe
that graph, right?

326
00:13:51,809 --> 00:13:55,929
Okay. Cool. This basically
marks the end of,

327
00:13:55,929 --> 00:13:58,369
uh, pipeline par schedule.

328
00:13:58,369 --> 00:14:00,170
I think up until today,

329
00:14:00,170 --> 00:14:01,810
people are still
publishing this area.

330
00:14:01,810 --> 00:14:03,630
They are writing
more and more papers

331
00:14:03,630 --> 00:14:06,020
to coming up with a better
schedule. Yeah, please.

332
00:14:06,020 --> 00:14:10,990
Is there a way
synchronous together?

333
00:14:12,630 --> 00:14:15,229
No, you cannot because once

334
00:14:15,229 --> 00:14:18,010
your model parameter
becomes a synchronous,

335
00:14:18,010 --> 00:14:20,569
grading is sequential,
you update,

336
00:14:20,569 --> 00:14:22,110
you update, you
update, you update.

337
00:14:22,110 --> 00:14:24,150
Once at the early step,

338
00:14:24,150 --> 00:14:26,610
you update and you are
using a steel gradit,

339
00:14:26,610 --> 00:14:28,049
then your trajectory is

340
00:14:28,049 --> 00:14:30,009
different from using
a synchronous one.

341
00:14:30,009 --> 00:14:33,810
You cannot go back. Yeah.
Does that make sense?

342
00:14:33,810 --> 00:14:40,360
Yeah, okay. Okay. Then
let's move on, okay.

343
00:14:40,360 --> 00:14:43,200
Let's move into a
more interesting,

344
00:14:43,200 --> 00:14:47,879
like a model or how we model
the intraop partism, right?

345
00:14:47,879 --> 00:14:50,279
I think for interpartism,
we already talk about that.

346
00:14:50,279 --> 00:14:51,979
It's basically the bubble. Okay.

347
00:14:51,979 --> 00:14:55,580
Whenever you talk about
pipeline PismT about interrupt,

348
00:14:55,580 --> 00:14:57,359
you think bubble, okay?

349
00:14:57,359 --> 00:15:01,739
But in intra, this is more
kind of, like, complicated.

350
00:15:01,739 --> 00:15:03,684
Okay? How we model this?

351
00:15:03,684 --> 00:15:08,349
So basically intra Pism
just to recap, right,

352
00:15:08,349 --> 00:15:10,309
we will partition a
single operator and

353
00:15:10,309 --> 00:15:14,589
explore the parism inside
of that operator, right?

354
00:15:14,589 --> 00:15:17,350
And so basically in
the following slides,

355
00:15:17,350 --> 00:15:20,090
we are going to study how to
paralyze a single operator.

356
00:15:20,090 --> 00:15:23,030
And once we know how to
paralyze a single operator,

357
00:15:23,030 --> 00:15:25,129
we can generalize it to

358
00:15:25,129 --> 00:15:27,809
basically how to paralyze
all the operators in algram.

359
00:15:27,809 --> 00:15:30,770
And we are going to develop
a model mathematical model

360
00:15:30,770 --> 00:15:33,590
to help you analyse
the cost, okay?

361
00:15:33,890 --> 00:15:36,509
So let's move on.

362
00:15:36,509 --> 00:15:39,049
So let's still begin
with the simplest case.

363
00:15:39,049 --> 00:15:41,089
I think this one I have
covered this many,

364
00:15:41,089 --> 00:15:42,750
many times in
different lectures.

365
00:15:42,750 --> 00:15:45,249
So this is animal
wise operator, right?

366
00:15:45,249 --> 00:15:47,649
And here's a code for

367
00:15:47,649 --> 00:15:51,470
an animal wise addition on
two dimensional tensors.

368
00:15:51,470 --> 00:15:53,510
Okay? And the code corresponds

369
00:15:53,510 --> 00:15:57,289
to two level four
loop, which is there.

370
00:15:57,610 --> 00:16:00,930
Okay, by observing this
two level four loop,

371
00:16:00,930 --> 00:16:05,409
you observe that there's no
dependency on these 24 loops.

372
00:16:05,409 --> 00:16:08,029
Okay. So basically,
no dependency.

373
00:16:08,029 --> 00:16:10,330
This follop just go independent.
You can swap the order.

374
00:16:10,330 --> 00:16:11,430
Okay, doesn't matter.

375
00:16:11,430 --> 00:16:13,929
Okay. The reason I
highlight this is

376
00:16:13,929 --> 00:16:16,850
because um because
there's no dependency,

377
00:16:16,850 --> 00:16:18,590
so we can arbitrarily split

378
00:16:18,590 --> 00:16:19,789
the four loops on

379
00:16:19,789 --> 00:16:23,409
different devices for
paralyzation, right?

380
00:16:23,650 --> 00:16:26,129
Now, if we have four devices

381
00:16:26,129 --> 00:16:27,949
denoted by the four
different colors,

382
00:16:27,949 --> 00:16:29,230
then there are multiple ways

383
00:16:29,230 --> 00:16:31,169
to paralyze this
operator, right.

384
00:16:31,169 --> 00:16:33,190
So for example, we can paralyze

385
00:16:33,190 --> 00:16:35,289
this loop N, which is
the outer loop, right?

386
00:16:35,289 --> 00:16:38,609
We can also paralyze the loop D,

387
00:16:38,609 --> 00:16:40,449
which is the inner loop, right?

388
00:16:40,449 --> 00:16:43,849
And uh so basically,

389
00:16:43,849 --> 00:16:46,810
when we pariselop
N in this figure,

390
00:16:46,810 --> 00:16:48,989
and in this case, all
three confers how to

391
00:16:48,989 --> 00:16:51,369
be row partition, right?

392
00:16:51,369 --> 00:16:54,489
Because you need to give
each device some work to do.

393
00:16:54,489 --> 00:16:57,030
And if you choose to
partition the loop N,

394
00:16:57,030 --> 00:17:00,109
then, um, the input
array A and B,

395
00:17:00,109 --> 00:17:02,490
they need to be partitioned
following the low partition,

396
00:17:02,490 --> 00:17:03,690
right on four devices,

397
00:17:03,690 --> 00:17:05,149
and the result
will be written in

398
00:17:05,149 --> 00:17:07,489
a row parting way at the
output device. Okay?

399
00:17:07,489 --> 00:17:10,909
Makes sense, right.
Cool. One thing

400
00:17:10,909 --> 00:17:12,509
you observe is when you do this,

401
00:17:12,509 --> 00:17:14,530
there's no communication cost.

402
00:17:14,530 --> 00:17:17,510
You just do your
partitioning. You are good.

403
00:17:18,270 --> 00:17:23,169
Or we can paralyze
both loop and loop D,

404
00:17:23,169 --> 00:17:25,249
as shown in the right figure.

405
00:17:25,249 --> 00:17:27,690
In this case, all
tensors are partitioned

406
00:17:27,690 --> 00:17:30,569
along both row and
column machine.

407
00:17:30,569 --> 00:17:33,449
Each device basically
stores a sub region of

408
00:17:33,449 --> 00:17:37,130
the original tensor and
works on its own separation.

409
00:17:37,130 --> 00:17:40,130
In this case, there's
still no communication.

410
00:17:40,130 --> 00:17:42,889
Yeah, because the difference
between these two is

411
00:17:42,889 --> 00:17:44,169
basically you partition or you

412
00:17:44,169 --> 00:17:45,510
assign regions differently.

413
00:17:45,510 --> 00:17:49,109
But there's no communication.
Okay. So basically,

414
00:17:49,109 --> 00:17:51,449
depending on the dimension

415
00:17:51,449 --> 00:17:53,390
of the tensor and
the lumber devices,

416
00:17:53,390 --> 00:17:54,810
for example, I can
generalize this into

417
00:17:54,810 --> 00:17:56,669
four dimensional tensor or
the in transformer, right?

418
00:17:56,669 --> 00:17:58,289
I can have 1,000 devices.

419
00:17:58,289 --> 00:18:00,409
Okay. And depending
on this dimension of

420
00:18:00,409 --> 00:18:02,970
tensor and also the
lumbre devices,

421
00:18:02,970 --> 00:18:04,929
there are a lot of
other variants.

422
00:18:04,929 --> 00:18:07,789
And, for example, for
five dimensional tensor,

423
00:18:07,789 --> 00:18:09,729
there's a five level four loop,

424
00:18:09,729 --> 00:18:13,009
and, which leads to
more part histrgs.

425
00:18:13,009 --> 00:18:14,570
But the thing here
is no dependency,

426
00:18:14,570 --> 00:18:15,890
so there's no communication.

427
00:18:15,890 --> 00:18:20,379
Okay? Of course, we're
going to do this, right?

428
00:18:20,379 --> 00:18:24,999
So let's move on to more
complicated case, met M. Okay.

429
00:18:24,999 --> 00:18:27,139
Met mo basically corresponds to

430
00:18:27,139 --> 00:18:29,439
this three level
for loop, right?

431
00:18:29,439 --> 00:18:31,320
And similarly, we can paralyze

432
00:18:31,320 --> 00:18:35,000
one loop or we can paralyze
several loops together,

433
00:18:35,000 --> 00:18:37,819
and assign the computation
to different devices.

434
00:18:37,819 --> 00:18:41,460
Okay. So basically, let's
look at each loop, okay?

435
00:18:41,460 --> 00:18:43,519
So loop I and loop J,

436
00:18:43,519 --> 00:18:45,620
they are very easy to paralyze

437
00:18:45,620 --> 00:18:47,579
because there's no
dependency, right?

438
00:18:47,579 --> 00:18:48,859
They are the independent loops.

439
00:18:48,859 --> 00:18:53,260
Okay. And no carried
on these four loops,

440
00:18:53,260 --> 00:18:55,059
and we can arbitrarily split

441
00:18:55,059 --> 00:18:57,559
these two loops on
different devices.

442
00:18:57,559 --> 00:19:00,499
But the complicates in
Mami basically we have a

443
00:19:00,499 --> 00:19:02,939
loop K. This loop K,

444
00:19:02,939 --> 00:19:05,699
we call it a reduction loop
because in this loop k,

445
00:19:05,699 --> 00:19:07,059
we are accumulating the results.

446
00:19:07,059 --> 00:19:08,359
I think you did this
in your homework.

447
00:19:08,359 --> 00:19:10,300
This is a reduction loop,

448
00:19:10,300 --> 00:19:12,019
and if we paralyze loop K,

449
00:19:12,019 --> 00:19:13,839
we basically need to accumulate

450
00:19:13,839 --> 00:19:17,540
the partial results
from all devices,

451
00:19:17,540 --> 00:19:22,029
which can be done by by what?

452
00:19:22,029 --> 00:19:25,009
Bio coommunication
primitive, what is that?

453
00:19:25,009 --> 00:19:28,189
Or reduce. Yeah. Because
we have partial results,

454
00:19:28,189 --> 00:19:31,449
so basically accumulate results.

455
00:19:31,449 --> 00:19:33,469
Again, if we have

456
00:19:33,469 --> 00:19:36,729
four devices denoted by
four different colors,

457
00:19:36,729 --> 00:19:38,729
we also use gray

458
00:19:38,729 --> 00:19:41,679
here to denote if a tensor
is replicated or not.

459
00:19:41,679 --> 00:19:43,830
So first, we can paralyze,

460
00:19:43,830 --> 00:19:45,629
uh, basically loop I.

461
00:19:45,629 --> 00:19:49,290
This means that matrix C and
A have to be partitioned.

462
00:19:49,290 --> 00:19:53,690
Each device, um, so here,
this difference okay.

463
00:19:53,690 --> 00:19:56,369
This is different from pre
phis Y is element addition.

464
00:19:56,369 --> 00:20:00,029
But here, if I choose
to partition A loop I

465
00:20:00,029 --> 00:20:01,609
then A will be

466
00:20:01,609 --> 00:20:04,949
partition Be I is basically
a dimension of A,

467
00:20:04,949 --> 00:20:08,469
and I is also dimension
of the resulting matrix.

468
00:20:08,469 --> 00:20:10,229
But here, I cannot
part in B, why?

469
00:20:10,229 --> 00:20:12,310
Because if I choose
to partition loop,

470
00:20:12,310 --> 00:20:15,329
then B has to be replicated
across all devices.

471
00:20:15,329 --> 00:20:18,174
No the difference, MO
is quite different.

472
00:20:18,174 --> 00:20:21,300
Okay. Um, okay.

473
00:20:21,300 --> 00:20:23,040
In order to compute
this assigned portion,

474
00:20:23,040 --> 00:20:24,379
each device has to access

475
00:20:24,379 --> 00:20:27,040
the whole matrix B.
That's my point.

476
00:20:27,040 --> 00:20:30,339
Therefore, to run this computation
without communication,

477
00:20:30,339 --> 00:20:34,819
matrix B has to be
replicated on all devices.

478
00:20:34,819 --> 00:20:36,699
And this can also be illustrated

479
00:20:36,699 --> 00:20:40,019
by this block matrix
application notation,

480
00:20:40,019 --> 00:20:42,540
where C and A are split into

481
00:20:42,540 --> 00:20:45,900
four blocks where B are
replicated on all devices.

482
00:20:45,900 --> 00:20:50,999
Okay? Any question?
Good, right? Okay.

483
00:20:50,999 --> 00:20:52,780
We're going to make it
more and more complicated.

484
00:20:52,780 --> 00:20:59,279
Okay. The second case is
basically to paralyze loop K,

485
00:20:59,279 --> 00:21:01,380
which is the reduction loop.

486
00:21:01,380 --> 00:21:05,040
Similarly, we can't
derive that matrix

487
00:21:05,040 --> 00:21:09,559
A has to be column protein
and B has to be partin.

488
00:21:09,559 --> 00:21:15,459
Why? Because K is a
dimension of both A and B.

489
00:21:15,459 --> 00:21:17,539
If we choose to part in loop K,

490
00:21:17,539 --> 00:21:19,259
then we have to part
in both A and B.

491
00:21:19,259 --> 00:21:23,139
Okay? To get the result of C,

492
00:21:23,139 --> 00:21:25,140
we need to use reduce,

493
00:21:25,140 --> 00:21:26,280
like I said, because after

494
00:21:26,280 --> 00:21:28,639
this commutation device
only have a partial sum,

495
00:21:28,639 --> 00:21:31,799
reduce the results to
get the result C. Okay.

496
00:21:31,799 --> 00:21:35,659
Um, so basically we need to
use reduce to accumulate,

497
00:21:35,659 --> 00:21:37,419
the partial reduction results.

498
00:21:37,419 --> 00:21:40,039
And the final C is replicated on

499
00:21:40,039 --> 00:21:43,659
all devices because of it
as a result of reduce.

500
00:21:43,659 --> 00:21:48,239
Okay? This can also be
illustrated by, this notation.

501
00:21:48,239 --> 00:21:49,639
The red notation,
I think you are

502
00:21:49,639 --> 00:21:51,439
very familiar if I read
it in this form, right?

503
00:21:51,439 --> 00:21:53,719
I part in A, following
these columns,

504
00:21:53,719 --> 00:21:56,560
I part in B, following us
and just add them together.

505
00:21:56,560 --> 00:21:57,960
And each part of communication

506
00:21:57,960 --> 00:21:59,800
happens only on one device.

507
00:21:59,800 --> 00:22:02,459
Okay. This basically give you uh

508
00:22:02,459 --> 00:22:03,699
some very low level like

509
00:22:03,699 --> 00:22:06,960
visualization of what's going
on when we paralyze memo.

510
00:22:09,130 --> 00:22:12,429
We can also paralyze
two loops together,

511
00:22:12,429 --> 00:22:14,970
and things will become
more and more complicated.

512
00:22:14,970 --> 00:22:18,369
For example, we can
paralyze loop I

513
00:22:18,369 --> 00:22:20,449
and loop J because

514
00:22:20,449 --> 00:22:22,770
these two loops are
not reduction loop,

515
00:22:22,770 --> 00:22:24,669
so we don't need or reduce.

516
00:22:24,669 --> 00:22:26,829
As figure.

517
00:22:26,829 --> 00:22:29,410
Matrix C is both
column partitioned

518
00:22:29,410 --> 00:22:32,609
and lo paritioned
into four regions,

519
00:22:32,609 --> 00:22:35,650
and each device is responsible
for computing one region.

520
00:22:35,650 --> 00:22:38,969
In order to compute this
C without communication,

521
00:22:38,969 --> 00:22:43,009
matrix A and matrix B how
to be partially tiled.

522
00:22:43,009 --> 00:22:46,720
So do you know what
is partially tiled?

523
00:22:46,720 --> 00:22:49,149
You first decompose it

524
00:22:49,149 --> 00:22:51,489
into two parts and
then you tie it.

525
00:22:51,489 --> 00:22:53,569
Okay? And I will

526
00:22:53,569 --> 00:22:56,309
give you some reading
material later, but uh,

527
00:22:56,309 --> 00:22:58,470
let's grind this a
little bit later,

528
00:22:58,470 --> 00:23:00,009
basically images and B have

529
00:23:00,009 --> 00:23:02,049
to be partially
tiled matrix A is

530
00:23:02,049 --> 00:23:05,589
partitioned along the
row into two parts.

531
00:23:05,589 --> 00:23:08,490
The first part is
replicated both

532
00:23:08,490 --> 00:23:11,609
on device one and
device B. You got.

533
00:23:11,609 --> 00:23:14,470
Basically we part Images
A using row party,

534
00:23:14,470 --> 00:23:17,629
and then we replicate the
upper part onto two devices,

535
00:23:17,629 --> 00:23:20,009
the right part into
another two devices.

536
00:23:20,009 --> 00:23:23,364
That is how we part
matrix A here.

537
00:23:23,364 --> 00:23:27,340
The second part is wrap it on
both device three and four.

538
00:23:27,340 --> 00:23:30,220
By storing A and B in this way,

539
00:23:30,220 --> 00:23:31,599
we can basically compute C in

540
00:23:31,599 --> 00:23:33,980
parallel without
any communication.

541
00:23:35,220 --> 00:23:38,219
Same for matrix B, what
do we do is basically we

542
00:23:38,219 --> 00:23:41,360
first column part in
matrix B into two parts.

543
00:23:41,360 --> 00:23:43,079
But here we have four devices.

544
00:23:43,079 --> 00:23:44,979
We rap eftter part on

545
00:23:44,979 --> 00:23:47,299
two devices and wrap on
another two devices,

546
00:23:47,299 --> 00:23:48,900
we can proceed this computation

547
00:23:48,900 --> 00:23:51,460
without problem
no communication.

548
00:23:53,660 --> 00:23:56,840
Similarly, we can parallelze

549
00:23:56,840 --> 00:24:00,619
both loop I and loop
K. The results,

550
00:24:00,619 --> 00:24:04,969
this basically results in
partially tiled output C. Um,

551
00:24:04,969 --> 00:24:06,649
and because we
parallelse loop K,

552
00:24:06,649 --> 00:24:09,529
we also need an use to get C.

553
00:24:09,529 --> 00:24:12,029
Okay? And as you can see,

554
00:24:12,029 --> 00:24:13,949
uh, there are a lot of variants,

555
00:24:13,949 --> 00:24:17,929
depends on which loop you
choose to paralyze and,

556
00:24:17,929 --> 00:24:21,489
and how you map the loops
to your devices, right?

557
00:24:21,489 --> 00:24:24,090
And actually, it
is very tedious,

558
00:24:24,090 --> 00:24:27,489
um, to basically
go through this.

559
00:24:27,489 --> 00:24:29,549
It's very tedious. There are
many many possibilities,

560
00:24:29,549 --> 00:24:31,410
but it's not impossible.

561
00:24:31,410 --> 00:24:35,630
I just take a p like one
day to grind this, okay?

562
00:24:35,630 --> 00:24:38,089
And I hope you spend
some time grind this.

563
00:24:38,089 --> 00:24:39,890
And for this two dimension memo,

564
00:24:39,890 --> 00:24:41,790
there are only like 16 cases.

565
00:24:41,790 --> 00:24:45,330
Yeah. And you can basically
understand all these cases.

566
00:24:45,330 --> 00:24:47,709
You can enumerate them, okay?

567
00:24:47,709 --> 00:24:50,390
But there's something
that even more difficult,

568
00:24:50,390 --> 00:24:54,479
that is um, column two D. Okay?

569
00:24:54,479 --> 00:24:57,759
A is basically actually,
it's before, right.

570
00:24:57,759 --> 00:25:00,740
E is basically seven
level for loop.

571
00:25:00,740 --> 00:25:03,459
And we can characterize
loops one by one.

572
00:25:03,459 --> 00:25:05,720
So like I said, I think
I did this before.

573
00:25:05,720 --> 00:25:08,100
This is the spatial loops,
and it's very easy.

574
00:25:08,100 --> 00:25:11,819
There's no reduction. So you
can arbitrarily split it.

575
00:25:11,819 --> 00:25:14,339
Okay? And these are basically

576
00:25:14,339 --> 00:25:17,880
another stencil computton loop,

577
00:25:17,880 --> 00:25:20,999
and um this is basically
handling the boundary condition.

578
00:25:20,999 --> 00:25:22,819
You need to be very careful
because you sliding

579
00:25:22,819 --> 00:25:26,419
the uh convolutional
filter from left to right,

580
00:25:26,419 --> 00:25:27,859
from up to bottom and you want

581
00:25:27,859 --> 00:25:30,300
to handle the boundary
condition, okay?

582
00:25:30,300 --> 00:25:33,419
Apparently, this one is the
most difficult because it is

583
00:25:33,419 --> 00:25:34,899
a reduction loop you
are accumulating

584
00:25:34,899 --> 00:25:36,880
results in this loop.

585
00:25:36,880 --> 00:25:39,620
And another two reacting loops.

586
00:25:39,620 --> 00:25:42,020
But you don't have to
paralyze it because it's

587
00:25:42,020 --> 00:25:44,559
a very small reaction
only five elements.

588
00:25:44,559 --> 00:25:47,659
Be most convolutional
two implement,

589
00:25:47,659 --> 00:25:49,859
you only use a small filter.

590
00:25:50,690 --> 00:25:55,009
Okay. In order to
paralyze this operator,

591
00:25:55,009 --> 00:25:56,470
you can imagine how many cases

592
00:25:56,470 --> 00:25:58,449
you need to enumerate a lot.

593
00:25:58,449 --> 00:26:00,149
But we're not going to do this.

594
00:26:00,149 --> 00:26:03,310
Okay. It's fine.
Yeah. Let's give.

595
00:26:03,310 --> 00:26:06,270
Okay. I think the reason

596
00:26:06,270 --> 00:26:09,189
I go through this is because
I want to give you first,

597
00:26:09,189 --> 00:26:11,969
understanding how we
paralyze a single operator.

598
00:26:11,969 --> 00:26:13,649
It's essentially like we choose

599
00:26:13,649 --> 00:26:15,209
a few loops to paralyze to

600
00:26:15,209 --> 00:26:19,029
partition and then we might
each partito um a device.

601
00:26:19,029 --> 00:26:21,329
And because of the
way we partition, uh,

602
00:26:21,329 --> 00:26:23,409
it will result into
different commuting patterns

603
00:26:23,409 --> 00:26:26,209
and communicating
patterns. Okay?

604
00:26:28,530 --> 00:26:30,869
So so far, you have learned how

605
00:26:30,869 --> 00:26:32,769
to paralyze a single operator.

606
00:26:32,769 --> 00:26:34,649
Now, let us basically

607
00:26:34,649 --> 00:26:36,489
move to a graph and
see some examples.

608
00:26:36,489 --> 00:26:38,749
Okay? So in this page,

609
00:26:38,749 --> 00:26:40,349
we all show that basically

610
00:26:40,349 --> 00:26:42,149
data partism is actually

611
00:26:42,149 --> 00:26:44,049
a special case of
intraoperatism.

612
00:26:44,049 --> 00:26:45,989
I think we did this, but
I just want to recap.

613
00:26:45,989 --> 00:26:48,630
Okay? So first,
we'll use legends

614
00:26:48,630 --> 00:26:50,849
to represent whether tensor

615
00:26:50,849 --> 00:26:52,849
is replicated row parison
and common parison,

616
00:26:52,849 --> 00:26:54,569
of course, and we
will use two types of

617
00:26:54,569 --> 00:26:57,109
intra parallel strategies
for matm here.

618
00:26:57,109 --> 00:27:00,469
Okay? Um, so the type one

619
00:27:00,469 --> 00:27:03,789
is basically we partition
the met along row, right?

620
00:27:03,789 --> 00:27:06,089
If you still remember
this type one,

621
00:27:06,089 --> 00:27:08,589
we don't need any
communication, right? Okay.

622
00:27:08,589 --> 00:27:11,250
Because if we choose this
type one partitioning,

623
00:27:11,250 --> 00:27:13,089
the input A is partition and

624
00:27:13,089 --> 00:27:15,309
the input B is replicated and we

625
00:27:15,309 --> 00:27:19,429
get subaring results
on the result matrix.

626
00:27:19,429 --> 00:27:23,569
Okay? The type B
basically partitions,

627
00:27:23,810 --> 00:27:26,669
the inner loop
reduction K, right?

628
00:27:26,669 --> 00:27:31,150
And so if we do this
type two partitioning,

629
00:27:31,150 --> 00:27:33,229
then type two basically parting

630
00:27:33,229 --> 00:27:35,810
both inputs along the
reduction dimension,

631
00:27:35,810 --> 00:27:38,570
uh it basically requires
finish this computation,

632
00:27:38,570 --> 00:27:40,649
it requires or reduce, right?

633
00:27:40,649 --> 00:27:44,500
Okay. Now, let's draw

634
00:27:44,500 --> 00:27:47,079
them graph for two
layer MLP, right?

635
00:27:47,079 --> 00:27:49,280
So here is a forward path.

636
00:27:49,280 --> 00:27:51,659
The wet W one and W two are

637
00:27:51,659 --> 00:27:54,119
basically replicated.
This is data partism?

638
00:27:54,119 --> 00:27:57,299
And all other communications
such as Mat Moss and Relu,

639
00:27:57,299 --> 00:28:00,120
um, they are partition
along the batch dimension.

640
00:28:00,120 --> 00:28:02,739
Okay? Uh, there's
no communication.

641
00:28:02,739 --> 00:28:05,830
So running this requires
no communication.

642
00:28:05,830 --> 00:28:08,179
Then we draw the
backward graph, right?

643
00:28:08,179 --> 00:28:10,999
And now, I reveal the
whole figure to you.

644
00:28:10,999 --> 00:28:13,320
I think I did this when
I talk about data pism.

645
00:28:13,320 --> 00:28:16,179
Um, the backward
pass consists of

646
00:28:16,179 --> 00:28:19,259
one type one memo
and Type two memo.

647
00:28:19,259 --> 00:28:22,819
Right. One type one memo and
two Type two memo. Okay.

648
00:28:22,819 --> 00:28:27,640
Um, so it requires some
reds on the gradients,

649
00:28:27,640 --> 00:28:30,479
and this is basically
the same as datapism.

650
00:28:30,479 --> 00:28:32,719
Okay? Now, I basically connect

651
00:28:32,719 --> 00:28:34,260
this datapism communication

652
00:28:34,260 --> 00:28:36,260
to this operative partitioning.

653
00:28:36,260 --> 00:28:39,580
And now you can see, Datapism
the reason dataparism

654
00:28:39,580 --> 00:28:41,219
requires reduce is because we

655
00:28:41,219 --> 00:28:43,420
choose to partition the loop,

656
00:28:43,420 --> 00:28:46,440
the metamo in this way, okay?

657
00:28:46,440 --> 00:28:49,079
Later, I will show you a lot of

658
00:28:49,079 --> 00:28:50,639
other parallel
strategies that they

659
00:28:50,639 --> 00:28:52,379
are basically just
special cases like this.

660
00:28:52,379 --> 00:28:54,940
So basically, we're basically
playing this coloring game,

661
00:28:54,940 --> 00:28:56,680
we give different color

662
00:28:56,680 --> 00:28:58,759
to different regions
of the operator,

663
00:28:58,759 --> 00:29:00,840
and it will result into
different communication.

664
00:29:00,840 --> 00:29:04,839
Okay? Any question?

665
00:29:05,970 --> 00:29:08,629
Cool. Let's continue.

666
00:29:08,629 --> 00:29:10,950
Okay. So when you're applying

667
00:29:10,950 --> 00:29:14,230
this intra alismu
for whole graph,

668
00:29:14,230 --> 00:29:18,310
there are repartitioning
communication cost on the edges.

669
00:29:18,310 --> 00:29:22,130
Okay, let me explain. This is
because different operators

670
00:29:22,130 --> 00:29:24,449
paralyzing strategy
might require

671
00:29:24,449 --> 00:29:27,370
different partitioning
formats of the symptoms.

672
00:29:27,370 --> 00:29:30,829
So we need to do
repartition or recharging.

673
00:29:30,829 --> 00:29:34,489
Let's still use this two
layer MLP as an example.

674
00:29:34,489 --> 00:29:37,309
Consider the edge between

675
00:29:37,309 --> 00:29:41,630
Lu and the second
mano, the longer edge.

676
00:29:41,630 --> 00:29:45,149
The partitioning
format of this al uh,

677
00:29:45,149 --> 00:29:48,549
follows the partitioning
format of the previous memo.

678
00:29:48,549 --> 00:29:51,329
It's basically inherent
from there. Okay?

679
00:29:51,570 --> 00:29:56,230
Assuming this figure, assuming
the Lu is low partition.

680
00:29:56,230 --> 00:29:59,589
That is, I assume the previous
graph was low partition.

681
00:29:59,589 --> 00:30:01,649
Okay. It basically
inherits the sharding,

682
00:30:01,649 --> 00:30:03,190
although it's Lu because Lu

683
00:30:03,190 --> 00:30:04,870
is element wise, you don't
have to do anything.

684
00:30:04,870 --> 00:30:08,019
Okay? So for the second memo,

685
00:30:08,019 --> 00:30:11,279
um, depending on its
chosen paten strategy,

686
00:30:11,279 --> 00:30:12,939
type one or type
two or type three,

687
00:30:12,939 --> 00:30:15,440
it may have a
different requirement

688
00:30:15,440 --> 00:30:18,340
of the input for the
input partin format.

689
00:30:18,340 --> 00:30:20,139
So if the second,

690
00:30:20,139 --> 00:30:23,679
meto also use a type one
partitioning strategy,

691
00:30:23,679 --> 00:30:25,359
which I showed two
slides before, right?

692
00:30:25,359 --> 00:30:26,639
You parting along the row.

693
00:30:26,639 --> 00:30:29,519
Okay? If you choose type
one paren strategy,

694
00:30:29,519 --> 00:30:33,399
then this Lu actually also
needs to be row paten, right?

695
00:30:33,399 --> 00:30:35,739
Remember in type one,
the first input need to

696
00:30:35,739 --> 00:30:36,879
be row partition and the second

697
00:30:36,879 --> 00:30:38,240
input need to be replicated.

698
00:30:38,240 --> 00:30:41,300
Okay? So if we like
this memo to choose

699
00:30:41,300 --> 00:30:45,060
the type one partin
strategy, Okay, um,

700
00:30:45,060 --> 00:30:46,839
We found that this rio

701
00:30:46,839 --> 00:30:49,140
actually matches with
the previous rival.

702
00:30:49,140 --> 00:30:50,739
They are both row partition.

703
00:30:50,739 --> 00:30:52,999
Therefore, this one is
good because you can

704
00:30:52,999 --> 00:30:55,700
proceed with the competon
without any problem.

705
00:30:55,700 --> 00:30:57,179
Because that alo has

706
00:30:57,179 --> 00:31:00,339
all the content it needed
on each device, okay?

707
00:31:00,350 --> 00:31:03,169
But if the second Mdm use

708
00:31:03,169 --> 00:31:08,049
a slightly different par
strategy, freedom here.

709
00:31:08,049 --> 00:31:10,029
I choose a different
type of strategy.

710
00:31:10,029 --> 00:31:12,930
Well, I require this
u to be replicated,

711
00:31:12,930 --> 00:31:15,050
but the W two to be column part.

712
00:31:15,050 --> 00:31:17,650
Okay? And in this case,

713
00:31:17,650 --> 00:31:20,649
this alo needs to be
replicated, right?

714
00:31:20,649 --> 00:31:22,729
Which means that
both devices need to

715
00:31:22,729 --> 00:31:24,789
have a copy of the Lu results.

716
00:31:24,789 --> 00:31:27,690
Okay? Uh, in this case,

717
00:31:27,690 --> 00:31:29,949
you can see the edge becomes a

718
00:31:29,949 --> 00:31:33,150
slightly different one.
So what is that edge?

719
00:31:35,750 --> 00:31:39,470
It is a connective
communication parameter.

720
00:31:39,470 --> 00:31:43,670
I think I cover. Okay.
Yeah, it's all gather.

721
00:31:43,670 --> 00:31:45,249
Yeah. So in this way, if you

722
00:31:45,249 --> 00:31:47,189
choose different
patina strategy for

723
00:31:47,189 --> 00:31:51,590
different metamol you suffer
a repartitioning cost,

724
00:31:51,590 --> 00:31:53,849
and this repartiting
cost will become

725
00:31:53,849 --> 00:31:57,029
a format of a connective
permissive. Okay.

726
00:31:57,029 --> 00:32:00,529
And in this figure is
basically all gather.

727
00:32:00,529 --> 00:32:04,069
Okay. Cool. So in

728
00:32:04,069 --> 00:32:07,729
order to minimize the cost of
executing the entire graph,

729
00:32:07,729 --> 00:32:09,329
we also need to carefully

730
00:32:09,329 --> 00:32:12,249
choose strategies
for each operator to

731
00:32:12,249 --> 00:32:14,369
minimize this
repartining cost when we

732
00:32:14,369 --> 00:32:17,609
design our partin strategy
for the entire new network.

733
00:32:17,609 --> 00:32:21,899
That's the point I want to
make. Just to recap, right?

734
00:32:21,899 --> 00:32:24,180
So I think now you can
basically enumerate

735
00:32:24,180 --> 00:32:25,920
all the possible
communicating costs

736
00:32:25,920 --> 00:32:27,079
when you do recharging, right?

737
00:32:27,079 --> 00:32:28,099
I think you guys are very

738
00:32:28,099 --> 00:32:29,620
familiar with this
figure already.

739
00:32:29,620 --> 00:32:33,539
Um, repartitioning is basically
like we ask you to swap

740
00:32:33,539 --> 00:32:37,939
the coloring scheme of
the tensors, right?

741
00:32:37,939 --> 00:32:40,219
And depending on how
you want to swap,

742
00:32:40,219 --> 00:32:42,580
you basically introduce
connective primitive.

743
00:32:42,580 --> 00:32:47,400
Okay. Okay, let's summarize

744
00:32:47,400 --> 00:32:49,660
the problem of paralyzing

745
00:32:49,660 --> 00:32:51,520
all the operators in
your graph, okay?

746
00:32:51,520 --> 00:32:54,179
So given a graph,
for each operator,

747
00:32:54,179 --> 00:32:57,559
there are multiple paralyzing
strategies for it, okay?

748
00:32:57,559 --> 00:33:02,479
And our goal is to pick
one parallel strategy for

749
00:33:02,479 --> 00:33:04,639
each operator so that we can

750
00:33:04,639 --> 00:33:08,435
minimize the cost of
executing the entire graph.

751
00:33:08,435 --> 00:33:10,969
Does that make sense?
So I'm basically

752
00:33:10,969 --> 00:33:13,250
stating this problem as
an optimization now.

753
00:33:13,250 --> 00:33:16,829
Okay? You want to pick a
coloring scheme for each node,

754
00:33:16,829 --> 00:33:18,170
and then you enumerate

755
00:33:18,170 --> 00:33:20,190
all the possible commuting costs

756
00:33:20,190 --> 00:33:21,890
and you want to
minimize that cost.

757
00:33:21,890 --> 00:33:25,809
That will basically make
your graph run fast. Okay?

758
00:33:26,530 --> 00:33:29,010
So the cost basically consists

759
00:33:29,010 --> 00:33:32,670
of the node cost
and the edge cost.

760
00:33:32,670 --> 00:33:34,429
Okay? The node cost are

761
00:33:34,429 --> 00:33:36,630
basically computation
and communication cost.

762
00:33:36,630 --> 00:33:38,729
Because when you choose,

763
00:33:38,729 --> 00:33:40,790
when you choose a
particular type

764
00:33:40,790 --> 00:33:43,370
of pigon you already
have some cost.

765
00:33:43,370 --> 00:33:46,874
You choose type two mama
paig you have or reduce.

766
00:33:46,874 --> 00:33:51,520
Right. Okay. And the
second cause is edge case.

767
00:33:51,520 --> 00:33:55,000
That is, you choose protein
srgy for that node,

768
00:33:55,000 --> 00:33:57,040
and you choose protein
unit for second node.

769
00:33:57,040 --> 00:33:59,720
And because of the sharing
of the first node,

770
00:33:59,720 --> 00:34:03,860
output and the second nodes
input are not all green.

771
00:34:03,860 --> 00:34:05,480
So you're suffering edge case.

772
00:34:05,480 --> 00:34:07,739
That is, you need to reshare and

773
00:34:07,739 --> 00:34:11,000
that shard comes at a
form of connecting.

774
00:34:11,000 --> 00:34:12,659
Therefore, if you want to write

775
00:34:12,659 --> 00:34:14,060
down this problem is basically,

776
00:34:14,060 --> 00:34:19,319
um, that equation, node
cost plus edge cost.

777
00:34:19,319 --> 00:34:24,019
Edge cost is a resting cost
and nod cost is a cost

778
00:34:24,019 --> 00:34:26,139
inherent to the specific type of

779
00:34:26,139 --> 00:34:29,439
parting strategy you
choose for our p. Okay.

780
00:34:29,439 --> 00:34:31,739
And now I think you roughly have

781
00:34:31,739 --> 00:34:35,599
an idea how to solve
this question, right?

782
00:34:35,599 --> 00:34:38,400
One easy way to solve this
problem is basically,

783
00:34:38,400 --> 00:34:40,619
you enumerate all
the possibilities.

784
00:34:40,619 --> 00:34:42,920
You keep enumeraating the graph,

785
00:34:42,920 --> 00:34:44,360
and you basically enumeraating

786
00:34:44,360 --> 00:34:46,200
every type every possibility,

787
00:34:46,200 --> 00:34:47,820
how you can partition operator.

788
00:34:47,820 --> 00:34:49,279
And once you lock down

789
00:34:49,279 --> 00:34:52,180
a possibility of the partitioning
of all the operators,

790
00:34:52,180 --> 00:34:55,039
you basically can't derive
all the cost, right?

791
00:34:55,039 --> 00:34:57,660
You write down, how many
connective communication

792
00:34:57,660 --> 00:34:59,799
I incurred because
I choose this.

793
00:34:59,799 --> 00:35:02,540
And basically, then
you start optimizing

794
00:35:02,540 --> 00:35:03,979
right you try to figure out

795
00:35:03,979 --> 00:35:05,984
the one that basically
has the lowest cost.

796
00:35:05,984 --> 00:35:08,690
Okay. That is
basically intraparism.

797
00:35:08,690 --> 00:35:11,429
Okay? It is slightly
different from Inter.

798
00:35:11,429 --> 00:35:15,670
Still remember in Inter, our
goal is to minimize bubbles.

799
00:35:15,670 --> 00:35:17,890
Who a schedule that
minimize bubbles,

800
00:35:17,890 --> 00:35:20,570
and we want to make sure
the stages are equivalent.

801
00:35:20,570 --> 00:35:24,189
Okay? But in Intra is
basically this equation. Okay?

802
00:35:24,189 --> 00:35:29,939
Any question? Cool. I
hope you get it, okay?

803
00:35:29,939 --> 00:35:31,539
If you don't I hope you

804
00:35:31,539 --> 00:35:33,939
spending more time
understanding the slides, okay?

805
00:35:33,939 --> 00:35:35,379
And this is very
important because

806
00:35:35,379 --> 00:35:37,379
once you understand mathematics,

807
00:35:37,379 --> 00:35:39,280
you will be able to understand

808
00:35:39,280 --> 00:35:41,139
all the partisan
people invented today.

809
00:35:41,139 --> 00:35:42,359
Okay? What they are doing is

810
00:35:42,359 --> 00:35:44,259
basically try to
minimize this cost.

811
00:35:44,259 --> 00:35:47,300
Okay? So to solve this problem,

812
00:35:47,300 --> 00:35:49,760
people have used
various solutions,

813
00:35:49,760 --> 00:35:53,519
including, manual design,
randomized search,

814
00:35:53,519 --> 00:35:56,280
dynamic programming, and the
integer linear programming.

815
00:35:56,280 --> 00:35:57,499
As I said, they
are basically the

816
00:35:57,499 --> 00:35:59,439
optimizing servers to
solve this problem.

817
00:35:59,439 --> 00:36:01,199
Okay? And I will show

818
00:36:01,199 --> 00:36:03,490
a few solutions which
are very notable.

819
00:36:03,490 --> 00:36:06,899
Okay. I will first

820
00:36:06,899 --> 00:36:09,159
introduce some
strategies that are

821
00:36:09,159 --> 00:36:12,460
manually designed
for specific notes,

822
00:36:12,460 --> 00:36:16,520
including AlexNet
Makaton RM and THR MOE.

823
00:36:16,520 --> 00:36:18,819
AlexNet is the beginning
of deep learning, right?

824
00:36:18,819 --> 00:36:20,960
Macron RM is the one for media,

825
00:36:20,960 --> 00:36:22,539
okay? Use every day.

826
00:36:22,539 --> 00:36:24,420
TR is a one by Google.

827
00:36:24,420 --> 00:36:27,239
Okay? As well I introduced
several systems that

828
00:36:27,239 --> 00:36:30,299
basically, designed for this.

829
00:36:30,299 --> 00:36:32,729
Okay? So right from

830
00:36:32,729 --> 00:36:34,689
the beginning of
modern deep learning,

831
00:36:34,689 --> 00:36:37,810
um, people start
using intra partism.

832
00:36:37,810 --> 00:36:40,449
Okay? Um, Alex a good example.

833
00:36:40,449 --> 00:36:44,149
This figure was quoted
from Axon paper, right?

834
00:36:44,149 --> 00:36:46,569
And I also showed this figure
in my first lecture, okay?

835
00:36:46,569 --> 00:36:50,129
Um so basically in this
figure, you can see,

836
00:36:50,129 --> 00:36:53,089
the author tries to
convey a idea where

837
00:36:53,089 --> 00:36:56,289
the layer was partitioned
and each layer has a part,

838
00:36:56,289 --> 00:37:00,229
acute different uh on
different devices, okay?

839
00:37:00,229 --> 00:37:03,770
And basically it partitions
a group convolution,

840
00:37:03,770 --> 00:37:06,970
uh into two parts and
align them to GPS

841
00:37:06,970 --> 00:37:08,769
this enables Alexa to

842
00:37:08,769 --> 00:37:11,350
basically increase
the top one accuracy.

843
00:37:11,350 --> 00:37:14,509
Uh, buy almost
like 2% on EB net.

844
00:37:14,509 --> 00:37:15,489
But just doing this because

845
00:37:15,489 --> 00:37:16,769
you home multi people who train.

846
00:37:16,769 --> 00:37:22,769
E. A more recent example
is basically Microtron RM,

847
00:37:22,769 --> 00:37:24,189
and this is a given reading.

848
00:37:24,189 --> 00:37:25,009
I think at this point,

849
00:37:25,009 --> 00:37:26,649
you guys already
understand that paper.

850
00:37:26,649 --> 00:37:28,989
But what I try to do is
I basically try to draw

851
00:37:28,989 --> 00:37:32,049
the connection between the
strategies in that paper and,

852
00:37:32,049 --> 00:37:34,530
what I explained in the
mathematical modeling.

853
00:37:34,530 --> 00:37:37,549
Okay? So basically, it used

854
00:37:37,549 --> 00:37:41,230
intraoperatism to train
the language models,

855
00:37:41,230 --> 00:37:44,369
and it designs a very
specific strategy to

856
00:37:44,369 --> 00:37:47,930
paralyze the MLP and math
attention in the transformer.

857
00:37:47,930 --> 00:37:50,570
So here, um, I basically
show the figure

858
00:37:50,570 --> 00:37:54,090
three from the paper on
how to paralyze the MLP.

859
00:37:54,090 --> 00:37:57,349
Um, and a similar
stretch is also

860
00:37:57,349 --> 00:38:00,649
applied to the multi head
attention if you read a paper.

861
00:38:00,649 --> 00:38:03,970
Okay? These figure looks
very very complicated.

862
00:38:03,970 --> 00:38:07,910
And we can simplify it once
you understand our model.

863
00:38:07,910 --> 00:38:10,949
No cost and edge cost.
So how to simplify it.

864
00:38:10,949 --> 00:38:13,410
Basically for this
two layer MLP,

865
00:38:13,410 --> 00:38:15,429
we cannot do data
parison because

866
00:38:15,429 --> 00:38:17,549
the weight matrix is too large.

867
00:38:17,549 --> 00:38:22,050
To partition it while minimizing
the communication cost,

868
00:38:22,050 --> 00:38:26,090
we can make the first weight
matrix column partition

869
00:38:26,090 --> 00:38:28,590
and the second row partition.

870
00:38:28,590 --> 00:38:31,189
And by doing this, we
only need to do one or

871
00:38:31,189 --> 00:38:32,429
reduce during the forward and

872
00:38:32,429 --> 00:38:34,200
another reduced during backward.

873
00:38:34,200 --> 00:38:36,130
Okay. This is basically

874
00:38:36,130 --> 00:38:40,169
our visualization for
the microtome paper.

875
00:38:40,169 --> 00:38:43,669
Does this make sense? Okay?

876
00:38:43,669 --> 00:38:47,269
Yeah. And if you just read
try to parse the graph,

877
00:38:47,269 --> 00:38:50,109
you can see, we basically
just have to use.

878
00:38:50,109 --> 00:38:56,189
Okay. Yeah. Um, and

879
00:38:56,189 --> 00:38:58,210
note that some compution
such as dropout

880
00:38:58,210 --> 00:39:00,869
are replicated because
dropout is pretty minimal.

881
00:39:00,869 --> 00:39:02,489
Okay, so we just need
to replicate them.

882
00:39:02,489 --> 00:39:06,449
Okay? But this is pretty
affordable, yeah.

883
00:39:06,449 --> 00:39:10,229
Okay. And today, this
strategy has a lame.

884
00:39:10,229 --> 00:39:11,570
It's called tenser Perl.

885
00:39:11,570 --> 00:39:13,329
Okay. If you go to
any library like

886
00:39:13,329 --> 00:39:17,629
VOMhuinf deep speed microphones
called tenser Barrel.

887
00:39:17,629 --> 00:39:19,110
And it's essentially described.

888
00:39:19,110 --> 00:39:24,779
Okay. Okay. Now, you
understand Megatro RM.

889
00:39:24,779 --> 00:39:28,900
Another example is
basically G hard MOE,

890
00:39:28,900 --> 00:39:30,900
this hard MOE is basically

891
00:39:30,900 --> 00:39:34,420
the first inventor of the
so called per parallel.

892
00:39:34,420 --> 00:39:37,500
Okay. It is a new strategy

893
00:39:37,500 --> 00:39:40,600
for partitioning
experts in MOE model,

894
00:39:40,600 --> 00:39:42,200
and I'm going to cover
the next lecture.

895
00:39:42,200 --> 00:39:44,340
Okay. Per parallel is basically

896
00:39:44,340 --> 00:39:46,979
the key technology
enables Deepi with three.

897
00:39:46,979 --> 00:39:51,099
Okay? Again, I will
show the figure from

898
00:39:51,099 --> 00:39:53,600
the paper on how to paralyze

899
00:39:53,600 --> 00:39:56,960
model you can see this
figure is very complicated.

900
00:39:56,960 --> 00:40:01,770
And uh I'm going to simplify
it using our notation.

901
00:40:02,090 --> 00:40:04,949
So basically what this
paper is doing is

902
00:40:04,949 --> 00:40:07,750
basically for the normal
transformer layers,

903
00:40:07,750 --> 00:40:11,309
it basically does data paring
which is straightforward.

904
00:40:11,309 --> 00:40:13,369
So basically the weights, and

905
00:40:13,369 --> 00:40:16,050
basically replicate it
for the normal layers.

906
00:40:16,050 --> 00:40:18,490
But for the MOE layers,

907
00:40:18,490 --> 00:40:21,029
we partition the weights and

908
00:40:21,029 --> 00:40:24,210
the batch met along
the expert dimension.

909
00:40:24,210 --> 00:40:26,990
And this minimize
the communication.

910
00:40:26,990 --> 00:40:30,650
And you can go back and enumerate
all possible partition.

911
00:40:30,650 --> 00:40:32,410
This is the one that
minimize the communication.

912
00:40:32,410 --> 00:40:33,789
Okay. And you can see

913
00:40:33,789 --> 00:40:36,490
expert design is
indeed expert design.

914
00:40:36,490 --> 00:40:39,909
Okay? Yeah, a lot
of augmentation.

915
00:40:39,909 --> 00:40:43,010
And to switch between

916
00:40:43,010 --> 00:40:44,769
the normal layers to

917
00:40:44,769 --> 00:40:47,669
the MOE layers, we
need a communication.

918
00:40:47,669 --> 00:40:54,369
So what is the communication?
Let me highlight it.

919
00:40:56,570 --> 00:40:59,689
So what is the
communication here?

920
00:41:01,130 --> 00:41:04,609
Sorry? It's at.

921
00:41:04,609 --> 00:41:05,969
Yeah. It's basically auto.

922
00:41:05,969 --> 00:41:08,209
Okay. Makes sense, right?

923
00:41:08,209 --> 00:41:11,410
Because we are swapping the
partitioning dimension.

924
00:41:11,410 --> 00:41:12,949
So that's why it's auto.

925
00:41:12,949 --> 00:41:15,469
Now, you understand why
deep sig inte a lot of

926
00:41:15,469 --> 00:41:18,229
effort on implementing a really,

927
00:41:18,229 --> 00:41:22,329
really fast auto
auto is super small,

928
00:41:22,530 --> 00:41:25,450
because you want to
partition in dimension,

929
00:41:25,450 --> 00:41:27,789
you have to do Auto and
that's all need to be

930
00:41:27,789 --> 00:41:31,199
fast otherwise sonal, okay?

931
00:41:33,930 --> 00:41:38,230
Cool. You get it.
In this homework,

932
00:41:38,230 --> 00:41:39,849
I ask you to implement auto,

933
00:41:39,849 --> 00:41:41,729
but I never ask you to use it

934
00:41:41,729 --> 00:41:43,870
because in the third
part of the homework,

935
00:41:43,870 --> 00:41:46,429
I ask you to basically
use or reduce you

936
00:41:46,429 --> 00:41:48,649
write to Tinder Parl which

937
00:41:48,649 --> 00:41:52,049
is this one ask you to do this.

938
00:41:52,049 --> 00:41:54,830
But in the next homework,

939
00:41:54,830 --> 00:41:57,929
the first assignment,
you're going to do this.

940
00:41:57,929 --> 00:42:01,709
Basically you use your own
auto to implement this part.

941
00:42:01,709 --> 00:42:04,269
Okay. Okay. So this slide is

942
00:42:04,269 --> 00:42:07,290
pretty important for you
to finish the alignment,

943
00:42:07,290 --> 00:42:08,990
first problem of
the next homework.

944
00:42:08,990 --> 00:42:12,149
Okay. Cool.

945
00:42:13,710 --> 00:42:15,509
Okay.

946
00:42:15,509 --> 00:42:19,110
Any question. These
two are basically

947
00:42:19,110 --> 00:42:22,770
the most used intra
parallel strategy.

948
00:42:22,770 --> 00:42:25,290
Let me repeat again,
ten parallel,

949
00:42:25,290 --> 00:42:27,890
where basically you partition

950
00:42:27,890 --> 00:42:31,329
the two weed matrix follow
in different one row,

951
00:42:31,329 --> 00:42:33,630
column, and you
incur one or reduce.

952
00:42:33,630 --> 00:42:35,630
Another is expert parallel,

953
00:42:35,630 --> 00:42:37,550
where you basically partition

954
00:42:37,550 --> 00:42:40,690
the patch met follow the
expert dimension at the MOE,

955
00:42:40,690 --> 00:42:43,410
but you do data parallel
at the normal layers.

956
00:42:43,410 --> 00:42:44,569
And when you switch from

957
00:42:44,569 --> 00:42:47,804
the first party to the second
parity you incur auto.

958
00:42:47,804 --> 00:42:49,519
Okay. That is the idea.

959
00:42:49,519 --> 00:42:53,640
Okay? Now I'm going to introduce
a third part of interop.

960
00:42:53,640 --> 00:42:55,659
It's called zero optimor.

961
00:42:55,659 --> 00:42:58,339
This one was invented
by Deep Speed Team,

962
00:42:58,339 --> 00:43:00,379
this one is also one of

963
00:43:00,379 --> 00:43:04,720
the most used um optimizer,
parting strategy.

964
00:43:04,720 --> 00:43:07,879
In petrology you want to find
a different name for it.

965
00:43:07,879 --> 00:43:12,100
It's called FSDPFully
shared data parallel.

966
00:43:12,100 --> 00:43:15,380
But essentially, their
mechanism is basically

967
00:43:15,380 --> 00:43:18,989
based on this zero
opmeor. Let me explain.

968
00:43:18,989 --> 00:43:21,519
So basically, uh, this
project is called

969
00:43:21,519 --> 00:43:23,999
Zero oppmentar and it's
a memory oppmentation,

970
00:43:23,999 --> 00:43:26,759
um, of data datapoism.

971
00:43:26,759 --> 00:43:30,459
So recall in datapoism we
replicate motor ways, right?

972
00:43:30,459 --> 00:43:33,680
Um but this is not the
only thing we replicate.

973
00:43:33,680 --> 00:43:36,640
So for example, when we choose

974
00:43:36,640 --> 00:43:39,944
to use that atom Oppener, um,

975
00:43:39,944 --> 00:43:44,069
uh, when we do this kind of
mixed precision training,

976
00:43:44,069 --> 00:43:48,349
we also need to store FP
32 master copy, right?

977
00:43:48,349 --> 00:43:51,149
I still remember
in memory lecture,

978
00:43:51,149 --> 00:43:54,910
we also need to store all
those optimal states,

979
00:43:54,910 --> 00:43:56,350
first and second moments,

980
00:43:56,350 --> 00:43:59,710
which are also stored in
LP 32 high precision.

981
00:43:59,710 --> 00:44:03,649
Okay? And this
auxiliary states from

982
00:44:03,649 --> 00:44:08,530
optimor actually is much
larger than LP 16 weights.

983
00:44:08,530 --> 00:44:10,129
It takes a lot of memory.

984
00:44:10,129 --> 00:44:12,190
I hope you still
remember the factor.

985
00:44:12,190 --> 00:44:15,909
So for the atom Opmeor it
takes a factor of 12, right?

986
00:44:15,909 --> 00:44:18,584
And for the rest, it only
take a factor of four.

987
00:44:18,584 --> 00:44:21,300
Okay, it's three times
the weight and gradients.

988
00:44:21,300 --> 00:44:24,459
Okay? And to solve this problem,

989
00:44:24,459 --> 00:44:27,920
this paper proposed to
partition all these sensors,

990
00:44:27,920 --> 00:44:30,500
okay, into three stages.

991
00:44:30,500 --> 00:44:33,320
Okay. Uh This table
basically shows,

992
00:44:33,320 --> 00:44:37,019
the memory usage and
communicating cost of

993
00:44:37,019 --> 00:44:38,599
one in a data parallel and

994
00:44:38,599 --> 00:44:41,379
three stages of their oppumentu.

995
00:44:41,379 --> 00:44:43,639
Okay? And this table is one

996
00:44:43,639 --> 00:44:45,839
of the most important
slides today as well, okay?

997
00:44:45,839 --> 00:44:47,939
And you should
internalize this later.

998
00:44:47,939 --> 00:44:50,779
Okay. And this will be
super helpful for you to

999
00:44:50,779 --> 00:44:55,279
analyze the cost when
you do FSDP, okay?

1000
00:44:55,400 --> 00:44:57,880
Um, and here, basically,

1001
00:44:57,880 --> 00:44:59,240
let me introduce a notation.

1002
00:44:59,240 --> 00:45:00,619
So let M be

1003
00:45:00,619 --> 00:45:04,160
the amber parameters and
M be the ambo devices.

1004
00:45:04,160 --> 00:45:05,980
You are going to
perform parallelism.

1005
00:45:05,980 --> 00:45:08,939
Okay? Um assuming we use

1006
00:45:08,939 --> 00:45:12,939
Atom optimizer with LP 16
mixed precision training,

1007
00:45:12,939 --> 00:45:17,079
the optimeor states,
it takes how many?

1008
00:45:17,820 --> 00:45:20,659
12, right? I repeat this many,

1009
00:45:20,659 --> 00:45:22,460
many times. Okay. You
need to remember.

1010
00:45:22,460 --> 00:45:24,659
The gradients is two, right?

1011
00:45:24,659 --> 00:45:26,679
Because it's PC 16, right?

1012
00:45:26,679 --> 00:45:28,819
It's two bytes. That's what two.

1013
00:45:28,819 --> 00:45:32,319
The model is two,
because it's also two.

1014
00:45:32,319 --> 00:45:35,340
Okay. Therefore, if
you do data partism,

1015
00:45:35,340 --> 00:45:37,299
sically you are saying,

1016
00:45:37,299 --> 00:45:38,859
I'm going to replicate all on

1017
00:45:38,859 --> 00:45:41,520
all devices only partision
along the badge dimension.

1018
00:45:41,520 --> 00:45:42,919
Therefore, the memory cost of

1019
00:45:42,919 --> 00:45:45,240
each device is basically 16.

1020
00:45:45,240 --> 00:45:46,779
Does that make sense?

1021
00:45:46,779 --> 00:45:49,014
Okay, you replicate
everything, right?

1022
00:45:49,014 --> 00:45:51,669
And what's the
communicating cost?

1023
00:45:51,669 --> 00:45:54,129
I already explained
many many times,

1024
00:45:54,129 --> 00:45:56,089
In data partism the
communing cost is

1025
00:45:56,089 --> 00:45:58,870
basically all reduced
of the weights.

1026
00:45:58,870 --> 00:46:01,529
Therefore, it's all reduced to.

1027
00:46:02,090 --> 00:46:06,909
And depending on the stages
you choose to apply zero,

1028
00:46:06,909 --> 00:46:11,349
you are essentially partitioning
different parts of this.

1029
00:46:11,349 --> 00:46:14,709
In zero stage one, you choose
to parti in optimal states.

1030
00:46:14,709 --> 00:46:16,429
In zero stage two, you choose to

1031
00:46:16,429 --> 00:46:18,770
partition optional
states and ingredients,

1032
00:46:18,770 --> 00:46:20,369
and in stage three,

1033
00:46:20,369 --> 00:46:24,319
you pi in all and depending
on how you partition it,

1034
00:46:24,319 --> 00:46:25,780
you will incur different
communication,

1035
00:46:25,780 --> 00:46:27,699
which I will help
you go through next.

1036
00:46:27,699 --> 00:46:31,180
But now, assume you already
know this petition,

1037
00:46:31,180 --> 00:46:33,499
we are able to derive
the memory cost

1038
00:46:33,499 --> 00:46:35,479
and communicating cost, right?

1039
00:46:35,479 --> 00:46:37,999
So maybe let's skip

1040
00:46:37,999 --> 00:46:40,759
communicating cost first
because I need to explain it.

1041
00:46:40,759 --> 00:46:43,039
But for memory it's very
easy to understand, right?

1042
00:46:43,039 --> 00:46:44,979
So if you're part
various states but

1043
00:46:44,979 --> 00:46:46,984
not partying ingredients
and model ways,

1044
00:46:46,984 --> 00:46:48,749
then your per device memory is

1045
00:46:48,749 --> 00:46:51,450
basically these two
will be replicated.

1046
00:46:51,450 --> 00:46:53,969
There is a term of four.

1047
00:46:53,969 --> 00:46:57,370
But this one is partition
across all N devices.

1048
00:46:57,370 --> 00:47:00,709
Therefore, each device only
need to stor 12 divided by

1049
00:47:00,709 --> 00:47:06,030
N. This is basically
the memory cost of 01.

1050
00:47:06,030 --> 00:47:08,429
Similarly, you can do
exactly the same thing

1051
00:47:08,429 --> 00:47:10,229
that you keep piting
more and more,

1052
00:47:10,229 --> 00:47:11,989
and then you are
basically amortize

1053
00:47:11,989 --> 00:47:14,270
these two terms
across all devices.

1054
00:47:14,270 --> 00:47:16,469
That's why in sty three,

1055
00:47:16,469 --> 00:47:19,809
each device only need
to store 16 divided by

1056
00:47:19,809 --> 00:47:24,360
N. That's a big memory
saving compared to datapat.

1057
00:47:24,360 --> 00:47:27,810
Okay. And you also
know that basically,

1058
00:47:27,810 --> 00:47:29,889
the difference between
zero stage one, two,

1059
00:47:29,889 --> 00:47:32,289
three, they are essentially
like these two.

1060
00:47:32,289 --> 00:47:34,389
Okay. And the big part is,

1061
00:47:34,389 --> 00:47:37,409
like I said, operal state
because there's a 12.

1062
00:47:37,409 --> 00:47:41,689
Okay. Okay. Now, you
understand the math.

1063
00:47:41,689 --> 00:47:43,750
Let's go through each stage.

1064
00:47:43,750 --> 00:47:46,230
Let's try to understand
the communication.

1065
00:47:46,230 --> 00:47:51,379
Any question? Okay.
One more word.

1066
00:47:51,379 --> 00:47:53,439
So I petric if you do FSDP

1067
00:47:53,439 --> 00:47:56,139
that is fully shaded data
Barrels basically this one.

1068
00:47:56,139 --> 00:47:58,039
There will say
three. It's called

1069
00:47:58,039 --> 00:47:59,159
fully shaded data Barrel.

1070
00:47:59,159 --> 00:48:01,499
That fully means everything
is shared. Okay.

1071
00:48:01,499 --> 00:48:03,800
Okay? To help you remember.

1072
00:48:03,800 --> 00:48:07,080
Okay, let's see what
zero does, okay?

1073
00:48:07,080 --> 00:48:10,499
So the key idea of zero is to

1074
00:48:10,499 --> 00:48:14,659
replace or reduce with reduce
scatter and or gather.

1075
00:48:14,659 --> 00:48:17,699
Okay? This equation, help
you still remember, right?

1076
00:48:17,699 --> 00:48:19,940
I covered this in
collective communication.

1077
00:48:19,940 --> 00:48:22,399
Okay, let me change my pin.

1078
00:48:22,399 --> 00:48:25,219
Okay. So the left hand side

1079
00:48:25,219 --> 00:48:28,199
and the right hand side have
the same communicating cost.

1080
00:48:28,199 --> 00:48:30,559
But by doing this decomposition,

1081
00:48:30,559 --> 00:48:34,879
we can partition more replicate
answers and computations.

1082
00:48:34,879 --> 00:48:39,565
So here is Wanina data parison.

1083
00:48:39,565 --> 00:48:43,950
And it runs or reduce to
accumulate partial results,

1084
00:48:43,950 --> 00:48:45,669
partial gradients, I said

1085
00:48:45,669 --> 00:48:49,630
that other ways and opener
states are replicated.

1086
00:48:49,630 --> 00:48:51,349
That's why this graph
looks like this.

1087
00:48:51,349 --> 00:48:54,469
Okay? No, I already zoom
into the opener part.

1088
00:48:54,469 --> 00:48:56,509
Okay, I ignore the forward
and backward part.

1089
00:48:56,509 --> 00:48:58,769
Okay. So here we start with

1090
00:48:58,769 --> 00:49:01,789
a partial gradient because
we part in along a row.

1091
00:49:01,789 --> 00:49:03,649
And after the backward
recommendation,

1092
00:49:03,649 --> 00:49:05,930
each device has a
partial gradient.

1093
00:49:05,930 --> 00:49:08,069
But as I said,

1094
00:49:08,069 --> 00:49:10,010
in data parism we
replicate everything,

1095
00:49:10,010 --> 00:49:12,249
including open mender
states, right?

1096
00:49:12,249 --> 00:49:15,090
In order for the open
miner to compute,

1097
00:49:15,090 --> 00:49:20,179
we find that there's a there's
a restarting cost, right?

1098
00:49:20,179 --> 00:49:22,479
This opium need a full copy of

1099
00:49:22,479 --> 00:49:25,380
ingredients on HDS
because it's replicated.

1100
00:49:25,380 --> 00:49:27,220
But at the beginning,

1101
00:49:27,220 --> 00:49:29,000
we only have a partile.

1102
00:49:29,000 --> 00:49:31,120
Therefore, this is all reduced.

1103
00:49:31,120 --> 00:49:35,059
Okay? But that makes sense
This is data partism, okay?

1104
00:49:36,310 --> 00:49:41,189
So what is 02? In error two, um,

1105
00:49:41,189 --> 00:49:42,829
it basically replace or

1106
00:49:42,829 --> 00:49:46,789
reduce with reduced
scatter. Okay?

1107
00:49:46,789 --> 00:49:49,329
So the result of,

1108
00:49:49,329 --> 00:49:52,869
um, so basically here.

1109
00:49:52,910 --> 00:49:55,669
In R two, we said
that we are going to

1110
00:49:55,669 --> 00:49:57,889
partition all the
operar states, right?

1111
00:49:57,889 --> 00:50:02,529
Therefore, open states
this momentum like first,

1112
00:50:02,529 --> 00:50:03,149
second moment,

1113
00:50:03,149 --> 00:50:06,505
they partisonFolloing
in the batch dimension.

1114
00:50:06,505 --> 00:50:09,239
And here in S two,

1115
00:50:09,239 --> 00:50:13,299
it replaced or reduced
with reduced scatter here.

1116
00:50:13,299 --> 00:50:17,599
The result or reduced scatter
is still partitioned.

1117
00:50:17,599 --> 00:50:21,379
Because most of the later
competon are element wise,

1118
00:50:21,379 --> 00:50:24,380
the computation can be
done on partition format.

1119
00:50:24,380 --> 00:50:26,659
No problem. That is

1120
00:50:26,659 --> 00:50:28,920
open states are patina

1121
00:50:28,920 --> 00:50:31,680
and each partin only perform
its own competition.

1122
00:50:31,680 --> 00:50:34,020
It's fine. Okay.

1123
00:50:35,960 --> 00:50:38,739
But at some point,
we need to store

1124
00:50:38,739 --> 00:50:40,819
the gradits and open
states in parison format.

1125
00:50:40,819 --> 00:50:43,159
But because of this,

1126
00:50:43,159 --> 00:50:46,480
right, remember this factor.

1127
00:50:46,880 --> 00:50:49,239
We have a benefit
of this, right?

1128
00:50:49,239 --> 00:50:50,740
So the open states are basically

1129
00:50:50,740 --> 00:50:53,240
amortized across all devices.

1130
00:50:53,480 --> 00:50:58,059
Okay. Um, and this
reduce the memory cost.

1131
00:50:58,059 --> 00:51:00,140
And after always updated,

1132
00:51:00,140 --> 00:51:02,884
we can basically
use them or gather.

1133
00:51:02,884 --> 00:51:06,710
To get the replicate width
for the next iteration.

1134
00:51:06,710 --> 00:51:08,930
Then we proceed to competation.

1135
00:51:08,930 --> 00:51:12,289
In reality, what we do
is we do this per layer.

1136
00:51:12,289 --> 00:51:14,289
Every time we go to one layer,

1137
00:51:14,289 --> 00:51:18,869
we find that um uh sorry,
it's not per layer.

1138
00:51:18,869 --> 00:51:20,929
It's 03, three, we
do it per layer.

1139
00:51:20,929 --> 00:51:22,509
But for this in opera state,

1140
00:51:22,509 --> 00:51:23,930
that is once we proceed

1141
00:51:23,930 --> 00:51:25,370
into the operar
state competition,

1142
00:51:25,370 --> 00:51:27,069
we find anything that

1143
00:51:27,069 --> 00:51:29,770
is require recharging, we
will perform recharging.

1144
00:51:29,770 --> 00:51:32,250
If you do the um, analysis,

1145
00:51:32,250 --> 00:51:34,070
you'll find that essentially
we are basically

1146
00:51:34,070 --> 00:51:37,730
decomposing or reduce into
reduced schedule and getter.

1147
00:51:37,730 --> 00:51:41,584
If you compare the upper
graph and lower graph,

1148
00:51:41,584 --> 00:51:43,779
you'll find that
the communicating

1149
00:51:43,779 --> 00:51:45,279
cost is exactly the same,

1150
00:51:45,279 --> 00:51:46,899
because all reduce equals to

1151
00:51:46,899 --> 00:51:48,759
reduce scatter plus
or gather, right?

1152
00:51:48,759 --> 00:51:50,659
In this graph, you
do one reduce.

1153
00:51:50,659 --> 00:51:52,960
In this graph, you do reduce
scatter plus orgater.

1154
00:51:52,960 --> 00:51:54,659
But the memory is much better.

1155
00:51:54,659 --> 00:51:56,259
In the second graph, all

1156
00:51:56,259 --> 00:51:57,860
the operant states
they are partition.

1157
00:51:57,860 --> 00:52:00,300
But in the first graph,
they are replicated.

1158
00:52:00,300 --> 00:52:02,839
This is zero C two.

1159
00:52:04,760 --> 00:52:07,939
And now let's try to

1160
00:52:07,939 --> 00:52:11,320
see how we can
partition more stages,

1161
00:52:11,320 --> 00:52:14,019
more tensors to
get zero C three.

1162
00:52:14,019 --> 00:52:17,359
Here is still zero C two.

1163
00:52:17,359 --> 00:52:22,239
Basically, um, the widths
are still replicated, right?

1164
00:52:22,239 --> 00:52:26,319
I zero stage two, we
only partition the um,

1165
00:52:26,319 --> 00:52:28,599
gradients and the
operant states.

1166
00:52:28,599 --> 00:52:31,760
The widths are still replicated
because both forward

1167
00:52:31,760 --> 00:52:35,220
and backward requires a
full copy of the weights,

1168
00:52:35,220 --> 00:52:38,500
so we replicate the weights
to reduce the communication.

1169
00:52:38,500 --> 00:52:40,699
But if the model is too large,

1170
00:52:40,699 --> 00:52:42,659
we can also part in the width,

1171
00:52:42,659 --> 00:52:45,179
as well to get zero stage three.

1172
00:52:45,179 --> 00:52:47,119
As you can see, compared to

1173
00:52:47,119 --> 00:52:49,259
zero S two and zero stage three,

1174
00:52:49,259 --> 00:52:52,765
what do we do is we continue
to part in these weights.

1175
00:52:52,765 --> 00:52:56,329
Okay. Here in those two,
we replicate the width.

1176
00:52:56,329 --> 00:52:58,769
But I said, this width is
going to be pretty big.

1177
00:52:58,769 --> 00:53:01,349
For example, when you
GB really is big.

1178
00:53:01,349 --> 00:53:03,149
You also want to partition it.

1179
00:53:03,149 --> 00:53:04,729
And what's your partition it?

1180
00:53:04,729 --> 00:53:07,549
Because during the
spec competition, uh,

1181
00:53:07,549 --> 00:53:11,649
you require this one to
be a full copy like this.

1182
00:53:11,649 --> 00:53:14,749
So in order to recover
from here to here,

1183
00:53:14,749 --> 00:53:17,010
what do you do is basically
you perform all together.

1184
00:53:17,010 --> 00:53:18,929
And you can do this
layer by layer.

1185
00:53:18,929 --> 00:53:22,889
That is because your weight
is is partion now, right?

1186
00:53:22,889 --> 00:53:25,049
And you proceed to one
layer and you find

1187
00:53:25,049 --> 00:53:28,090
the layers with partition
across all devices.

1188
00:53:28,090 --> 00:53:29,610
But in order to
compute the forward,

1189
00:53:29,610 --> 00:53:32,209
you have to get the full
version of the width.

1190
00:53:32,209 --> 00:53:34,989
What do you do is you perform
an gather to get the width,

1191
00:53:34,989 --> 00:53:36,530
and you proceed to commutation.

1192
00:53:36,530 --> 00:53:37,850
And once you finish computation,

1193
00:53:37,850 --> 00:53:39,289
you proceed to the
next layer and you

1194
00:53:39,289 --> 00:53:40,990
throw away the or
gather results.

1195
00:53:40,990 --> 00:53:44,689
You still preserve one
chart on your device.

1196
00:53:44,930 --> 00:53:49,710
Okay. To summarize,
the difference

1197
00:53:49,710 --> 00:53:51,729
02-03 is basically a trade

1198
00:53:51,729 --> 00:53:53,810
off between memory
and communication.

1199
00:53:53,810 --> 00:53:56,489
So if you want to upgrade 02-03,

1200
00:53:56,489 --> 00:53:58,490
you are going to
see be more memory,

1201
00:53:58,490 --> 00:54:02,190
but you are going to pay
the cost of one or gather.

1202
00:54:02,190 --> 00:54:08,090
That's why the communicating
cost of 03 is 1.5 ordos.

1203
00:54:08,090 --> 00:54:11,350
Because reduce Gater
and orgaeryeach

1204
00:54:11,350 --> 00:54:15,489
basically half or
redos. Any question?

1205
00:54:16,560 --> 00:54:20,600
Okay. Let me consolidate
this understanding.

1206
00:54:20,600 --> 00:54:23,459
I think I explained
the difference between

1207
00:54:23,459 --> 00:54:27,159
all these four different
paten the memory cost,

1208
00:54:27,159 --> 00:54:28,880
now you understand it
is basically putting

1209
00:54:28,880 --> 00:54:31,679
one more into the upper
part of this one.

1210
00:54:31,679 --> 00:54:34,119
Now you understand
the community cost.

1211
00:54:34,119 --> 00:54:36,980
In dataparism community
cost is reduced.

1212
00:54:36,980 --> 00:54:39,999
In stage 12, the
commuting cost is

1213
00:54:39,999 --> 00:54:42,220
basically decomposing
this reduced

1214
00:54:42,220 --> 00:54:44,959
into reduced setter and
orgether, it's the same.

1215
00:54:44,959 --> 00:54:48,819
But if we continue
to evolve into 03,

1216
00:54:48,819 --> 00:54:51,240
we are introduced
one more orgether.

1217
00:54:51,240 --> 00:54:53,410
It's 1.5 reds.

1218
00:54:53,410 --> 00:54:57,539
Yeah. Reason to use one?

1219
00:54:57,539 --> 00:55:00,779
Very good question. 01 is
already like no one use that.

1220
00:55:00,779 --> 00:55:03,760
Yeah. Why? Because if you
compare these two numbers,

1221
00:55:03,760 --> 00:55:06,739
communication cost is the same.

1222
00:55:06,739 --> 00:55:09,160
But zero C two
saves more memory.

1223
00:55:09,160 --> 00:55:11,779
Yeah. So today,
people start with 02.

1224
00:55:11,779 --> 00:55:18,080
Yeah. Okay? Okay, this
is very important,

1225
00:55:18,080 --> 00:55:21,019
and I think that give you
a lot of reading on this,

1226
00:55:21,019 --> 00:55:22,499
and you should connect those

1227
00:55:22,499 --> 00:55:24,160
readings to this illustration,

1228
00:55:24,160 --> 00:55:25,980
and I think it will enhance
your understanding.

1229
00:55:25,980 --> 00:55:31,439
Okay. Next project is
basically match tender flow,

1230
00:55:31,439 --> 00:55:34,299
It is a system
developed by Google for

1231
00:55:34,299 --> 00:55:37,360
writing model parallel
tender flow programs.

1232
00:55:37,360 --> 00:55:40,480
And the key idea here
is basically specify

1233
00:55:40,480 --> 00:55:42,239
the Pagen strategy by

1234
00:55:42,239 --> 00:55:44,780
mapping tenser dimension
to match dimensions.

1235
00:55:44,780 --> 00:55:47,580
So here is a code example.

1236
00:55:47,580 --> 00:55:51,480
User need to use this special
API for writing programs.

1237
00:55:51,480 --> 00:55:53,719
And in the code, users give

1238
00:55:53,719 --> 00:55:55,159
every tensor
dimension a lime and

1239
00:55:55,159 --> 00:55:57,299
every match dimension
lime. So what is mash?

1240
00:55:57,299 --> 00:56:01,660
So Mash is basically a
matrix of devices arranged,

1241
00:56:01,660 --> 00:56:04,939
for example, arranged in
like two dimension, okay?

1242
00:56:04,939 --> 00:56:08,200
And, um the user
defined a mapping

1243
00:56:08,200 --> 00:56:09,600
that basically maps

1244
00:56:09,600 --> 00:56:11,480
the tinder dimension
to match dimension.

1245
00:56:11,480 --> 00:56:13,300
And the system will
basically generate

1246
00:56:13,300 --> 00:56:15,799
a paralyzed tinder flow graph

1247
00:56:15,799 --> 00:56:18,399
following the specialized
mapping, okay?

1248
00:56:18,399 --> 00:56:20,360
And this system requires

1249
00:56:20,360 --> 00:56:22,840
a rewriting of the
whole program.

1250
00:56:22,840 --> 00:56:25,319
So it is not very
user friendly and

1251
00:56:25,319 --> 00:56:28,720
later it was replaced by
another system, okay?

1252
00:56:29,190 --> 00:56:33,149
And this is GSMPDPMD is

1253
00:56:33,149 --> 00:56:34,369
basically considered as a

1254
00:56:34,369 --> 00:56:36,649
successor of the messenger flow.

1255
00:56:36,649 --> 00:56:40,190
It follows a very similar
compiler based approach,

1256
00:56:40,190 --> 00:56:42,369
but it generates
that idea, okay?

1257
00:56:42,369 --> 00:56:45,409
User do not need to
write the whole graph.

1258
00:56:45,409 --> 00:56:48,850
Instead, they just
insert some notations

1259
00:56:48,850 --> 00:56:50,670
to specify the parallel strategy

1260
00:56:50,670 --> 00:56:52,590
of certain important tensors.

1261
00:56:52,590 --> 00:56:54,910
And later the compiler
will basically propagate

1262
00:56:54,910 --> 00:56:57,109
the partntrategy
to the whole graph

1263
00:56:57,109 --> 00:56:59,889
and generate a partons
strategy for the entire graph.

1264
00:56:59,889 --> 00:57:02,629
There are some automation here.

1265
00:57:05,410 --> 00:57:09,009
Okay. And last, I
want to point out

1266
00:57:09,009 --> 00:57:12,710
that intraop and interop,
they should be combined.

1267
00:57:12,710 --> 00:57:15,070
Okay? Because there
are trade offs between

1268
00:57:15,070 --> 00:57:17,589
these two kinds of
parisms and we can

1269
00:57:17,589 --> 00:57:20,069
combine them to
exploit the trade

1270
00:57:20,069 --> 00:57:22,950
off and achieve the
optimal performance.

1271
00:57:22,950 --> 00:57:25,249
Okay? Here is the
computer graph, right?

1272
00:57:25,249 --> 00:57:29,490
And, which we slice
into four stages.

1273
00:57:29,490 --> 00:57:33,569
Okay. And instead of assigning
a stage on one device,

1274
00:57:33,569 --> 00:57:38,499
we can basically assign each
stage to a to a device mesh.

1275
00:57:38,499 --> 00:57:42,279
A group of devices.
And, um, for example,

1276
00:57:42,279 --> 00:57:45,839
we can add four GPUs to
ask you the first stage,

1277
00:57:45,839 --> 00:57:47,740
two GBs second stage,

1278
00:57:47,740 --> 00:57:49,339
and six GB the third stage,

1279
00:57:49,339 --> 00:57:51,000
and two GB first stage.

1280
00:57:51,000 --> 00:57:57,509
So why would do this? Reason is

1281
00:57:57,509 --> 00:57:58,729
pretty simple
because like I said,

1282
00:57:58,729 --> 00:58:00,309
in Pipeline parism
you want to make sure

1283
00:58:00,309 --> 00:58:02,749
the ascuon time of which
stage is the same.

1284
00:58:02,749 --> 00:58:07,189
So model, if your neuralnetwork
graph is heterogeneous,

1285
00:58:07,189 --> 00:58:09,149
that it's not like
equivalent layers,

1286
00:58:09,149 --> 00:58:10,950
then you have to align
different devices

1287
00:58:10,950 --> 00:58:13,209
for different parts
of the stage, right?

1288
00:58:13,209 --> 00:58:15,469
Okay. And once you
align this you

1289
00:58:15,469 --> 00:58:18,269
notice that actually for
that small stage blue stage,

1290
00:58:18,269 --> 00:58:19,629
we are asking them in

1291
00:58:19,629 --> 00:58:23,470
four devices and how to ask
you a stage in four devices.

1292
00:58:23,470 --> 00:58:26,829
That is where you how to
also do intraparis, right?

1293
00:58:26,829 --> 00:58:30,400
So so basically inside
of the G stage,

1294
00:58:30,400 --> 00:58:31,740
you hold multiple devices.

1295
00:58:31,740 --> 00:58:34,119
And what you do
is basically, um,

1296
00:58:34,119 --> 00:58:35,579
you also introduce some sort

1297
00:58:35,579 --> 00:58:37,799
of intraparism
inside of G stage.

1298
00:58:37,799 --> 00:58:39,400
And this method has actually

1299
00:58:39,400 --> 00:58:41,820
achieved the best
scalability, okay?

1300
00:58:41,820 --> 00:58:44,900
The way you skill this
deep neuronal computation

1301
00:58:44,900 --> 00:58:47,759
beyond say 1,000 GPU
is basically this.

1302
00:58:47,759 --> 00:58:49,619
Okay. And I think today,

1303
00:58:49,619 --> 00:58:51,039
if you want to train
a neurotro on say,

1304
00:58:51,039 --> 00:58:52,499
ten k or 20 GPU,

1305
00:58:52,499 --> 00:58:54,160
you have to use this
kind of strategy.

1306
00:58:54,160 --> 00:58:57,839
Okay. Any question?

1307
00:58:59,909 --> 00:59:03,729
Okay. So to summarize, okay, um,

1308
00:59:03,729 --> 00:59:06,769
we can paralyze a single
operator by exploiting,

1309
00:59:06,769 --> 00:59:08,529
uh, its internal partism.

1310
00:59:08,529 --> 00:59:12,149
To do this, for Hu
computation graph,

1311
00:59:12,149 --> 00:59:14,909
we need to choose
strategies for all nodes in

1312
00:59:14,909 --> 00:59:18,409
the graph to minimize the node
cost and edge cost. Okay?

1313
00:59:18,409 --> 00:59:21,409
Um, and last, this
interrupt and the interrupt

1314
00:59:21,409 --> 00:59:24,489
can be combined to achieve
the best performance, okay?

1315
00:59:24,489 --> 00:59:27,049
And in this part,

1316
00:59:27,049 --> 00:59:30,329
I think we primarily
focus on partism.

1317
00:59:30,329 --> 00:59:32,290
But in reality, what
you do is basically,

1318
00:59:32,290 --> 00:59:33,729
you also want to combine this

1319
00:59:33,729 --> 00:59:36,089
with what I covered in
my previous lecture.

1320
00:59:36,089 --> 00:59:38,349
Okay? You want to enable
this partism while

1321
00:59:38,349 --> 00:59:40,869
you also enable some sort
of memory opposition.

1322
00:59:40,869 --> 00:59:42,709
You turn on grading
checkpointing,

1323
00:59:42,709 --> 00:59:44,669
you turn on swapping.

1324
00:59:44,669 --> 00:59:46,489
You also turn on quantization,

1325
00:59:46,489 --> 00:59:49,289
specification and some low
rank approximation in order to

1326
00:59:49,289 --> 00:59:52,329
fit your model into limited
lumbo devices, okay?

1327
00:59:52,329 --> 00:59:53,829
Because TPU is so expensive.

1328
00:59:53,829 --> 00:59:57,189
You want to treat your model
with a lot of its cost.

1329
00:59:57,230 --> 01:00:02,050
Okay, so this is basically
the end of, uh intra partism.

1330
01:00:02,050 --> 01:00:09,209
Any question? Yeah.
Okay. Uh huh.

1331
01:00:11,890 --> 01:00:14,189
Both. Of course, both.

1332
01:00:14,189 --> 01:00:19,929
Yeah. Because inside the
mess you are doing intra op.

1333
01:00:24,010 --> 01:00:26,509
You have a me, yes, but

1334
01:00:26,509 --> 01:00:30,070
minic is just like you have
a super high bandwidth.

1335
01:00:30,070 --> 01:00:31,949
You still want to
minimize the lumbar,

1336
01:00:31,949 --> 01:00:34,689
the volume of the message
you communicate between An.

1337
01:00:34,689 --> 01:00:39,069
Does that make sense?
Yeah. You does

1338
01:00:39,069 --> 01:00:40,490
not mean that you have infinite

1339
01:00:40,490 --> 01:00:42,729
the communication has no cost.

1340
01:00:42,729 --> 01:00:45,389
Yeah. If you are not careful
on designing the strate,

1341
01:00:45,389 --> 01:00:47,129
you are still going to be small.

1342
01:00:47,129 --> 01:00:53,429
Okay. Okay, then let's

1343
01:00:53,429 --> 01:00:56,290
move on to the final
section, auto part edition.

1344
01:00:56,290 --> 01:00:58,629
Okay. Um, I think

1345
01:00:58,629 --> 01:01:00,570
the idea of Autopart edition

1346
01:01:00,570 --> 01:01:01,890
is very very similar
to compiler.

1347
01:01:01,890 --> 01:01:04,369
So, you know, computer
scientists they always want

1348
01:01:04,369 --> 01:01:07,149
to automate on workflow, right?

1349
01:01:07,149 --> 01:01:09,054
Designing this
kind of, like, um,

1350
01:01:09,054 --> 01:01:11,880
Uh, paras reality is very
tedious and very difficult.

1351
01:01:11,880 --> 01:01:13,379
You have to enumerate,
like I said,

1352
01:01:13,379 --> 01:01:14,759
and you count cost.

1353
01:01:14,759 --> 01:01:18,619
Okay? So the idea is why don't
we actually automate them?

1354
01:01:18,619 --> 01:01:21,559
In fact, automation
makes lot of sense

1355
01:01:21,559 --> 01:01:24,259
because I think in
basically past few years,

1356
01:01:24,259 --> 01:01:26,719
you can see people develop
many different models.

1357
01:01:26,719 --> 01:01:29,859
Okay. Even today, models are
still evolving a little bit.

1358
01:01:29,859 --> 01:01:30,939
For example, you evolving from

1359
01:01:30,939 --> 01:01:34,814
GBR rmers into MOE into
something like MLA, right?

1360
01:01:34,814 --> 01:01:36,349
And meanwhile, you have

1361
01:01:36,349 --> 01:01:38,209
different sorts of
pisms like data.

1362
01:01:38,209 --> 01:01:40,490
Operator, zero Pipeline Pism.

1363
01:01:40,490 --> 01:01:42,309
And depending on which
model you design

1364
01:01:42,309 --> 01:01:44,609
and which partism we
are going to choose?

1365
01:01:44,609 --> 01:01:47,789
People roughly design a
system. Okay, for it.

1366
01:01:47,789 --> 01:01:50,229
Okay. And you can see, if

1367
01:01:50,229 --> 01:01:52,650
we continue to skill the
number of models and pisms,

1368
01:01:52,650 --> 01:01:54,750
we are going to design
one more systems.

1369
01:01:54,750 --> 01:01:56,409
The idea is basically,
why don't we just

1370
01:01:56,409 --> 01:01:58,110
actually reuse some
of the systems,

1371
01:01:58,110 --> 01:02:00,169
right? The compiler philosophy.

1372
01:02:00,169 --> 01:02:01,870
Why don't we reuse
some component,

1373
01:02:01,870 --> 01:02:03,670
so we can automate this process.

1374
01:02:03,670 --> 01:02:06,619
Okay. Yeah.

1375
01:02:06,619 --> 01:02:09,399
And also it solves the
problem that facing many,

1376
01:02:09,399 --> 01:02:10,479
many developers, which are

1377
01:02:10,479 --> 01:02:13,220
not parallel system developers.

1378
01:02:13,220 --> 01:02:15,039
That is, which one is

1379
01:02:15,039 --> 01:02:17,779
which podism is the right
fit for my model, right?

1380
01:02:17,779 --> 01:02:22,399
Okay. So if you forget
about exist systems,

1381
01:02:22,399 --> 01:02:26,219
so what if I tell you that
there's such a system am?

1382
01:02:26,219 --> 01:02:29,560
Given arbitrary model
and class config,

1383
01:02:29,560 --> 01:02:31,419
I will automatically figure out

1384
01:02:31,419 --> 01:02:34,440
the best suitable
parallel strategy

1385
01:02:34,440 --> 01:02:37,139
that maximize the training
performance, for example.

1386
01:02:37,139 --> 01:02:38,899
And basically we write down into

1387
01:02:38,899 --> 01:02:40,859
this kind of opening
equation, right?

1388
01:02:40,859 --> 01:02:43,079
We want to maximize
the performance,

1389
01:02:43,639 --> 01:02:45,639
where we want to figure out the

1390
01:02:45,639 --> 01:02:46,979
best strategy and the strategy

1391
01:02:46,979 --> 01:02:52,439
is strategy shoot as a
combination of inter inter parts.

1392
01:02:52,439 --> 01:02:55,199
Okay? And this is

1393
01:02:55,199 --> 01:02:56,660
basically the problem
that autoparts

1394
01:02:56,660 --> 01:02:58,239
trying to solve, okay?

1395
01:02:58,239 --> 01:03:00,119
So how can we solve
this problem?

1396
01:03:00,119 --> 01:03:02,940
In the previous content
of the lecture,

1397
01:03:02,940 --> 01:03:04,579
we have basically established

1398
01:03:04,579 --> 01:03:06,779
some mathematical
understanding of how to

1399
01:03:06,779 --> 01:03:09,319
analyze the cost of
inter and intra, right?

1400
01:03:09,319 --> 01:03:10,979
Now, what we do is
basically we write down

1401
01:03:10,979 --> 01:03:12,939
the cost into one equation
and we try to solve it.

1402
01:03:12,939 --> 01:03:17,959
Okay? So remember, our model
is a comio graph, right?

1403
01:03:17,959 --> 01:03:19,880
And such as this MLP.

1404
01:03:19,880 --> 01:03:23,219
And our class in most
setup is basically

1405
01:03:23,219 --> 01:03:25,599
a cluster of nodes
where each node

1406
01:03:25,599 --> 01:03:28,139
is installed with a few GPUs.

1407
01:03:28,139 --> 01:03:30,359
And like I said, inside of GPU,

1408
01:03:30,359 --> 01:03:31,559
you have manik which

1409
01:03:31,559 --> 01:03:33,099
you have a decent
amount of bandwidth.

1410
01:03:33,099 --> 01:03:34,459
But between nodes, you have

1411
01:03:34,459 --> 01:03:36,825
just a little bandwidth
to communicate.

1412
01:03:36,825 --> 01:03:39,649
And we try to figure out a
strategy where we put this,

1413
01:03:39,649 --> 01:03:41,369
we partition the
graph in a way that

1414
01:03:41,369 --> 01:03:45,349
run in the most
efficient way. Okay.

1415
01:03:45,349 --> 01:03:47,669
And as you can imagine, um,

1416
01:03:47,669 --> 01:03:50,249
I already talked about
so many possibilities

1417
01:03:50,249 --> 01:03:52,969
how to partition
operators and stages.

1418
01:03:52,969 --> 01:03:55,729
And apparently the
sur space is huge.

1419
01:03:55,729 --> 01:03:59,289
To give you a sense how
large the sur space is, um,

1420
01:03:59,289 --> 01:04:02,769
the number of operations in
real model is between 110 k,

1421
01:04:02,769 --> 01:04:04,529
right? A lot of mad mods.

1422
01:04:04,529 --> 01:04:07,229
Okay. Number of operator types

1423
01:04:07,229 --> 01:04:10,449
is basically 2,200 and if
you want to model them more.

1424
01:04:10,449 --> 01:04:12,169
But I already said

1425
01:04:12,169 --> 01:04:14,389
that the most important
operator space is Mm.

1426
01:04:14,389 --> 01:04:19,569
Okay. And the
classroom size, right?

1427
01:04:19,569 --> 01:04:21,929
You can actually models up
to 1,000 GPS or even more.

1428
01:04:21,929 --> 01:04:24,749
Okay. So you have to basically
iterate on this space.

1429
01:04:24,749 --> 01:04:28,609
Okay. So in the past,

1430
01:04:28,609 --> 01:04:30,070
researchers in the area,

1431
01:04:30,070 --> 01:04:32,889
they basically developed
three categories of

1432
01:04:32,889 --> 01:04:36,189
methods that can basically
automate this kind of pardon.

1433
01:04:36,189 --> 01:04:38,049
The first one is search based.

1434
01:04:38,049 --> 01:04:40,419
This one is very
easy to understand.

1435
01:04:40,419 --> 01:04:42,430
You just keep enumerating.

1436
01:04:42,430 --> 01:04:45,830
Okay. And once you
have a strategy,

1437
01:04:45,830 --> 01:04:48,809
you run um on

1438
01:04:48,809 --> 01:04:51,129
your classroom and you
get a run time, right?

1439
01:04:51,129 --> 01:04:54,990
And once you have a run time,
you basically try to search

1440
01:04:54,990 --> 01:04:58,849
around those areas
where you think are

1441
01:04:58,849 --> 01:05:00,749
more promising and
you return the

1442
01:05:00,749 --> 01:05:03,149
best one after your
budget was exhausted.

1443
01:05:03,149 --> 01:05:06,329
Okay? And that's indeed one
of the most effective way.

1444
01:05:06,329 --> 01:05:07,709
That most companies are doing.

1445
01:05:07,709 --> 01:05:09,669
They're basically searching.

1446
01:05:09,730 --> 01:05:12,870
The second way, like I
said, similar recipe,

1447
01:05:12,870 --> 01:05:15,630
learning based, merge
learning for systems.

1448
01:05:15,630 --> 01:05:19,789
And we basically formulate
it as a learning problem

1449
01:05:19,789 --> 01:05:21,729
and we try to train a model

1450
01:05:21,729 --> 01:05:24,290
that can predict what's
the best strategy.

1451
01:05:24,290 --> 01:05:26,069
And it's slightly better than

1452
01:05:26,069 --> 01:05:28,009
search base because
in search base,

1453
01:05:28,009 --> 01:05:29,589
you're basically
doing some kind of

1454
01:05:29,589 --> 01:05:31,970
a brute force searching.

1455
01:05:31,970 --> 01:05:33,369
But in learning
based, what you do

1456
01:05:33,369 --> 01:05:36,190
is after you collecting
of your data points,

1457
01:05:36,190 --> 01:05:37,689
you are able to train a model.

1458
01:05:37,689 --> 01:05:39,829
Hopefully, this data points

1459
01:05:39,829 --> 01:05:41,590
some trends that your
model can capture.

1460
01:05:41,590 --> 01:05:42,829
Therefore, it can basically

1461
01:05:42,829 --> 01:05:44,709
help you accelerate your search.

1462
01:05:44,709 --> 01:05:48,910
Okay. And the third method
is basically opienhmbs.

1463
01:05:48,910 --> 01:05:50,629
For Opiend hin Base,

1464
01:05:50,629 --> 01:05:52,289
essentially the same as search,

1465
01:05:52,289 --> 01:05:54,650
but you are introducing
some smarter

1466
01:05:54,650 --> 01:05:57,669
like open ended servers that
can accelerate your search.

1467
01:05:57,669 --> 01:06:01,309
Okay. What I'm going to do
next is I'm going to very

1468
01:06:01,309 --> 01:06:05,229
quickly go through the second
one and the third one.

1469
01:06:05,229 --> 01:06:11,449
Okay? So the second one,

1470
01:06:11,449 --> 01:06:14,949
I think one of the most
famous work is this one,

1471
01:06:14,949 --> 01:06:17,550
it's called device
placement organization,

1472
01:06:17,550 --> 01:06:19,049
still from Google Brain.

1473
01:06:19,049 --> 01:06:21,709
And the first author now
is faculty at Stanford.

1474
01:06:21,709 --> 01:06:23,929
If you're interested
in applying for BC,

1475
01:06:23,929 --> 01:06:25,789
she's pretty good. Okay.

1476
01:06:25,789 --> 01:06:30,769
And um um, in this
class, basically,

1477
01:06:30,769 --> 01:06:33,069
people use learning
based methods to solve

1478
01:06:33,069 --> 01:06:36,049
this basically, uh
discrete ogenation.

1479
01:06:36,049 --> 01:06:38,369
One of the most
representative approach

1480
01:06:38,369 --> 01:06:40,230
is basically this RO,

1481
01:06:40,230 --> 01:06:45,509
um, Clock RL also called
device placement organization.

1482
01:06:45,509 --> 01:06:49,469
So on the left, we see a
tensor flow, continuo graph,

1483
01:06:49,469 --> 01:06:53,730
visualized interparty the
approach basically aims

1484
01:06:53,730 --> 01:06:56,770
to design assign a
device placement

1485
01:06:56,770 --> 01:06:58,049
for each node in the graph.

1486
01:06:58,049 --> 01:06:59,769
That is giving each node

1487
01:06:59,769 --> 01:07:01,869
a device color in order to

1488
01:07:01,869 --> 01:07:04,749
maximize the distributed
execution performance.

1489
01:07:04,749 --> 01:07:07,050
And here, the approach

1490
01:07:07,050 --> 01:07:10,489
basically works only on the
space of interoperatism,

1491
01:07:10,489 --> 01:07:12,370
as you can imagine,
because it actually

1492
01:07:12,370 --> 01:07:14,789
only assigns one
color for each node.

1493
01:07:14,789 --> 01:07:16,369
That's why it only captures

1494
01:07:16,369 --> 01:07:19,444
the possibility in
interrupt not interrupt.

1495
01:07:19,444 --> 01:07:21,799
And given this space,
it's basically use

1496
01:07:21,799 --> 01:07:23,379
a machinery model to

1497
01:07:23,379 --> 01:07:26,599
predict the placement of
each node in the graph.

1498
01:07:26,599 --> 01:07:28,339
Well, sometimes, especially in

1499
01:07:28,339 --> 01:07:29,759
the early phase of training,

1500
01:07:29,759 --> 01:07:31,779
the machinary model
can go wrong.

1501
01:07:31,779 --> 01:07:34,239
I basically evaluated
this prediction on

1502
01:07:34,239 --> 01:07:38,070
real cluster to get its real
runtime. Uh, performance.

1503
01:07:38,070 --> 01:07:41,089
I need to then fits the
real random data back to

1504
01:07:41,089 --> 01:07:42,609
the predictive model and

1505
01:07:42,609 --> 01:07:44,970
update its performance
using policy gradients.

1506
01:07:44,970 --> 01:07:47,129
So policy gradient is
basically the one we use to

1507
01:07:47,129 --> 01:07:50,249
train reimburse
learning models today.

1508
01:07:50,249 --> 01:07:53,469
And um PPO, DPO GRPO,

1509
01:07:53,469 --> 01:07:54,789
they are all policy gradients.

1510
01:07:54,789 --> 01:07:57,790
Okay. I need to repeat

1511
01:07:57,790 --> 01:08:00,050
this policy gradient
update process

1512
01:08:00,050 --> 01:08:01,609
until a good enough
model is fun.

1513
01:08:01,609 --> 01:08:03,249
Good enough strategy is fine.

1514
01:08:03,249 --> 01:08:06,829
Okay? Like this.

1515
01:08:08,250 --> 01:08:10,530
Okay. And in the circles

1516
01:08:10,530 --> 01:08:12,709
the machinery model is
basically materialized as

1517
01:08:12,709 --> 01:08:16,469
a simple recurrent neural
network shown on this slide.

1518
01:08:16,469 --> 01:08:19,289
Okay. This arm basically
works as follows.

1519
01:08:19,289 --> 01:08:21,990
So it first, nearize

1520
01:08:21,990 --> 01:08:24,844
the cotero graph as
a sequence of nodes.

1521
01:08:24,844 --> 01:08:27,239
And then each node
corresponds to one operator.

1522
01:08:27,239 --> 01:08:29,899
That is nariz entire
graph as a sequence,

1523
01:08:29,899 --> 01:08:32,899
like a sentence,
sting language model.

1524
01:08:32,899 --> 01:08:34,959
Okay. And I give you a name,

1525
01:08:34,959 --> 01:08:37,579
for example, op one up to
op 100, blah, blah, blah.

1526
01:08:37,579 --> 01:08:39,499
Okay. And then it will embed

1527
01:08:39,499 --> 01:08:41,639
each node as a feature
actor embedding.

1528
01:08:41,639 --> 01:08:44,119
So it's a typical recipe
of train language models,

1529
01:08:44,119 --> 01:08:46,299
but it's for training
commit graph.

1530
01:08:46,299 --> 01:08:49,259
Okay. And then you basically
once you get the embedding,

1531
01:08:49,259 --> 01:08:50,699
you are able to fit
this into a sequence

1532
01:08:50,699 --> 01:08:51,979
to sequence prediction
model, right?

1533
01:08:51,979 --> 01:08:53,679
You basically predict
the placement

1534
01:08:53,679 --> 01:08:55,739
of each node. That's it.

1535
01:08:55,739 --> 01:08:58,519
Okay. Very simple.

1536
01:08:58,790 --> 01:09:00,889
Another objective is a

1537
01:09:00,889 --> 01:09:02,389
typical reinforced
learning objective,

1538
01:09:02,389 --> 01:09:03,669
where you try to maximize

1539
01:09:03,669 --> 01:09:06,089
the expected reward
conditional on your placement.

1540
01:09:06,089 --> 01:09:08,269
And this objective can

1541
01:09:08,269 --> 01:09:09,949
be optimized using
policy ingredient.

1542
01:09:09,949 --> 01:09:16,049
Okay, so how does this perform?

1543
01:09:16,049 --> 01:09:18,169
According to the paper, um,

1544
01:09:18,169 --> 01:09:21,329
it indeed return some
placement strategies that are

1545
01:09:21,329 --> 01:09:23,329
20 to 30% better

1546
01:09:23,329 --> 01:09:26,549
than the best available
ones designed by experts.

1547
01:09:26,549 --> 01:09:28,609
And if you take a look,

1548
01:09:28,609 --> 01:09:31,250
um, at the visualization,

1549
01:09:31,250 --> 01:09:34,149
which is a strategy
found by this method for

1550
01:09:34,149 --> 01:09:37,529
the very famous inception
WT model by Google,

1551
01:09:37,529 --> 01:09:40,289
um, I would say

1552
01:09:40,289 --> 01:09:43,189
it's certainly something that
cannot be found by human.

1553
01:09:43,189 --> 01:09:45,369
Okay? I don't think
a human is very

1554
01:09:45,369 --> 01:09:48,049
intuitive to design this
kind of like a placement,

1555
01:09:48,049 --> 01:09:50,189
right? Yeah, it's weird.

1556
01:09:50,189 --> 01:09:52,569
But like I said, this
method is not very

1557
01:09:52,569 --> 01:09:55,009
resource efficient
because it has to

1558
01:09:55,009 --> 01:09:57,969
use the entire cluster

1559
01:09:57,969 --> 01:09:59,869
to perform a real evaluation

1560
01:09:59,869 --> 01:10:01,949
for one given
placement strategy.

1561
01:10:01,949 --> 01:10:03,769
That takes a lot of resources.

1562
01:10:03,769 --> 01:10:05,889
And as you probably
already know,

1563
01:10:05,889 --> 01:10:07,969
O is not very efficient as well.

1564
01:10:07,969 --> 01:10:09,849
Yeah, it's not a very
efficient learning strategy,

1565
01:10:09,849 --> 01:10:11,749
so it takes a lot of iterations.

1566
01:10:11,749 --> 01:10:13,549
That's why this method is only

1567
01:10:13,549 --> 01:10:15,489
populated inside of Google.

1568
01:10:15,489 --> 01:10:19,329
I think outside of Google,
nobody uses this one.

1569
01:10:20,770 --> 01:10:24,209
Let's move on to second
classroom method,

1570
01:10:24,209 --> 01:10:26,764
basically optimal
inst based method.

1571
01:10:26,764 --> 01:10:28,879
So um, since

1572
01:10:28,879 --> 01:10:32,619
this discrete strategy
space is extremely complex.

1573
01:10:32,619 --> 01:10:34,179
So the rationale behind

1574
01:10:34,179 --> 01:10:37,039
this option based method
is basically, um,

1575
01:10:37,039 --> 01:10:39,019
we try to leverage
some structures of

1576
01:10:39,019 --> 01:10:40,439
the parting strategy to reduce

1577
01:10:40,439 --> 01:10:42,979
the space until the
problem size is,

1578
01:10:42,979 --> 01:10:47,299
um, doable, using
existing opiors, okay?

1579
01:10:47,299 --> 01:10:48,959
For example, IOP server or

1580
01:10:48,959 --> 01:10:51,219
like DP over, this
kind of thing, okay?

1581
01:10:51,219 --> 01:10:55,319
And the hope is that this
space reduction, um, uh,

1582
01:10:55,319 --> 01:10:58,580
heuristic does not actually
lose any optimality,

1583
01:10:58,580 --> 01:11:01,019
which sometimes
could be true, okay?

1584
01:11:01,019 --> 01:11:04,104
Um, let me introduce this
method and this is my paper.

1585
01:11:04,104 --> 01:11:06,409
Okay. And I think
this is a good paper.

1586
01:11:06,409 --> 01:11:09,659
Yeah, I use this paper to
get a faculty at USD. Yeah.

1587
01:11:09,659 --> 01:11:12,399
Okay. Um, to begin,

1588
01:11:12,399 --> 01:11:15,879
um, recall that at the
beginning of this part,

1589
01:11:15,879 --> 01:11:18,919
right, how we explained a lot in

1590
01:11:18,919 --> 01:11:21,179
interop and interop parts

1591
01:11:21,179 --> 01:11:23,620
and their unique
characteristics.

1592
01:11:23,620 --> 01:11:27,339
So basically, basically
interop communication,

1593
01:11:27,339 --> 01:11:30,659
uh communicate less, but
has device id, right?

1594
01:11:30,659 --> 01:11:34,179
Bubble. But the interop
actually communicate more,

1595
01:11:34,179 --> 01:11:36,820
um, but it has a higher unition.

1596
01:11:36,820 --> 01:11:40,539
Okay. So given a communal
graph and a cluster,

1597
01:11:40,539 --> 01:11:44,099
so how can we find
the good strategy

1598
01:11:44,099 --> 01:11:46,339
from the combined space of Inter

1599
01:11:46,339 --> 01:11:49,359
and interop partism?
That's our problem.

1600
01:11:49,970 --> 01:11:53,269
So we basically design
this algorithm called Opa.

1601
01:11:53,269 --> 01:11:55,789
And this Opa basically
leverage the fact that

1602
01:11:55,789 --> 01:11:59,469
many today cluster has a
corresponding structure.

1603
01:11:59,469 --> 01:12:02,749
That is closely located
devices in one node,

1604
01:12:02,749 --> 01:12:04,289
they can communicate with Minik

1605
01:12:04,289 --> 01:12:05,589
which is high bandwidth, right?

1606
01:12:05,589 --> 01:12:08,269
And distant devices
across different nodes,

1607
01:12:08,269 --> 01:12:09,589
they can communicate with they

1608
01:12:09,589 --> 01:12:11,389
can only communicate
with low bandwidth.

1609
01:12:11,389 --> 01:12:17,349
Okay? And given this
kind of like, um, um,

1610
01:12:17,349 --> 01:12:19,670
given this kind of
characteristics

1611
01:12:19,670 --> 01:12:22,649
and you can basically
find alignment that is,

1612
01:12:23,260 --> 01:12:25,900
in the upper part
of the interperism,

1613
01:12:25,900 --> 01:12:28,499
they basically favor, uh

1614
01:12:28,700 --> 01:12:31,439
they favor some communication

1615
01:12:31,439 --> 01:12:32,719
bandwidths that is not so high,

1616
01:12:32,719 --> 01:12:34,439
because it communicate
pretty less.

1617
01:12:34,439 --> 01:12:37,219
So basically you can map this
interopism across nodes.

1618
01:12:37,219 --> 01:12:38,939
You let most of the
parism happening

1619
01:12:38,939 --> 01:12:42,099
between TPUs distribute
across different nodes.

1620
01:12:42,099 --> 01:12:44,119
But for interperism,
like I said,

1621
01:12:44,119 --> 01:12:45,539
they need a lot of bandwis to

1622
01:12:45,539 --> 01:12:47,319
communicate in order
to be efficient.

1623
01:12:47,319 --> 01:12:48,819
So you have to map this kind of

1624
01:12:48,819 --> 01:12:50,979
inter parism into veining.

1625
01:12:50,979 --> 01:12:53,119
So you only allow
those communication

1626
01:12:53,119 --> 01:12:54,579
happening into aminis.

1627
01:12:54,579 --> 01:12:56,959
And this basically give
you alignment like this.

1628
01:12:56,959 --> 01:12:59,849
So how about you just part
in working in a way that

1629
01:12:59,849 --> 01:13:03,999
all the inter parism
happening across link.

1630
01:13:03,999 --> 01:13:05,979
But all the
interparism basically

1631
01:13:05,979 --> 01:13:08,459
happen across different nodes.

1632
01:13:08,459 --> 01:13:10,460
This is a strong heuristics

1633
01:13:10,460 --> 01:13:12,959
because now you don't have to
search over all the spaces.

1634
01:13:12,959 --> 01:13:15,754
You just need to search
for each subproblem.

1635
01:13:15,754 --> 01:13:18,189
Does this into make sense?

1636
01:13:18,189 --> 01:13:19,989
Okay. Cool. Yeah, this may work.

1637
01:13:19,989 --> 01:13:21,269
Yeah, I hope that makes sense.

1638
01:13:21,269 --> 01:13:25,069
Okay um the rationale

1639
01:13:25,069 --> 01:13:26,709
helps Opa to basically

1640
01:13:26,709 --> 01:13:28,869
decouple and reorganize
the search space.

1641
01:13:28,869 --> 01:13:32,029
Basically, this Opa generates
a full model parallel.

1642
01:13:32,029 --> 01:13:36,909
Asking strategy by hierarchically
specifying interop

1643
01:13:36,909 --> 01:13:40,089
and inter parallel strategy
respectively at each level.

1644
01:13:40,089 --> 01:13:42,889
More specifically, this
Opa will basically,

1645
01:13:42,889 --> 01:13:46,169
uh search for inter
parallel plan at

1646
01:13:46,169 --> 01:13:47,669
the first level that is how

1647
01:13:47,669 --> 01:13:49,689
to cut the graph into
different stages.

1648
01:13:49,689 --> 01:13:52,269
And then at the next level, uh,

1649
01:13:52,269 --> 01:13:55,389
upper derives the best
interop parallel plan

1650
01:13:55,389 --> 01:13:58,889
for each stage of the
interop parallel plan.

1651
01:13:58,889 --> 01:14:02,169
And by designing
an algorithm that

1652
01:14:02,169 --> 01:14:05,970
are basically both locally
optimal for each subproblem.

1653
01:14:05,970 --> 01:14:08,290
But we don't guarantee
global optimality.

1654
01:14:08,290 --> 01:14:09,569
We are just designing algorithms

1655
01:14:09,569 --> 01:14:10,969
that can guarantee like we are

1656
01:14:10,969 --> 01:14:12,409
optimal at the first level

1657
01:14:12,409 --> 01:14:14,029
and optimal at the second level,

1658
01:14:14,029 --> 01:14:18,229
and we hope the yielded
strategy is good enough,

1659
01:14:18,229 --> 01:14:19,569
because we already leverage

1660
01:14:19,569 --> 01:14:23,449
the uretical mapping the
binaries between parisms. Okay.

1661
01:14:23,530 --> 01:14:27,549
That's a Halla idea.
Okay. And this slide is

1662
01:14:27,549 --> 01:14:31,249
basically give an overview of
ogenation processes, okay?

1663
01:14:31,249 --> 01:14:36,469
The inputs, computational
graph and computer cluster.

1664
01:14:36,469 --> 01:14:38,609
And this OPA goes through

1665
01:14:38,609 --> 01:14:41,210
two passes to do this openation.

1666
01:14:41,210 --> 01:14:43,349
So the first interrupt pass,

1667
01:14:43,349 --> 01:14:47,289
it basically finds the best
interrupt part strategy

1668
01:14:47,289 --> 01:14:49,309
with the dynamic
programming algorithm,

1669
01:14:49,309 --> 01:14:50,784
which I explain later.

1670
01:14:50,784 --> 01:14:54,159
And then the second
interrup path will find

1671
01:14:54,159 --> 01:14:56,259
the best inter parallel strategy

1672
01:14:56,259 --> 01:14:58,839
with integer a
programming organization.

1673
01:14:58,839 --> 01:15:01,480
Okay? And this optim
is hierarchical,

1674
01:15:01,480 --> 01:15:03,379
which means that the higher

1675
01:15:03,379 --> 01:15:06,199
level interrupt paths will cause

1676
01:15:06,199 --> 01:15:08,139
lower level interruple paths

1677
01:15:08,139 --> 01:15:10,179
multiple times to make

1678
01:15:10,179 --> 01:15:12,099
a decision based on
feedback format.

1679
01:15:12,099 --> 01:15:14,519
That is optimize at
two levels, okay?

1680
01:15:14,519 --> 01:15:17,139
And I try to combine the
results in some way.

1681
01:15:17,139 --> 01:15:19,529
Okay. So how do you do this?

1682
01:15:19,529 --> 01:15:21,799
So let's walk through an example

1683
01:15:21,799 --> 01:15:23,499
and show how the
op engine works.

1684
01:15:23,499 --> 01:15:25,579
Okay? So for the interrupt pass,

1685
01:15:25,579 --> 01:15:28,059
um, so basically, given
this commuting graph,

1686
01:15:28,059 --> 01:15:30,319
right, we need to cut graph

1687
01:15:30,319 --> 01:15:33,059
into multiple stages
to form a pipeline.

1688
01:15:33,059 --> 01:15:35,239
That's interrupt. And there

1689
01:15:35,239 --> 01:15:37,339
are different ways of
paging this graph,

1690
01:15:37,339 --> 01:15:40,679
um, in this interrupt
strategy space.

1691
01:15:40,679 --> 01:15:42,799
So we want to get
the best one. Okay?

1692
01:15:42,799 --> 01:15:44,579
I only solve for the subroglem,

1693
01:15:44,579 --> 01:15:45,839
I don't worry at this moment

1694
01:15:45,839 --> 01:15:47,499
how to solve it
inside of this stage.

1695
01:15:47,499 --> 01:15:52,919
Okay. So even if we pick
a way to part a graph,

1696
01:15:52,919 --> 01:15:55,199
such as this one choose, suppose

1697
01:15:55,199 --> 01:15:57,239
we pick this one,
this one is optimal.

1698
01:15:57,239 --> 01:16:00,299
We still need to assign
each stage to a set of

1699
01:16:00,299 --> 01:16:03,499
devices to ask the stage
using intrap parti.

1700
01:16:03,499 --> 01:16:05,784
That's the second
level of the problem.

1701
01:16:05,784 --> 01:16:09,469
So in order to do so,
basically we abstract out

1702
01:16:09,469 --> 01:16:11,309
all the devices as two D.

1703
01:16:11,309 --> 01:16:13,409
Device mash like
this, four by four.

1704
01:16:13,409 --> 01:16:15,449
Okay. And assume that
the device along

1705
01:16:15,449 --> 01:16:16,969
each match dimension has

1706
01:16:16,969 --> 01:16:18,749
the same communicating
bandwidth.

1707
01:16:18,749 --> 01:16:21,729
For example, for TPGPU
classroom like this,

1708
01:16:21,729 --> 01:16:24,189
we can set one dimension to

1709
01:16:24,189 --> 01:16:26,809
be all the nodes, nodes, okay.

1710
01:16:26,809 --> 01:16:28,310
And all the communication

1711
01:16:28,310 --> 01:16:30,309
along this dimension
will go through,

1712
01:16:30,309 --> 01:16:33,449
like I said, slow cross
node interconnects, right?

1713
01:16:33,449 --> 01:16:35,449
We can set another dimension

1714
01:16:35,449 --> 01:16:37,509
to be all the GPs
instead of node.

1715
01:16:37,509 --> 01:16:39,369
So communicating
along this dimension

1716
01:16:39,369 --> 01:16:41,449
is pretty fast. Go
through Milink.

1717
01:16:41,449 --> 01:16:46,909
Okay. So what we do is basically
we assign devices, um,

1718
01:16:46,909 --> 01:16:49,429
um, uh, to each stage by pick

1719
01:16:49,429 --> 01:16:53,169
the best submatch choice
from this device classroom.

1720
01:16:53,169 --> 01:16:54,729
So we keep enumerating.

1721
01:16:54,729 --> 01:16:56,189
Okay, we partition
the device into

1722
01:16:56,189 --> 01:16:58,129
subgroups and we assign that to

1723
01:16:58,129 --> 01:16:59,789
the upper partitioning and we

1724
01:16:59,789 --> 01:17:01,889
evaluate and find
out the runtime.

1725
01:17:01,889 --> 01:17:04,089
And then we keep
trying until we figure

1726
01:17:04,089 --> 01:17:07,339
out the one that
is the best. Okay.

1727
01:17:07,779 --> 01:17:12,079
Um, so basically we

1728
01:17:12,079 --> 01:17:14,059
basically find that
this joint problem

1729
01:17:14,059 --> 01:17:15,139
of how to partition

1730
01:17:15,139 --> 01:17:18,659
the computi graph and how
to assign each assign

1731
01:17:18,659 --> 01:17:20,619
how to assign each
parton stage to

1732
01:17:20,619 --> 01:17:22,899
a set of device from
the device mesh.

1733
01:17:22,899 --> 01:17:24,719
They can be nicely formulated

1734
01:17:24,719 --> 01:17:26,459
as a dynamic
programming program,

1735
01:17:26,459 --> 01:17:27,779
which basically minimize

1736
01:17:27,779 --> 01:17:30,079
the pipeline
execution time, okay?

1737
01:17:30,079 --> 01:17:33,279
Um, but in order to solve
this dynamic programming, uh,

1738
01:17:33,279 --> 01:17:35,819
we also need to know
the optimal time

1739
01:17:35,819 --> 01:17:38,499
of running a stage
on align mesh.

1740
01:17:38,499 --> 01:17:42,209
Because S, if you pick
dynamesh for this device,

1741
01:17:42,209 --> 01:17:45,419
if you pick this submash
for this part of stage,

1742
01:17:45,419 --> 01:17:47,559
you still need to know what is

1743
01:17:47,559 --> 01:17:50,039
the best way of running
that graph on this mash,

1744
01:17:50,039 --> 01:17:52,319
using your best introp partism.

1745
01:17:52,319 --> 01:17:55,259
So you are subject to
another level option

1746
01:17:55,259 --> 01:17:57,679
you probably want to solve,
which is for the intrap.

1747
01:17:57,679 --> 01:18:03,119
Okay? Yeah, I think that
is all I have today.

1748
01:18:03,119 --> 01:18:06,099
I'm going to finish
this next maybe

1749
01:18:06,099 --> 01:18:10,119
on Thursday and then we move
onto Mo. Cool. Thank you.
