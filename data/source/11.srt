1
00:00:14,360 --> 00:00:20,120
Okay, uh, yeah, thanks for
coming. Let's get started.

2
00:00:24,870 --> 00:00:29,229
Okay, so hope you
enjoyed PA one.

3
00:00:29,710 --> 00:00:34,590
Yeah. So pA two will be
released by midnight tonight.

4
00:00:34,590 --> 00:00:37,109
And we heard your feedback.

5
00:00:37,109 --> 00:00:38,649
So we are going to
reduce workload

6
00:00:38,649 --> 00:00:40,710
in p two, but it's
going to be deeper.

7
00:00:40,710 --> 00:00:45,109
Which means that if you want
to really do pretty well,

8
00:00:45,109 --> 00:00:46,529
you can spend more
time on pA two.

9
00:00:46,529 --> 00:00:48,549
Yeah. But you can
also choose not to

10
00:00:48,549 --> 00:00:50,929
because we all design question

11
00:00:50,929 --> 00:00:52,889
in a way where you are able to

12
00:00:52,889 --> 00:00:55,850
optimize your algorithm
and get more grades.

13
00:00:55,850 --> 00:01:00,010
Okay? Because it's
going to be deeper,

14
00:01:00,010 --> 00:01:02,489
so we will allow
you to collaborate.

15
00:01:02,489 --> 00:01:06,929
You can try to form a team
with up to three members,

16
00:01:06,929 --> 00:01:08,590
and you guys can discuss how

17
00:01:08,590 --> 00:01:10,830
to write a better
version of the code,

18
00:01:10,830 --> 00:01:12,649
you'll be able to collaborate.

19
00:01:12,649 --> 00:01:14,030
And when you submit PA two,

20
00:01:14,030 --> 00:01:17,810
you all need to mark who
are your collaborators,

21
00:01:19,850 --> 00:01:23,869
IP three, we are going to
make it pretty lightweight,

22
00:01:23,869 --> 00:01:27,030
very few coding compared
to PA one and PA two.

23
00:01:27,030 --> 00:01:28,610
And we are going
to give you some

24
00:01:28,610 --> 00:01:30,190
very fun question in ps three,

25
00:01:30,190 --> 00:01:32,130
you'll be able to
understand what is going

26
00:01:32,130 --> 00:01:34,570
on in sync value.

27
00:01:34,570 --> 00:01:35,890
That's one major topic I have

28
00:01:35,890 --> 00:01:37,984
been always talking
about in this course.

29
00:01:37,984 --> 00:01:43,640
Yeah. Cool. Okay. Today, I

30
00:01:43,640 --> 00:01:45,260
want to finish the
condensation part

31
00:01:45,260 --> 00:01:47,419
because I think in
my previous lecture,

32
00:01:47,419 --> 00:01:50,100
we talk about linear
condadonTday,

33
00:01:50,100 --> 00:01:51,340
I want to finish that part.

34
00:01:51,340 --> 00:01:53,520
Then I need to introduce
a very important thing

35
00:01:53,520 --> 00:01:56,300
that like I said,

36
00:01:56,300 --> 00:01:57,520
probably you need to memorize

37
00:01:57,520 --> 00:01:59,160
that part that is
mixed precision,

38
00:01:59,160 --> 00:02:00,700
because mixed
precision is one of

39
00:02:00,700 --> 00:02:05,279
the most adaptive techniques
from contaation, okay?

40
00:02:05,279 --> 00:02:06,980
And then we will start our

41
00:02:06,980 --> 00:02:09,099
next big chapter,
which is paralyzation.

42
00:02:09,099 --> 00:02:12,019
Okay. Okay. Just
to recap, right?

43
00:02:12,019 --> 00:02:14,619
Last lecture, we starting from

44
00:02:14,619 --> 00:02:15,999
the floating point orientation

45
00:02:15,999 --> 00:02:18,179
and the floating
point arithmetics.

46
00:02:18,179 --> 00:02:19,959
And we start talking about how

47
00:02:19,959 --> 00:02:21,959
to quantize floating
points into.

48
00:02:21,959 --> 00:02:25,259
The first one is key
means based condensation,

49
00:02:25,259 --> 00:02:27,160
where we are able
to reduce storage,

50
00:02:27,160 --> 00:02:29,099
but we are not able to
save compute, right.

51
00:02:29,099 --> 00:02:30,959
And we also introduce a more

52
00:02:30,959 --> 00:02:32,840
common more general
codoon method,

53
00:02:32,840 --> 00:02:35,220
linear condensation,
where we'll be able

54
00:02:35,220 --> 00:02:38,880
to reduce the precision from
floating point to integer.

55
00:02:38,880 --> 00:02:43,279
And we can also basically
migrate the computation from

56
00:02:43,279 --> 00:02:45,219
floating point arithmetic into

57
00:02:45,219 --> 00:02:48,580
say integer or target
precision arithmetic,

58
00:02:48,580 --> 00:02:51,655
which we also save
to compute, okay?

59
00:02:51,655 --> 00:02:54,289
I think when we talk
about this condensation,

60
00:02:54,289 --> 00:02:55,509
especially in the
near condition,

61
00:02:55,509 --> 00:02:58,049
we always assume the
first one, right?

62
00:02:58,049 --> 00:03:01,269
Here I give the lime, that
is per tensor condensation.

63
00:03:01,269 --> 00:03:03,170
That is give me a tensor.

64
00:03:03,170 --> 00:03:05,050
I'm going to apply the
same condensation,

65
00:03:05,050 --> 00:03:06,970
right, for all the elements,

66
00:03:06,970 --> 00:03:08,530
all the entries in a tensor.

67
00:03:08,530 --> 00:03:11,690
But in practice, this does
not always work well, okay?

68
00:03:11,690 --> 00:03:13,989
So sometimes we probably need

69
00:03:13,989 --> 00:03:16,970
more fun green
condensation granularity,

70
00:03:16,970 --> 00:03:19,549
and this will bring us

71
00:03:19,549 --> 00:03:22,690
to more common use
cotton technique,

72
00:03:22,690 --> 00:03:24,610
or I would say
quantitation scheme,

73
00:03:24,610 --> 00:03:28,099
which is per channel condensation
and group condensation.

74
00:03:28,099 --> 00:03:30,509
Okay. I many today's RM,

75
00:03:30,509 --> 00:03:31,769
I think people basically do,

76
00:03:31,769 --> 00:03:34,430
for example, group condition
or poteno coddation.

77
00:03:34,430 --> 00:03:36,390
I'm going to
introduce what they.

78
00:03:36,390 --> 00:03:40,369
Before that, let's revisit
potential condition.

79
00:03:40,369 --> 00:03:42,589
If you remember this
linear condition,

80
00:03:42,589 --> 00:03:45,490
we need to determine so
here is the real number,

81
00:03:45,490 --> 00:03:46,949
for example, flowing
point number and

82
00:03:46,949 --> 00:03:49,190
the Q is quantized number.

83
00:03:49,190 --> 00:03:51,650
And if we want to apply
linear codiation,

84
00:03:51,650 --> 00:03:53,929
we need to figure out
two key parameters,

85
00:03:53,929 --> 00:03:55,249
which is the scale S,

86
00:03:55,249 --> 00:03:56,849
and the Z zero point.

87
00:03:56,849 --> 00:03:59,029
Normally, we decide
the Z are zero, right?

88
00:03:59,029 --> 00:04:00,609
So basically we have
one parameter that

89
00:04:00,609 --> 00:04:03,375
is skinning factor, okay?

90
00:04:03,375 --> 00:04:05,220
And given this tensor,

91
00:04:05,220 --> 00:04:06,399
what we do is
basically we figure

92
00:04:06,399 --> 00:04:10,020
out in the floating point
in the original domain,

93
00:04:10,020 --> 00:04:13,200
uh, so what is QMAX and Q?

94
00:04:13,200 --> 00:04:15,199
What is the RMAX? What
is the Rmin, right?

95
00:04:15,199 --> 00:04:17,979
And then we calculate
the skin factor

96
00:04:17,979 --> 00:04:19,760
like this. Right. Okay.

97
00:04:19,760 --> 00:04:23,039
Pretty simple. Okay.
But in reality,

98
00:04:23,039 --> 00:04:24,980
this does not always
work, like I said.

99
00:04:24,980 --> 00:04:27,280
So this is the real
example where I'm

100
00:04:27,280 --> 00:04:30,280
going to do uh matrix
multiplication.

101
00:04:30,280 --> 00:04:34,120
Um, so I have the
weight matrix which

102
00:04:34,120 --> 00:04:35,560
is W and W has

103
00:04:35,560 --> 00:04:38,260
different channels it's
multi dimensional, ok?

104
00:04:38,260 --> 00:04:41,960
And if you draw the
uh values of, uh,

105
00:04:41,960 --> 00:04:45,339
each channel and try to
project it into this graph,

106
00:04:45,339 --> 00:04:47,500
you see, there are
a few channels that

107
00:04:47,500 --> 00:04:50,120
have a very large
dynamic range, right?

108
00:04:50,120 --> 00:04:52,499
But most of the channels
they just basically stay

109
00:04:52,499 --> 00:04:55,060
within a small
range around zero.

110
00:04:55,060 --> 00:04:56,460
But you can see,

111
00:04:56,460 --> 00:04:58,020
there are a few
channels that goes

112
00:04:58,020 --> 00:05:00,120
up and down, right,
very drastically.

113
00:05:00,120 --> 00:05:02,859
And uh and if we use

114
00:05:02,859 --> 00:05:07,059
single scale a to do
quantdation, what's the problem?

115
00:05:07,059 --> 00:05:09,600
So basically, when
we calculate that S,

116
00:05:09,600 --> 00:05:11,680
the problem is the S will

117
00:05:11,680 --> 00:05:14,020
adapt to those large dynamic
range numbers, right?

118
00:05:14,020 --> 00:05:16,920
And we all lose precision for
those small numbers, right?

119
00:05:16,920 --> 00:05:21,380
So a common there is that
when we use single S,

120
00:05:21,380 --> 00:05:24,940
we'll be strongly influenced
by those outlier weights.

121
00:05:24,940 --> 00:05:26,060
For example, in this figure, you

122
00:05:26,060 --> 00:05:27,559
see quite a few layer weights,

123
00:05:27,559 --> 00:05:29,560
very large or large,

124
00:05:29,560 --> 00:05:31,439
positive or are negative.

125
00:05:31,439 --> 00:05:33,340
Okay? And the solution is

126
00:05:33,340 --> 00:05:35,479
basically we try to it's
very straightforward.

127
00:05:35,479 --> 00:05:38,260
When you see this, you come
up with a solution that we

128
00:05:38,260 --> 00:05:39,640
basically do antradt for

129
00:05:39,640 --> 00:05:41,619
each channel that will
solve our problem,

130
00:05:41,619 --> 00:05:43,740
okay? So how do we do that?

131
00:05:43,740 --> 00:05:45,779
So here I'm going to compare

132
00:05:45,779 --> 00:05:48,319
pertensor conuation
and perteno conation.

133
00:05:48,319 --> 00:05:50,319
So given that matrix, okay.

134
00:05:50,319 --> 00:05:52,480
Let's assume we basically
do the same thing,

135
00:05:52,480 --> 00:05:53,940
at least two bit condensation.

136
00:05:53,940 --> 00:05:55,759
So if we do pretensor connation,

137
00:05:55,759 --> 00:05:58,100
we basically need to
figure out the RMX Here I

138
00:05:58,100 --> 00:05:59,240
simply about a little bit by

139
00:05:59,240 --> 00:06:00,940
assuming zero point is aligned,

140
00:06:00,940 --> 00:06:03,159
which means this
is equal to zero.

141
00:06:03,159 --> 00:06:06,859
And then we figure
out RMX is 2.12.

142
00:06:06,859 --> 00:06:08,860
And we basically do
this equation, right,

143
00:06:08,860 --> 00:06:11,344
and we get the skin vector.

144
00:06:11,344 --> 00:06:13,489
And we apply this
skinning factor

145
00:06:13,489 --> 00:06:15,590
using our linear
conduon equation,

146
00:06:15,590 --> 00:06:17,869
we basically get the
contact weight, okay?

147
00:06:17,869 --> 00:06:19,430
And we can use quantat weight

148
00:06:19,430 --> 00:06:21,050
to reconstruct the
original weight,

149
00:06:21,050 --> 00:06:22,669
and we'll get the matrix.

150
00:06:22,669 --> 00:06:25,509
Okay? This is pretty easy.

151
00:06:25,509 --> 00:06:27,809
So what if we do per
channel connation?

152
00:06:27,809 --> 00:06:29,810
So if we do per channel

153
00:06:29,810 --> 00:06:32,750
codation then we have
more flexibility. Okay?

154
00:06:32,750 --> 00:06:34,190
So for each channel,
we are going

155
00:06:34,190 --> 00:06:36,430
to observe the matrix there

156
00:06:36,430 --> 00:06:38,110
and we try to figure out a

157
00:06:38,110 --> 00:06:40,690
specific skinning factor for it.

158
00:06:40,690 --> 00:06:42,750
Okay? So here, each
row is a channel.

159
00:06:42,750 --> 00:06:45,739
So what we do is basically Um,

160
00:06:45,739 --> 00:06:47,160
for the first row,
first channel,

161
00:06:47,160 --> 00:06:50,599
we figure out the
MAX equal to 2.09,

162
00:06:50,599 --> 00:06:53,180
and we can calculate the
first row skinning actor,

163
00:06:53,180 --> 00:06:56,460
which is the lumber, and we
continue doing so, right?

164
00:06:56,460 --> 00:06:58,620
So we need to maintain
four skinning vectors now.

165
00:06:58,620 --> 00:07:00,940
Okay? Remember, screen factor
is floating point lumber.

166
00:07:00,940 --> 00:07:02,900
Okay. That means
we actually spend

167
00:07:02,900 --> 00:07:04,719
a little bit more storage

168
00:07:04,719 --> 00:07:07,319
to store more skinning
actors, okay?

169
00:07:07,319 --> 00:07:09,820
But the good news
is, if we do this,

170
00:07:09,820 --> 00:07:12,320
we can apply this skinning
vector per channel,

171
00:07:12,320 --> 00:07:15,279
and we'll get a
new contest width,

172
00:07:15,279 --> 00:07:18,654
and we can reconstruct it, okay?

173
00:07:18,654 --> 00:07:22,169
And here we can basically
compare the um,

174
00:07:22,169 --> 00:07:24,910
basically the
reconstructed weight, uh,

175
00:07:24,910 --> 00:07:27,510
versus the original
weight we can try to

176
00:07:27,510 --> 00:07:30,989
quantize the try to measure
the reconstruction arrow.

177
00:07:30,989 --> 00:07:32,710
So in the left part,
we basically get

178
00:07:32,710 --> 00:07:36,174
the quantity arrow which
is two point uh 28.

179
00:07:36,174 --> 00:07:38,840
And in the part, we can
do a little bit better.

180
00:07:38,840 --> 00:07:41,099
Okay? That basically
give intuition.

181
00:07:41,099 --> 00:07:44,300
If we do more fun
grand uh conduation

182
00:07:44,300 --> 00:07:46,860
we are going to
reduce coening error.

183
00:07:46,860 --> 00:07:48,260
Which means that this will be

184
00:07:48,260 --> 00:07:49,720
reflected in the model
accuracy, right,

185
00:07:49,720 --> 00:07:51,499
because you all have
a little bit more

186
00:07:51,499 --> 00:07:53,600
precision or dynamic range.

187
00:07:53,600 --> 00:07:56,440
So your model will perform
a little bit better than,

188
00:07:56,440 --> 00:07:58,699
for example, doing
potential condation.

189
00:07:58,699 --> 00:08:02,780
Okay? But at the cost of
saving more S, right?

190
00:08:02,780 --> 00:08:05,099
So S is actually
pretty heavy compared

191
00:08:05,099 --> 00:08:08,339
to two bit integer because
each S is a floating point.

192
00:08:08,339 --> 00:08:10,079
And in order to represent
a floating point,

193
00:08:10,079 --> 00:08:13,969
I think you at least need
LP eight or LP 16, okay?

194
00:08:13,969 --> 00:08:17,680
Cool. So this is very
straightforward idea, and,

195
00:08:17,680 --> 00:08:19,680
um, I think people apply this

196
00:08:19,680 --> 00:08:21,959
a lot in today's
machinery models.

197
00:08:21,959 --> 00:08:24,920
And apparently, uh,
if we want to do even

198
00:08:24,920 --> 00:08:29,060
better that is basically
group connation, right?

199
00:08:29,060 --> 00:08:32,380
I can do more and
more finer green, um,

200
00:08:32,380 --> 00:08:33,900
condensation, and I will o

201
00:08:33,900 --> 00:08:36,640
basically get group
condensation.

202
00:08:36,640 --> 00:08:40,200
For example, I can
basically try to inspect

203
00:08:40,200 --> 00:08:41,999
the values in this matrix and

204
00:08:41,999 --> 00:08:44,200
I try to figure out those
values that are close.

205
00:08:44,200 --> 00:08:46,599
And I basically apply
a skin vector for

206
00:08:46,599 --> 00:08:49,400
that group of values
which are close in range.

207
00:08:49,400 --> 00:08:51,060
And then I figure
out another group

208
00:08:51,060 --> 00:08:53,379
and I calculate
another scale, right?

209
00:08:53,379 --> 00:08:57,449
That means that I'm doing fine
green condensation, okay?

210
00:08:57,449 --> 00:08:59,339
So the prose is basically we

211
00:08:59,339 --> 00:09:00,939
have more accuracy
because if you

212
00:09:00,939 --> 00:09:02,840
do per element codation

213
00:09:02,840 --> 00:09:04,439
you are essentially
doing nothing, right?

214
00:09:04,439 --> 00:09:07,380
Yeah, you are basically
doing no conduation.

215
00:09:07,380 --> 00:09:09,640
Okay. And if you do
percentage quantity,

216
00:09:09,640 --> 00:09:12,920
basically you use one
scale for all the values.

217
00:09:12,920 --> 00:09:16,199
Okay. The pros is if
you are more fun green,

218
00:09:16,199 --> 00:09:17,880
then you are basically
doing more accuracy.

219
00:09:17,880 --> 00:09:19,459
You have less duty error.

220
00:09:19,459 --> 00:09:22,420
Okay? But the counts is
if you do per value,

221
00:09:22,420 --> 00:09:24,140
like I said, you are
not doing dutation,

222
00:09:24,140 --> 00:09:25,560
then you are rolling back to use

223
00:09:25,560 --> 00:09:26,700
the flowing point orientation

224
00:09:26,700 --> 00:09:28,740
and the flowing
point arithmetic,

225
00:09:28,740 --> 00:09:30,239
you are saving nothing, right?

226
00:09:30,239 --> 00:09:33,179
So basically, eventually you
are facing a problem that is

227
00:09:33,179 --> 00:09:34,619
what is what is

228
00:09:34,619 --> 00:09:36,979
the quantuation granularity
you want to do here?

229
00:09:36,979 --> 00:09:42,919
Yeah. H, so it seems
like we got a channel.

230
00:09:42,919 --> 00:09:46,839
We got a fing high
precision and discounting.

231
00:09:46,839 --> 00:09:52,219
That is Yeah.

232
00:10:02,060 --> 00:10:11,060
Yeah, you can do that.
Yeah. Okay. Cool. This is

233
00:10:11,060 --> 00:10:13,079
a little bit extreme
because like I said,

234
00:10:13,079 --> 00:10:15,219
it's very complicated
because it really

235
00:10:15,219 --> 00:10:18,139
depends on how you looking like,

236
00:10:18,140 --> 00:10:20,640
more practical way is basically

237
00:10:20,640 --> 00:10:22,100
do two level condensation.

238
00:10:22,100 --> 00:10:24,179
Here, originally, we have

239
00:10:24,179 --> 00:10:28,440
time quantize weight
minus zero point, okay?

240
00:10:28,440 --> 00:10:31,760
What we can do is we
just two level, okay?

241
00:10:31,760 --> 00:10:34,940
So in this matrix, you
can see, we are doing,

242
00:10:34,940 --> 00:10:37,280
uh, the miss modification

243
00:10:37,280 --> 00:10:39,520
and we get the results
on the right hand side.

244
00:10:39,520 --> 00:10:42,060
So we can do two level.

245
00:10:42,060 --> 00:10:43,820
The first level is we apply

246
00:10:43,820 --> 00:10:47,100
global condensation
factor Gamma here,

247
00:10:47,100 --> 00:10:49,579
which will be applied
pretensor, okay?

248
00:10:49,579 --> 00:10:52,679
And then on top of that, we
continue to quantize it.

249
00:10:52,679 --> 00:10:54,580
But in the first level
condition we quantize

250
00:10:54,580 --> 00:10:57,724
it to lower bit of
freedom 16-8 bit.

251
00:10:57,724 --> 00:10:59,229
And then at the first level,

252
00:10:59,229 --> 00:11:02,330
we contact it from eight bit
two to say four bit integer.

253
00:11:02,330 --> 00:11:05,129
Okay? We do this hierarchical
condadation. Okay.

254
00:11:05,129 --> 00:11:08,210
So if we represent
this as equation

255
00:11:08,210 --> 00:11:09,850
is basically we are
basically changing

256
00:11:09,850 --> 00:11:11,070
from quit to equation.

257
00:11:11,070 --> 00:11:12,249
Here we have two
skinning factor.

258
00:11:12,249 --> 00:11:14,889
Okay. We first scale
all elements in

259
00:11:14,889 --> 00:11:17,710
the tensor using a common
skinning factor gamma,

260
00:11:17,710 --> 00:11:22,430
and then we skill
per group, yeah, SQ.

261
00:11:22,430 --> 00:11:23,770
So the reason this is

262
00:11:23,770 --> 00:11:25,550
better is because we
save a lot of storage.

263
00:11:25,550 --> 00:11:28,269
So we don't have to, like, do

264
00:11:28,269 --> 00:11:30,430
one skinning factor per group.

265
00:11:30,430 --> 00:11:32,369
Um, at least a higher precision,

266
00:11:32,369 --> 00:11:34,509
we don't need to do that, okay?

267
00:11:35,170 --> 00:11:38,790
Here, like I said, the Gamma
is a floating point cross

268
00:11:38,790 --> 00:11:40,949
green skinning factor and

269
00:11:40,949 --> 00:11:43,909
SQ is an integer per
vector skill factor.

270
00:11:43,909 --> 00:11:46,610
We only need one copy
of Gamma for oranges,

271
00:11:46,610 --> 00:11:50,530
but we need many many
different SQ for each group.

272
00:11:51,090 --> 00:11:54,350
Assume that we are doing
four bit quantization.

273
00:11:54,350 --> 00:11:57,450
That is our inventual
target is four bit,

274
00:11:57,450 --> 00:12:03,030
and our original basically
our original weight

275
00:12:03,030 --> 00:12:04,870
is basically floating point,

276
00:12:04,870 --> 00:12:07,810
we are doing this
global condensation.

277
00:12:07,810 --> 00:12:10,510
We are doing group
condition is four bit per

278
00:12:10,510 --> 00:12:12,710
vector skill every 16 elements.

279
00:12:12,710 --> 00:12:16,064
The cost is
essentially four plus.

280
00:12:16,064 --> 00:12:23,299
164 plus four, then amort has
across 16 numbers, right?

281
00:12:23,299 --> 00:12:25,589
Because each group has
16 elements, okay?

282
00:12:25,589 --> 00:12:28,999
Therefore, we are
essentially effectively well

283
00:12:28,999 --> 00:12:32,539
represented each number
using 4.25 bits.

284
00:12:32,539 --> 00:12:34,280
Okay? That is basically

285
00:12:34,280 --> 00:12:36,399
when you read some
papers, some people say,

286
00:12:36,399 --> 00:12:38,780
um, I'm basically
quantizing my weight

287
00:12:38,780 --> 00:12:41,499
using uh four point half bits.

288
00:12:41,499 --> 00:12:43,139
That's where it's
coming from. Okay?

289
00:12:43,139 --> 00:12:44,440
They are basically
doing this kind

290
00:12:44,440 --> 00:12:45,820
of hierarchical condensation.

291
00:12:45,820 --> 00:12:48,199
And there is one skinning factor

292
00:12:48,199 --> 00:12:50,059
which is amortized across
different elements.

293
00:12:50,059 --> 00:12:51,179
And so in average,

294
00:12:51,179 --> 00:12:52,980
you are basically
doing four point,

295
00:12:52,980 --> 00:12:58,100
uh, four and a quarter bit
condensation, per entry.

296
00:12:58,100 --> 00:13:03,559
Okay? And if you really, really,

297
00:13:03,559 --> 00:13:05,420
want to grind this
a little bit, okay,

298
00:13:05,420 --> 00:13:07,359
you can do like,

299
00:13:07,359 --> 00:13:09,200
you know, more like
aggressive weight.

300
00:13:09,200 --> 00:13:11,699
At least, you do multi level
hierarchical condensation.

301
00:13:11,699 --> 00:13:15,420
Okay? You just continue
to reduce your group set.

302
00:13:15,420 --> 00:13:17,799
And at a smaller group,

303
00:13:17,799 --> 00:13:18,980
you are going to
use a lower bit,

304
00:13:18,980 --> 00:13:20,180
and at a larger
group, for example,

305
00:13:20,180 --> 00:13:21,919
at the tensor level, you are
going to use a higher bit.

306
00:13:21,919 --> 00:13:23,359
And I can give you
a lot of accuracy,

307
00:13:23,359 --> 00:13:24,360
but it's more complicated.

308
00:13:24,360 --> 00:13:29,379
Okay. Yeah. Okay. Cool.
Any questions on this?

309
00:13:29,379 --> 00:13:33,140
So basically, this is how
denting looks like in practice.

310
00:13:33,140 --> 00:13:36,500
So, um, so when you start
working on this area,

311
00:13:36,500 --> 00:13:37,979
I think, you really want to

312
00:13:37,979 --> 00:13:39,579
look into the values
of your weight.

313
00:13:39,579 --> 00:13:41,000
For example, you
are given a model.

314
00:13:41,000 --> 00:13:43,160
You want to basically
plot your model

315
00:13:43,160 --> 00:13:45,120
width distribution, for example,

316
00:13:45,120 --> 00:13:48,039
per channel or basically
along some dimension,

317
00:13:48,039 --> 00:13:49,379
and you try to see
the distribution of

318
00:13:49,379 --> 00:13:51,120
these weights and try
to figure out what's

319
00:13:51,120 --> 00:13:55,559
the right strategy for a
perform quandon. Okay.

320
00:13:55,559 --> 00:13:57,639
Okay, we have been talking about

321
00:13:57,639 --> 00:13:59,200
how to quantize the weights,

322
00:13:59,200 --> 00:14:00,959
but, you know, in deepen work,

323
00:14:00,959 --> 00:14:02,799
we have another worry,

324
00:14:02,799 --> 00:14:06,340
uh like memory intensive
thing, which is activation.

325
00:14:06,340 --> 00:14:09,200
We also need to quantize our
activation sometimes, okay?

326
00:14:09,200 --> 00:14:11,579
So how to quantize activation.

327
00:14:11,579 --> 00:14:15,519
So the problem of activation is,

328
00:14:15,519 --> 00:14:18,259
weight, especially
inference time is static.

329
00:14:18,259 --> 00:14:21,134
At the training time, I think
is semi static because,

330
00:14:21,134 --> 00:14:23,669
especially when your model
is going to converge,

331
00:14:23,669 --> 00:14:27,750
you only have a small Delta
updated, um, to your weight.

332
00:14:27,750 --> 00:14:30,510
So your weight will basically
escalate among small range.

333
00:14:30,510 --> 00:14:32,069
I would say it's semi static.

334
00:14:32,069 --> 00:14:33,909
But for activity it's
quite different, right,

335
00:14:33,909 --> 00:14:35,569
because every time you
are giving a batch of

336
00:14:35,569 --> 00:14:37,350
data in inference
or in training,

337
00:14:37,350 --> 00:14:38,650
and the batch of data could be

338
00:14:38,650 --> 00:14:40,310
very different across batches.

339
00:14:40,310 --> 00:14:42,590
So every time when you
give a batch network,

340
00:14:42,590 --> 00:14:45,469
the activation value can
change a lot, right.

341
00:14:45,469 --> 00:14:48,690
So which means that for
every group of activation,

342
00:14:48,690 --> 00:14:51,949
you have to determine
the right army and RMAX.

343
00:14:51,949 --> 00:14:53,630
And for different
batches, this army

344
00:14:53,630 --> 00:14:55,485
and MAX can be very different.

345
00:14:55,485 --> 00:14:58,219
That's why when we try
to quantize activations,

346
00:14:58,219 --> 00:15:00,739
we need to apply a trick
called dynamic ranging.

347
00:15:00,739 --> 00:15:02,239
So we need to
dynamically determine

348
00:15:02,239 --> 00:15:04,420
army and RMAX for
each data point.

349
00:15:04,420 --> 00:15:07,099
Okay? So there are
two ways to determine

350
00:15:07,099 --> 00:15:10,779
army and RMAxu for activations.

351
00:15:10,779 --> 00:15:12,640
The first way is very simple.

352
00:15:12,640 --> 00:15:15,340
We basically do
moving average, okay?

353
00:15:15,340 --> 00:15:18,340
So that is every time
we observe a batch,

354
00:15:18,340 --> 00:15:22,340
we basically using
Apha to control,

355
00:15:22,340 --> 00:15:24,880
how much we want to learn
from this batch, okay?

356
00:15:24,880 --> 00:15:27,619
We use this offa
to slightly move

357
00:15:27,619 --> 00:15:30,879
our Army Max toward the
current batch, okay?

358
00:15:30,879 --> 00:15:33,040
And by doing this, I
think we can adapt

359
00:15:33,040 --> 00:15:35,679
to new badges which
are extremely say,

360
00:15:35,679 --> 00:15:38,820
have a different
dynamic range, okay?

361
00:15:38,900 --> 00:15:41,979
Okay, another way is that,

362
00:15:41,979 --> 00:15:44,040
you know your data. Assume
you know your data.

363
00:15:44,040 --> 00:15:45,340
For example, you
train your data on

364
00:15:45,340 --> 00:15:47,179
some model and you
know that say,

365
00:15:47,179 --> 00:15:49,940
I'm going to deploy my data, um,

366
00:15:49,940 --> 00:15:52,380
in autous driving
environment, and I know uh,

367
00:15:52,380 --> 00:15:54,340
most of the images or
videos I'm going to read

368
00:15:54,340 --> 00:15:56,740
in auto driving crises
from San Diego.

369
00:15:56,740 --> 00:15:59,139
So, um, then that

370
00:15:59,139 --> 00:16:00,099
means that you actually know

371
00:16:00,099 --> 00:16:01,419
your distribution data, right?

372
00:16:01,419 --> 00:16:04,100
So you can connect a dataset,

373
00:16:04,100 --> 00:16:07,340
which is called a
calibration dataset, okay?

374
00:16:07,340 --> 00:16:08,980
And before you
deploy your model,

375
00:16:08,980 --> 00:16:11,659
you first run your condensation

376
00:16:11,659 --> 00:16:12,919
on the calibration data set

377
00:16:12,919 --> 00:16:14,599
and figure out what would be

378
00:16:14,599 --> 00:16:17,100
the possible army and
RMAX for that dataset.

379
00:16:17,100 --> 00:16:18,300
And you can assume that at

380
00:16:18,300 --> 00:16:20,040
the test time inspect
the inference,

381
00:16:20,040 --> 00:16:22,139
your data is not
going to shift a lot.

382
00:16:22,139 --> 00:16:24,120
So this army and
RMAx should preserve

383
00:16:24,120 --> 00:16:27,000
for incoming batches,
new data, okay?

384
00:16:27,000 --> 00:16:29,960
And this is basically the
trick we use in practice.

385
00:16:29,960 --> 00:16:32,380
So in many Ms, when you
try to quantize the model,

386
00:16:32,380 --> 00:16:34,520
you have to figure out
your target domain.

387
00:16:34,520 --> 00:16:36,729
For example, you want M to do

388
00:16:36,729 --> 00:16:39,140
see a specific task

389
00:16:39,140 --> 00:16:41,880
that generates code
or doing chatbard.

390
00:16:41,880 --> 00:16:43,860
If that's your tax case
you basically connect

391
00:16:43,860 --> 00:16:45,959
some chatboard data and
you try to calibrate

392
00:16:45,959 --> 00:16:49,100
a little bit the Arman
RMAx and you quantat using

393
00:16:49,100 --> 00:16:52,900
the calibrated rmi RMAx and
that should work pretty well.

394
00:16:52,900 --> 00:16:56,760
Okay? So this is what we call
calibration in condition.

395
00:16:56,760 --> 00:16:58,579
I think this is a very
essential procedure to

396
00:16:58,579 --> 00:17:00,939
make sure your model indeed, uh,

397
00:17:00,939 --> 00:17:02,380
robust to those kind of

398
00:17:02,380 --> 00:17:06,939
outliers sorry, outlier
activations, okay?

399
00:17:06,939 --> 00:17:09,959
Cool. Any question here?

400
00:17:10,560 --> 00:17:14,320
Okay, that's all
about quantitation.

401
00:17:14,320 --> 00:17:16,020
I think this is a
very important area.

402
00:17:16,020 --> 00:17:19,359
Um, I'm going to talk a
little bit more about this,

403
00:17:19,359 --> 00:17:23,570
but before that, let's talk
about mixed precision, okay.

404
00:17:23,570 --> 00:17:26,720
So I think now with the
knowledge of connotation,

405
00:17:26,720 --> 00:17:27,519
you probably already know

406
00:17:27,519 --> 00:17:29,119
what mixed press is
going to do, right?

407
00:17:29,119 --> 00:17:30,880
That is, for the target domain,

408
00:17:30,880 --> 00:17:32,659
I'm not going to
restrict myself to be

409
00:17:32,659 --> 00:17:35,700
one specific uh, number of bits.

410
00:17:35,700 --> 00:17:38,500
I'm not going to target only
four bits or eight bits.

411
00:17:38,500 --> 00:17:41,119
I can basically mix all
this kind of possibility.

412
00:17:41,119 --> 00:17:42,819
And I try to figure
out a sweet spot

413
00:17:42,819 --> 00:17:45,039
where I can save as
much as possible.

414
00:17:45,039 --> 00:17:49,519
Well, I still have relatively
good accuracy, okay?

415
00:17:49,519 --> 00:17:51,559
So to motivate a
little bit, basically,

416
00:17:51,559 --> 00:17:53,279
my in our previous lecture,

417
00:17:53,279 --> 00:17:54,360
we said that each layer,

418
00:17:54,360 --> 00:17:56,900
we are going to
apply the same, uh,

419
00:17:56,900 --> 00:17:58,459
target number bits for,

420
00:17:58,459 --> 00:18:01,260
uh, either with or activation.

421
00:18:01,260 --> 00:18:03,220
This is called
uniform connotation.

422
00:18:03,220 --> 00:18:05,719
Okay. But we can be a
little bit smarter, right?

423
00:18:05,719 --> 00:18:07,389
So, we can do

424
00:18:07,389 --> 00:18:09,370
a long uniform condui

425
00:18:09,370 --> 00:18:11,090
which is called a mixed
precision condition.

426
00:18:11,090 --> 00:18:13,590
Okay. So for each layer, uh,

427
00:18:13,590 --> 00:18:17,090
I can basically target
different number of bits, uh,

428
00:18:17,090 --> 00:18:18,670
for weights and activation,

429
00:18:18,670 --> 00:18:21,469
because the intuition behind
this is each layer has

430
00:18:21,469 --> 00:18:22,669
a different dynamic range and

431
00:18:22,669 --> 00:18:25,690
different kind of
robustness to precision.

432
00:18:25,690 --> 00:18:27,669
Maybe for that layer,
it's not so sensitive,

433
00:18:27,669 --> 00:18:30,269
so I can more aggressively
apply codaon.

434
00:18:30,269 --> 00:18:31,930
But for that layer, uh,

435
00:18:31,930 --> 00:18:34,229
it's very, uh, like, uh,

436
00:18:34,229 --> 00:18:35,869
sensitive and you
cannot actually

437
00:18:35,869 --> 00:18:37,850
modify any small
amount of value,

438
00:18:37,850 --> 00:18:39,650
so you have to preserve
the original precision.

439
00:18:39,650 --> 00:18:41,690
Okay? And I can basically figure

440
00:18:41,690 --> 00:18:43,990
out what is the right precision
I use for each layer,

441
00:18:43,990 --> 00:18:46,689
uh, at least granularity.

442
00:18:46,689 --> 00:18:48,969
But the problem of
this approach is

443
00:18:48,969 --> 00:18:52,359
u it's too complicated, right,

444
00:18:52,359 --> 00:18:54,519
because a neural network
has so many layers,

445
00:18:54,519 --> 00:18:56,019
and you really need

446
00:18:56,019 --> 00:18:57,539
to do a lot of work
to figure out what is

447
00:18:57,539 --> 00:18:59,139
the right precision I use for

448
00:18:59,139 --> 00:19:01,899
activations for weight
and for each layer, okay?

449
00:19:01,899 --> 00:19:05,020
So how to do this? So, uh,

450
00:19:05,020 --> 00:19:06,739
before we figure
out how to do this,

451
00:19:06,739 --> 00:19:10,360
um, uh, I can show
you the complexity.

452
00:19:10,360 --> 00:19:12,019
So basically, for
each layer, say,

453
00:19:12,019 --> 00:19:15,560
our target bits is our
budget is eight bits, okay?

454
00:19:15,560 --> 00:19:17,840
Then for each layer for the
weight or for activation,

455
00:19:17,840 --> 00:19:20,699
you basically have
64 choices, right?

456
00:19:20,699 --> 00:19:22,500
And you have the many
layers and layers.

457
00:19:22,500 --> 00:19:25,900
So basically, you have a design
space which is 64 power,

458
00:19:25,900 --> 00:19:27,780
okay, which is super huge.

459
00:19:27,780 --> 00:19:31,059
And and this is very,

460
00:19:31,059 --> 00:19:33,980
um, for this kind of
problem, I think academia,

461
00:19:33,980 --> 00:19:35,359
people really like
that because you

462
00:19:35,359 --> 00:19:37,219
have something to grant, okay?

463
00:19:37,219 --> 00:19:38,759
You can publish new papers

464
00:19:38,759 --> 00:19:40,219
when you figure out
a new scheme, right?

465
00:19:40,219 --> 00:19:43,759
So, um, one way to figure out
this is basically you use,

466
00:19:43,759 --> 00:19:46,960
like I said, machine
learning four systems, okay?

467
00:19:46,960 --> 00:19:49,914
I think I talked about this
in compatible class, right.

468
00:19:49,914 --> 00:19:52,429
C era class you are going to
optimize those loops, right?

469
00:19:52,429 --> 00:19:54,570
You have many many ways
to enroll these loops.

470
00:19:54,570 --> 00:19:56,549
So if you ask

471
00:19:56,549 --> 00:19:58,169
human to do that, it
will take a lot of time.

472
00:19:58,169 --> 00:19:59,449
So you can also basically

473
00:19:59,449 --> 00:20:01,949
find some machine learning
models and you try to

474
00:20:01,949 --> 00:20:04,150
basically perform
some guided search

475
00:20:04,150 --> 00:20:06,770
and use a cost model
to guide search.

476
00:20:06,770 --> 00:20:08,810
So this slide is the
same idea, okay?

477
00:20:08,810 --> 00:20:10,289
I'm doing accurate critique.

478
00:20:10,289 --> 00:20:13,429
Okay? Every time I'm going
to enumerate a possibility

479
00:20:13,429 --> 00:20:14,949
that apply certain bits of

480
00:20:14,949 --> 00:20:17,390
condition to the width and
activation of each layer.

481
00:20:17,390 --> 00:20:20,469
And I then apply it
and I try to see

482
00:20:20,469 --> 00:20:22,189
how many accuracy I have and

483
00:20:22,189 --> 00:20:25,210
also how many memory or
computer I can save.

484
00:20:25,210 --> 00:20:27,374
Basically, I have
a cost function.

485
00:20:27,374 --> 00:20:30,019
And then I measure the
real cost and I try

486
00:20:30,019 --> 00:20:33,380
to fit this kind of
actor critical loop

487
00:20:33,380 --> 00:20:35,700
and try to learn some
model that can model

488
00:20:35,700 --> 00:20:39,500
this relation between lumber
bits and the eventual cost.

489
00:20:39,500 --> 00:20:41,839
And then I can
basically do this.

490
00:20:41,839 --> 00:20:44,799
I can basically deploy this
on a computer and let it run

491
00:20:44,799 --> 00:20:46,040
for seven days and return

492
00:20:46,040 --> 00:20:48,000
a good solution
in the discovery.

493
00:20:48,000 --> 00:20:51,740
Okay? And it turns out that
this is pretty effective.

494
00:20:51,740 --> 00:20:54,439
If you compare this HAQ
which is a method that I

495
00:20:54,439 --> 00:20:57,180
just described compared to
uniform codenation, uh,

496
00:20:57,180 --> 00:20:59,620
you can see it can indeed
figure out pretty good and

497
00:20:59,620 --> 00:21:01,539
scheme where you can

498
00:21:01,539 --> 00:21:04,359
preserve accuracy where still
have a pretty good latency.

499
00:21:04,359 --> 00:21:09,279
Okay. Okay. This is
very interesting,

500
00:21:09,279 --> 00:21:10,380
very good research field,

501
00:21:10,380 --> 00:21:12,519
and I think people
publish a lot in this.

502
00:21:12,519 --> 00:21:16,880
But next what I'm going to
introduce is, This paper?

503
00:21:16,880 --> 00:21:19,560
This paper is not put
on the reading list,

504
00:21:19,560 --> 00:21:21,620
but I highly recommend, okay?

505
00:21:21,620 --> 00:21:23,199
This is a mixed precision

506
00:21:23,199 --> 00:21:25,240
training paper written by media.

507
00:21:25,240 --> 00:21:27,199
And for some reason,
this paper becomes

508
00:21:27,199 --> 00:21:29,800
the standard today, okay?

509
00:21:29,800 --> 00:21:31,100
Um, By the way,

510
00:21:31,100 --> 00:21:32,360
this mixed preceding training

511
00:21:32,360 --> 00:21:33,459
is exactly what I described.

512
00:21:33,459 --> 00:21:36,000
So you basically apply
different precision

513
00:21:36,000 --> 00:21:38,759
for different operators
layers in your new network.

514
00:21:38,759 --> 00:21:40,619
But unfortunately, uh,

515
00:21:40,619 --> 00:21:43,629
this scheme is not discovered
by automatic framework.

516
00:21:43,629 --> 00:21:47,639
Okay, it's again, designed
by expert from media.

517
00:21:47,639 --> 00:21:49,199
So basically, I
think media people,

518
00:21:49,199 --> 00:21:51,079
they spend a lot of
time on figuring

519
00:21:51,079 --> 00:21:53,759
out what is the right way of
combining different bits,

520
00:21:53,759 --> 00:21:55,860
um, in Twin Networks

521
00:21:55,860 --> 00:21:57,200
and they eventually
publish this paper,

522
00:21:57,200 --> 00:21:58,939
and this paper
becomes the standard

523
00:21:58,939 --> 00:22:00,239
in train language models.

524
00:22:00,239 --> 00:22:01,619
Okay? So I want to

525
00:22:01,619 --> 00:22:03,200
spend some time diving
into this paper,

526
00:22:03,200 --> 00:22:04,959
and I want to make
sure you guys really

527
00:22:04,959 --> 00:22:07,890
understand what is
doing here, okay?

528
00:22:07,890 --> 00:22:10,880
So the intuition is, in many,

529
00:22:10,880 --> 00:22:12,999
many neural networks,
especially in language models.

530
00:22:12,999 --> 00:22:14,719
Okay? So layers are

531
00:22:14,719 --> 00:22:17,380
more sensitive to dynamic
range and precision.

532
00:22:17,380 --> 00:22:19,259
The reason I brought up this

533
00:22:19,259 --> 00:22:21,439
today is because you already
finished P one, right.

534
00:22:21,439 --> 00:22:23,439
And I think some of you probably

535
00:22:23,439 --> 00:22:25,599
got stuck on softmax, right?

536
00:22:25,599 --> 00:22:27,839
When you do softmax,
you already return,

537
00:22:27,839 --> 00:22:29,919
a lot of lumber value, right.

538
00:22:29,919 --> 00:22:32,500
So that is because for many,

539
00:22:32,500 --> 00:22:34,120
many operations like softmax,

540
00:22:34,120 --> 00:22:36,299
um, where you are

541
00:22:36,299 --> 00:22:37,539
basically involved
with this kind of

542
00:22:37,539 --> 00:22:39,319
operation, normalization.

543
00:22:39,319 --> 00:22:41,020
You take a floating point value,

544
00:22:41,020 --> 00:22:43,679
uh you take many floating
point value and you get

545
00:22:43,679 --> 00:22:46,700
a summion of the values and
then you normalize it, okay?

546
00:22:46,700 --> 00:22:48,240
For this kind of operation,

547
00:22:48,240 --> 00:22:50,680
it's very sensitive
to precision.

548
00:22:50,680 --> 00:22:52,640
Okay. If you use a
lower bit precision,

549
00:22:52,640 --> 00:22:54,980
you are going to lose a
lot of accuracy, okay?

550
00:22:54,980 --> 00:22:56,580
So intuition is when we try

551
00:22:56,580 --> 00:22:59,319
to calculate this kind of
normalization operation,

552
00:22:59,319 --> 00:23:01,199
we need to be very,
very precise.

553
00:23:01,199 --> 00:23:03,359
Uh, Being precise basically
means that you need to

554
00:23:03,359 --> 00:23:06,479
allocate more bits for fraction,

555
00:23:06,479 --> 00:23:09,685
right, if you remember
our lecture, okay?

556
00:23:09,685 --> 00:23:13,709
And similarly, uh, this is
part of Softmax, right?

557
00:23:13,709 --> 00:23:15,170
And remember in softmax,

558
00:23:15,170 --> 00:23:17,809
you also are taking
expansion function, right?

559
00:23:17,809 --> 00:23:19,990
So basically E Samsung number.

560
00:23:19,990 --> 00:23:22,449
And this number can easily
get explode, right?

561
00:23:22,449 --> 00:23:25,489
Because it's a
power of E, right?

562
00:23:25,489 --> 00:23:29,309
And if something really
is going to explode,

563
00:23:29,309 --> 00:23:31,509
then what should we do?

564
00:23:33,030 --> 00:23:35,509
Add a dynamic range, right.

565
00:23:35,509 --> 00:23:38,295
Okay, so how do add
a dynamic range?

566
00:23:38,295 --> 00:23:41,199
Give it more explod, right?

567
00:23:41,199 --> 00:23:43,839
That is allocate more bits
on the exploding part.

568
00:23:43,839 --> 00:23:46,899
Which means that softmax is
such a magical operation that

569
00:23:46,899 --> 00:23:50,340
is sensitive to dynamic range
and sensitive to precision.

570
00:23:50,340 --> 00:23:52,340
Okay? And this is intuition.

571
00:23:52,340 --> 00:23:53,559
So when you perform softmax,

572
00:23:53,559 --> 00:23:55,379
you probably should
allocate more bits

573
00:23:55,379 --> 00:23:58,079
on both explent and
fraction, right?

574
00:23:58,079 --> 00:24:00,280
That means that we really
don't have a choice.

575
00:24:00,280 --> 00:24:01,500
We have to use a higher

576
00:24:01,500 --> 00:24:04,060
precision floating
point for softmax.

577
00:24:04,060 --> 00:24:05,840
Okay. And similarly,

578
00:24:05,840 --> 00:24:08,219
for this kind of
grading accumulation,

579
00:24:08,219 --> 00:24:11,879
you always add grading to
the parameters, right?

580
00:24:11,879 --> 00:24:13,119
And sometimes your grading could

581
00:24:13,119 --> 00:24:14,359
be extremely large, right,

582
00:24:14,359 --> 00:24:16,020
especially at the
initial training

583
00:24:16,020 --> 00:24:17,799
or when you're now,
what do you do?

584
00:24:17,799 --> 00:24:19,840
It's very sensitive
to dynamic range,

585
00:24:19,840 --> 00:24:23,580
so you want to make sure
that when you accumulate,

586
00:24:23,580 --> 00:24:26,634
you don't explode your
rolling point bits, okay?

587
00:24:26,634 --> 00:24:28,770
So with this kind of intuition,

588
00:24:28,770 --> 00:24:30,310
so the paper I just showed,

589
00:24:30,310 --> 00:24:32,030
they basically come
up with a scheme.

590
00:24:32,030 --> 00:24:34,829
They are saying that we
should identify those ops

591
00:24:34,829 --> 00:24:36,129
that are extremely sensitive

592
00:24:36,129 --> 00:24:37,710
to precision in dynamic range.

593
00:24:37,710 --> 00:24:41,310
And then we should use full
precision, for example,

594
00:24:41,310 --> 00:24:46,150
1.32 for them, whenever
we met these kind of ops.

595
00:24:46,150 --> 00:24:48,069
But once we finish these ops,

596
00:24:48,069 --> 00:24:50,830
we can basically downcast
to lower precision

597
00:24:50,830 --> 00:24:52,089
because we always want to

598
00:24:52,089 --> 00:24:54,129
know always want to use
lower precision, right.

599
00:24:54,129 --> 00:24:56,030
Lower precision is more memory

600
00:24:56,030 --> 00:24:58,829
efficient and more
computer efficient, okay?

601
00:24:58,829 --> 00:25:00,250
And with this kind of intuition,

602
00:25:00,250 --> 00:25:01,730
they start doing a lot
of experiments, okay.

603
00:25:01,730 --> 00:25:02,809
They try to figure out what is

604
00:25:02,809 --> 00:25:04,870
the right way to combine
different precisions.

605
00:25:04,870 --> 00:25:06,770
And they come up
with this scheme.

606
00:25:06,770 --> 00:25:09,569
Okay. I'll like you look
at this for 30 seconds

607
00:25:09,569 --> 00:25:13,309
then we go through it in detail.

608
00:25:39,450 --> 00:25:43,570
Okay, so this is a figure
I quoted from that paper.

609
00:25:43,570 --> 00:25:45,390
And if you want to
know more detail,

610
00:25:45,390 --> 00:25:47,650
go read that paper, but
let's go through this, okay?

611
00:25:47,650 --> 00:25:50,870
So this is basically how
mixed precision works today.

612
00:25:50,870 --> 00:25:52,650
It's mixing two precisions.

613
00:25:52,650 --> 00:25:53,909
One is the floating 0.32.

614
00:25:53,909 --> 00:25:55,589
The other is flowing 0.16.

615
00:25:55,589 --> 00:25:59,810
Okay. And this is the standard
today until last month.

616
00:25:59,810 --> 00:26:01,390
Okay? Because last month,

617
00:26:01,390 --> 00:26:03,109
I think deep sik right, they

618
00:26:03,109 --> 00:26:04,129
release a paper saying that

619
00:26:04,129 --> 00:26:05,829
they do everything in AP eight.

620
00:26:05,829 --> 00:26:09,369
Okay. Uh, so I think it's
still good to bring up this,

621
00:26:09,369 --> 00:26:10,869
but I'm going to bring
up Deep sick later.

622
00:26:10,869 --> 00:26:12,110
Okay, how they do Peight.

623
00:26:12,110 --> 00:26:15,309
But this is still like
how CheBT how most of,

624
00:26:15,309 --> 00:26:18,450
like, today's model
are trained, okay.

625
00:26:18,930 --> 00:26:22,389
So here it is seen that,

626
00:26:22,389 --> 00:26:24,430
I have a master weight.

627
00:26:24,430 --> 00:26:25,849
Okay. This master weight was

628
00:26:25,849 --> 00:26:28,069
basically saved
in floating 0.32.

629
00:26:28,069 --> 00:26:30,050
Okay, high precision,
very high precision.

630
00:26:30,050 --> 00:26:32,930
Okay. And whenever I try to
start doing competition,

631
00:26:32,930 --> 00:26:35,870
what I do is I'm going
to first do a downcast.

632
00:26:35,870 --> 00:26:37,829
Okay? This is the dution right.

633
00:26:37,829 --> 00:26:41,115
So basically, I'm
downcasting 32-16 bit.

634
00:26:41,115 --> 00:26:46,660
Okay. I basically get
a floing 0.16 weights.

635
00:26:46,660 --> 00:26:50,880
I'm going to use fpin 16 weights
to do forward competion.

636
00:26:50,880 --> 00:26:53,879
This is pretty good
because if you

637
00:26:53,879 --> 00:26:56,999
still remember the ODS
product sheet in flopping,

638
00:26:56,999 --> 00:26:58,439
uh, I think 16 bit is

639
00:26:58,439 --> 00:27:00,300
much more higher
flops than 32 bits.

640
00:27:00,300 --> 00:27:03,330
You can use 16 bit
tensor word competion.

641
00:27:03,330 --> 00:27:06,080
And because you use weight
and activation is both

642
00:27:06,080 --> 00:27:08,600
in flowing 0.16 weights,

643
00:27:08,600 --> 00:27:10,279
you are going to also produce

644
00:27:10,279 --> 00:27:12,600
flowing 0.16 weight activations.

645
00:27:12,600 --> 00:27:14,939
Okay. And similarly,
I'm going to use

646
00:27:14,939 --> 00:27:16,540
this 16 bits of weights

647
00:27:16,540 --> 00:27:18,580
and activation to
do backward, okay?

648
00:27:18,580 --> 00:27:20,899
That will give me, that will

649
00:27:20,899 --> 00:27:23,279
basically produce um gradients,

650
00:27:23,279 --> 00:27:26,649
right, that is in
flowing 0.16. Okay.

651
00:27:27,650 --> 00:27:30,329
Then the problem
basically last here.

652
00:27:30,329 --> 00:27:32,829
So when we try to apply

653
00:27:32,829 --> 00:27:36,189
these gradients into
our priamter two.

654
00:27:36,189 --> 00:27:38,449
Okay. What we do is basically,

655
00:27:38,449 --> 00:27:40,269
we will take this gradings,

656
00:27:40,269 --> 00:27:42,349
and we are going to apply

657
00:27:42,349 --> 00:27:45,349
this floating 0.16 into
this master weight to

658
00:27:45,349 --> 00:27:47,370
upload to update
this master weight

659
00:27:47,370 --> 00:27:50,850
into version next version.

660
00:27:50,850 --> 00:27:53,029
And the way we
apply it basically,

661
00:27:53,029 --> 00:27:55,169
we are going to upcast it.

662
00:27:55,169 --> 00:27:58,890
We are going to upcast
from 16 bits into 32 bits.

663
00:27:58,890 --> 00:28:00,749
Okay? We are going to basically

664
00:28:00,749 --> 00:28:04,110
only do this kind
of weight update at

665
00:28:04,110 --> 00:28:06,029
higher precision to make sure we

666
00:28:06,029 --> 00:28:08,629
preserve precision and we

667
00:28:08,629 --> 00:28:11,210
can basically be robust
to dynamic range.

668
00:28:11,210 --> 00:28:13,710
Okay. This is a halo idea.

669
00:28:13,710 --> 00:28:18,029
But the dirty part happens
when we apply this to Adam.

670
00:28:18,029 --> 00:28:20,910
Okay? Because
remember in Adam, uh,

671
00:28:20,910 --> 00:28:22,510
Adam is the default optimism

672
00:28:22,510 --> 00:28:24,349
we use for optimize
neural networks.

673
00:28:24,349 --> 00:28:27,649
And when we apply this
I think for this,

674
00:28:27,649 --> 00:28:28,329
we are good, right,

675
00:28:28,329 --> 00:28:30,349
because it's basically
for the backward.

676
00:28:30,349 --> 00:28:31,870
It's not relevant with atom.

677
00:28:31,870 --> 00:28:34,569
But when we apply this into
Adam, what happens is,

678
00:28:34,569 --> 00:28:36,949
if you still remember in Adam,

679
00:28:36,949 --> 00:28:39,749
um, we have two moments, right.

680
00:28:39,749 --> 00:28:41,809
We need to calculate the
first and second moments,

681
00:28:41,809 --> 00:28:44,470
and we use those moments
to basically normalize

682
00:28:44,470 --> 00:28:46,169
our gradient libit
and then apply it

683
00:28:46,169 --> 00:28:48,489
into the original
version of the w. Okay.

684
00:28:48,489 --> 00:28:51,729
And now let's see how we
apply this in Adam. Okay.

685
00:28:51,730 --> 00:28:55,389
So I will still use
this as an example.

686
00:28:55,389 --> 00:28:56,969
So training GBD three,

687
00:28:56,969 --> 00:28:58,590
because this is the best example

688
00:28:58,590 --> 00:29:00,429
that will help you
understand what's going on.

689
00:29:00,429 --> 00:29:03,229
Okay, um, and what

690
00:29:03,229 --> 00:29:04,449
we care about is when we use

691
00:29:04,449 --> 00:29:05,970
this mixed precision training,

692
00:29:05,970 --> 00:29:08,549
um, framework, uh, how many
memory will actually use?

693
00:29:08,549 --> 00:29:09,729
How many memory we
can save, right?

694
00:29:09,729 --> 00:29:11,529
Because our eventual goal is,

695
00:29:11,529 --> 00:29:14,980
um, we want to save memory
through conation, okay?

696
00:29:14,980 --> 00:29:17,229
So just a little
bit recap, right?

697
00:29:17,229 --> 00:29:18,630
For the largest model,

698
00:29:18,630 --> 00:29:21,649
175 B, we need to store the
weights in 16 bits, right?

699
00:29:21,649 --> 00:29:26,789
So which is 350
giga activations,

700
00:29:26,789 --> 00:29:28,789
you did this in MCQ, right?

701
00:29:28,789 --> 00:29:30,449
So we checkpoint
at the boundary.

702
00:29:30,449 --> 00:29:33,629
We get this many of
activations, okay?

703
00:29:33,670 --> 00:29:36,230
The problem is optimal weights.

704
00:29:36,230 --> 00:29:37,709
So I'm not sure if you

705
00:29:37,709 --> 00:29:39,389
still remember in
previous lecture in

706
00:29:39,389 --> 00:29:41,549
the memory lecture
how many memory

707
00:29:41,549 --> 00:29:44,390
we need for optimal states.

708
00:29:46,710 --> 00:29:49,889
We have grading, right? We have

709
00:29:49,889 --> 00:29:51,969
first moment and
second moment, right?

710
00:29:51,969 --> 00:29:54,809
Okay. So basically, uh, uh,

711
00:29:54,809 --> 00:29:56,629
three copies of values and

712
00:29:56,629 --> 00:29:59,110
each value will be sip
in a target precision.

713
00:29:59,110 --> 00:30:01,309
Okay? Now, let's try to

714
00:30:01,309 --> 00:30:03,789
apply this in our mixed
precision training framework.

715
00:30:03,789 --> 00:30:06,090
Okay? So when we apply

716
00:30:06,090 --> 00:30:09,419
mixed precision training,
the story changes here.

717
00:30:09,419 --> 00:30:11,259
We cannot see weight in FP 16,

718
00:30:11,259 --> 00:30:12,840
right, because we want
to preserve accuracy.

719
00:30:12,840 --> 00:30:14,259
Okay? So we have to maintain

720
00:30:14,259 --> 00:30:17,899
a copy of master weight,
right, in this figure.

721
00:30:17,899 --> 00:30:21,200
Okay? So this copy
is basically four,

722
00:30:21,200 --> 00:30:23,440
four bits, right, 32 bits,

723
00:30:23,440 --> 00:30:25,540
four bits times 175.

724
00:30:25,540 --> 00:30:27,619
This is 700 giga, okay?

725
00:30:27,619 --> 00:30:31,940
So here we have a
factor of four Um,

726
00:30:31,940 --> 00:30:33,740
because we perform competition

727
00:30:33,740 --> 00:30:35,619
using our LP 16 inter core.

728
00:30:35,619 --> 00:30:37,059
So we all produce

729
00:30:37,059 --> 00:30:39,300
a copy of gradients
against the perimeter,

730
00:30:39,300 --> 00:30:44,300
and this copy of gradients is
saved in P 16, right here.

731
00:30:44,300 --> 00:30:47,880
Okay? So we have a
factor two here,

732
00:30:47,880 --> 00:30:49,879
which is the original copy of

733
00:30:49,879 --> 00:30:53,220
gradients we produce at 16 bit.

734
00:30:54,340 --> 00:30:59,419
Okay. And we also need to
have a running copy that is

735
00:30:59,419 --> 00:31:01,459
we need to somehow run

736
00:31:01,459 --> 00:31:04,100
this copy and apply
this copy to our wt.

737
00:31:04,100 --> 00:31:06,619
Okay? So another factor two.

738
00:31:07,900 --> 00:31:11,219
And the secret here is

739
00:31:11,219 --> 00:31:14,059
basically when you apply this
mixed precision in atom,

740
00:31:14,059 --> 00:31:16,019
you have to also store

741
00:31:16,019 --> 00:31:19,700
the first and second moments
in higher precision.

742
00:31:19,700 --> 00:31:22,400
Because remember,
malization eventually

743
00:31:22,400 --> 00:31:22,839
you want to use

744
00:31:22,839 --> 00:31:24,989
this first and second
moment to normalization.

745
00:31:24,989 --> 00:31:27,460
Basically lomize
gradients, okay?

746
00:31:27,460 --> 00:31:29,479
And the lomization
is very sensitive to

747
00:31:29,479 --> 00:31:32,840
precision and also
to, uh, dmic range.

748
00:31:32,840 --> 00:31:34,380
So we have to also store

749
00:31:34,380 --> 00:31:37,760
this first and second
moment in high precision.

750
00:31:37,760 --> 00:31:42,059
So here we get another factor
of four times two, okay?

751
00:31:42,059 --> 00:31:44,160
And we get another factor eight.

752
00:31:44,160 --> 00:31:46,219
Okay? This is what we do.

753
00:31:46,219 --> 00:31:50,080
What do we need to save when
we apply this open states.

754
00:31:51,030 --> 00:31:53,629
So here, I'm giving you one

755
00:31:53,629 --> 00:31:55,509
of the most important
message in this lecture.

756
00:31:55,509 --> 00:31:57,069
Okay, you need to remember this.

757
00:31:57,069 --> 00:32:00,269
So when we train
language models, um,

758
00:32:00,269 --> 00:32:04,889
the memory we need is
lower bounded by this one.

759
00:32:04,889 --> 00:32:07,109
Okay? And this is basically

760
00:32:07,109 --> 00:32:10,230
the first factor of four
second factor gradients,

761
00:32:10,230 --> 00:32:12,570
running copy, first
moment, second moment.

762
00:32:12,570 --> 00:32:14,570
Okay. This is the real
real memory usage

763
00:32:14,570 --> 00:32:15,790
for train language models.

764
00:32:15,790 --> 00:32:17,929
Yeah, because in my
previous lecture,

765
00:32:17,929 --> 00:32:19,689
I never mentioned the
mixed preceding training,

766
00:32:19,689 --> 00:32:20,969
but now I'm basically tell you,

767
00:32:20,969 --> 00:32:23,070
we're actually using
mixed preceding training.

768
00:32:23,070 --> 00:32:25,700
So which means that, uh,

769
00:32:25,700 --> 00:32:27,579
you ended up with a factor of 16

770
00:32:27,579 --> 00:32:29,920
N. We N is your
amber parameters,

771
00:32:29,920 --> 00:32:31,480
one copy of amber parameters.

772
00:32:31,480 --> 00:32:33,279
Why is this equation
so important?

773
00:32:33,279 --> 00:32:35,119
Because next time
when people ask you,

774
00:32:35,119 --> 00:32:37,640
can I train a model using H 100?

775
00:32:37,640 --> 00:32:39,840
Can I train Lama
seven B on H 100?

776
00:32:39,840 --> 00:32:42,299
On single H 100, you
immediately know,

777
00:32:42,299 --> 00:32:44,659
I use this, say,

778
00:32:44,659 --> 00:32:46,079
you have a seven B model Lama,

779
00:32:46,079 --> 00:32:48,019
and you use seven times 16

780
00:32:48,019 --> 00:32:50,899
and you know the lower
bound memory that you need.

781
00:32:50,899 --> 00:32:53,339
Which is apparently
this number is

782
00:32:53,339 --> 00:32:55,319
greater than 80 gigat that
means you are not able to

783
00:32:55,319 --> 00:32:59,339
train Lama seven B
on single H 100.

784
00:32:59,339 --> 00:33:03,530
Okay. Does it. Does
it make sense?

785
00:33:03,530 --> 00:33:05,470
Okay. So make sure you
remember this, okay?

786
00:33:05,470 --> 00:33:06,909
This is the lower
bound for training

787
00:33:06,909 --> 00:33:08,650
language models, okay?

788
00:33:08,650 --> 00:33:11,569
Any framework, yeah.
It apples to Petro,

789
00:33:11,569 --> 00:33:13,719
you apply to tender flow
jacks, any framework.

790
00:33:13,719 --> 00:33:17,090
Why you copy gradients?

791
00:33:17,090 --> 00:33:19,430
Because, on one hand,

792
00:33:19,430 --> 00:33:20,690
you need to derive
the activation

793
00:33:20,690 --> 00:33:22,149
gradients using weed gradits.

794
00:33:22,149 --> 00:33:23,589
You have to save that.
On the other hand,

795
00:33:23,589 --> 00:33:25,029
when you apply that
gradient to the primary,

796
00:33:25,029 --> 00:33:26,550
how to upcast it,
you need to copy

797
00:33:26,550 --> 00:33:29,269
to upcast. Yeah. Yeah. Okay?

798
00:33:29,510 --> 00:33:35,329
Okay, 16. And this is not
considering activations.

799
00:33:35,329 --> 00:33:38,750
Okay? So if you want to
also consider activations,

800
00:33:38,750 --> 00:33:40,269
you need to go back
to this slide and try

801
00:33:40,269 --> 00:33:42,569
to study this a little bit.

802
00:33:42,569 --> 00:33:44,750
Okay? So how we
save activations.

803
00:33:44,750 --> 00:33:49,449
Cool. Okay, memorize this.

804
00:33:49,449 --> 00:33:52,629
I think this is extremely
important. Okay.

805
00:33:52,629 --> 00:33:55,069
Yeah, that basically
wrap up my lecture

806
00:33:55,069 --> 00:33:57,429
on quantuon or more broadly,

807
00:33:57,429 --> 00:33:59,010
skinning down machine learning.

808
00:33:59,010 --> 00:34:01,570
I want to spend a few minutes
to talk about this area.

809
00:34:01,570 --> 00:34:02,909
So basically skinning down

810
00:34:02,909 --> 00:34:04,189
machine learning and
deep learning is

811
00:34:04,189 --> 00:34:08,409
a very active area of research
in machearing and systems.

812
00:34:08,409 --> 00:34:10,170
A lot of people a
lot of professors,

813
00:34:10,170 --> 00:34:12,350
students are doing this, okay?

814
00:34:12,350 --> 00:34:14,869
So because running
machine learning on

815
00:34:14,869 --> 00:34:18,249
edge devices is always
strongly demanded, right?

816
00:34:18,249 --> 00:34:20,249
I think everyone
wants to do that.

817
00:34:20,249 --> 00:34:24,065
That's why there's a high
demand on this scale, okay?

818
00:34:24,065 --> 00:34:28,379
But one thing that I want
to point out is, um, uh,

819
00:34:28,379 --> 00:34:30,219
this market of scaling down

820
00:34:30,219 --> 00:34:33,319
merginary it has a very
key characteristics.

821
00:34:33,319 --> 00:34:35,439
Okay? It's a very
fragmented market.

822
00:34:35,439 --> 00:34:37,859
So what do I mean by
fragmented market?

823
00:34:37,859 --> 00:34:40,179
So I'm comparing
this to cloud market

824
00:34:40,179 --> 00:34:42,180
where we scale up marginally.

825
00:34:42,180 --> 00:34:45,679
Okay. So why is if you
think about cloud market,

826
00:34:45,679 --> 00:34:47,299
it's not fragmented
at all, right?

827
00:34:47,299 --> 00:34:51,299
Basically, AWS has 70%,
uh, market share, right?

828
00:34:51,299 --> 00:34:52,800
And then GCP and Microsoft.

829
00:34:52,800 --> 00:34:54,879
Okay, three, big three, okay.

830
00:34:54,879 --> 00:34:58,039
But if you talk about skinning
down marchiney there's no,

831
00:34:58,039 --> 00:34:59,380
like, a winner, okay?

832
00:34:59,380 --> 00:35:01,399
So everyone is doing
their own thing. Why?

833
00:35:01,399 --> 00:35:05,200
Because in Cloud, we
only have media chips.

834
00:35:05,200 --> 00:35:08,720
Okay? We roughly deploy
everything, media chips.

835
00:35:08,720 --> 00:35:11,039
And in Cloud, AWS
is the only winner.

836
00:35:11,039 --> 00:35:12,359
You probably know, yeah.

837
00:35:12,359 --> 00:35:14,019
But when you talk
about screen down,

838
00:35:14,019 --> 00:35:15,599
there are so many diverse chips.

839
00:35:15,599 --> 00:35:17,140
I think in the previous lecture,

840
00:35:17,140 --> 00:35:20,179
our gas lecture change
et cetera, uh, uh,

841
00:35:20,179 --> 00:35:21,940
when we do merginy compilation,

842
00:35:21,940 --> 00:35:23,639
the main problem is there
are so many different chips

843
00:35:23,639 --> 00:35:25,400
that you have to compare
down to different,

844
00:35:25,400 --> 00:35:27,889
like, kernel libraries
or whatever, right?

845
00:35:27,889 --> 00:35:30,260
So it's a very frag
market market,

846
00:35:30,260 --> 00:35:31,259
which means that if you

847
00:35:31,259 --> 00:35:32,739
choose to do research
in this area,

848
00:35:32,739 --> 00:35:33,920
it's very easy to publish

849
00:35:33,920 --> 00:35:35,299
because there's
always a problem.

850
00:35:35,299 --> 00:35:36,920
Right, for target hardware.

851
00:35:36,920 --> 00:35:38,819
Yeah, because there
are not enough people.

852
00:35:38,819 --> 00:35:41,659
And if you choose to do
a business in this area,

853
00:35:41,659 --> 00:35:43,279
I would say it's a
good business, right,

854
00:35:43,279 --> 00:35:45,570
because especially for startups,

855
00:35:45,570 --> 00:35:47,389
right, because you
can always choose

856
00:35:47,389 --> 00:35:50,330
a target domain which
is under addressed,

857
00:35:50,330 --> 00:35:52,509
and you try to address
the problem for them.

858
00:35:52,509 --> 00:35:55,929
Indeed, this is a
pretty good, um, uh,

859
00:35:55,929 --> 00:35:57,789
area that you can build
a startup and get

860
00:35:57,789 --> 00:36:00,369
acquired. I can give
you an example.

861
00:36:00,369 --> 00:36:02,729
Ten chi right, his
company AutoML,

862
00:36:02,729 --> 00:36:04,690
they do pretty well
on Intel chips.

863
00:36:04,690 --> 00:36:06,889
And I think at some point,

864
00:36:06,889 --> 00:36:10,669
they got a pretty good offer
from Intel, to acquire them.

865
00:36:10,669 --> 00:36:14,810
But they declined, because
they want to do bigger.

866
00:36:14,810 --> 00:36:19,389
They want, right?
Eventually market changes.

867
00:36:19,389 --> 00:36:21,469
Okay. Another example is,

868
00:36:21,469 --> 00:36:23,970
um, a mating professor,

869
00:36:23,970 --> 00:36:27,150
the quantititi guy, I
mentioned Songhas company,

870
00:36:27,150 --> 00:36:30,089
and he's doing
condition a lot, right?

871
00:36:30,089 --> 00:36:31,770
He's a guide of
condition, basically.

872
00:36:31,770 --> 00:36:33,749
And he does a lot of
condensation work

873
00:36:33,749 --> 00:36:37,009
for Avidia and for
Qualcomm, okay?

874
00:36:37,009 --> 00:36:38,329
He basically optimize against

875
00:36:38,329 --> 00:36:40,550
Nvidia chips and Qualcomm chips.

876
00:36:40,550 --> 00:36:43,130
And it turns out that
he was very successful.

877
00:36:43,130 --> 00:36:45,370
I think in 2022, his company

878
00:36:45,370 --> 00:36:47,329
was acquired by Nvidia, okay?

879
00:36:47,329 --> 00:36:49,609
I heard that he got
two offers from both,

880
00:36:49,609 --> 00:36:51,690
like, Quadcom and Nvidia.

881
00:36:51,690 --> 00:36:54,049
Okay, he chose to go A
Midia in 2022, okay?

882
00:36:54,049 --> 00:36:56,169
That's a pretty good
deal, good deal, right?

883
00:36:56,169 --> 00:36:57,889
Because in 2022, WIDA Chip,

884
00:36:57,889 --> 00:37:00,875
Amdatock is still here,
right? Yeah, okay?

885
00:37:00,875 --> 00:37:02,800
And I also have a few friends

886
00:37:02,800 --> 00:37:04,480
which do this kind of business,

887
00:37:04,480 --> 00:37:06,740
and they basically choose,

888
00:37:06,740 --> 00:37:08,659
hardware winder
because, you know,

889
00:37:08,659 --> 00:37:11,639
hardware winder lot is
very rich today, right?

890
00:37:11,639 --> 00:37:12,899
And they have a lot of
money. They want to

891
00:37:12,899 --> 00:37:14,520
acquire every small companies.

892
00:37:14,520 --> 00:37:17,240
And they basically target
that speed hardware.

893
00:37:17,240 --> 00:37:18,659
They try to do very deep,

894
00:37:18,659 --> 00:37:21,000
like don or implementation
for them hardware,

895
00:37:21,000 --> 00:37:24,060
and they hope to be
acquired. And they always.

896
00:37:24,060 --> 00:37:27,024
Okay? But the bad news is, um,

897
00:37:27,024 --> 00:37:30,490
if you really want to build
a really big business,

898
00:37:30,490 --> 00:37:32,090
for example, as large as Google,

899
00:37:32,090 --> 00:37:34,549
as a Amazon, I
don't think this is

900
00:37:34,549 --> 00:37:37,569
a good area for doing
that because like I said,

901
00:37:37,569 --> 00:37:39,189
it's very easy to it's very

902
00:37:39,189 --> 00:37:41,189
difficult to unify
every chips, right?

903
00:37:41,189 --> 00:37:43,910
You have to design one
type of condensation

904
00:37:43,910 --> 00:37:46,830
and algorithm and a kernel
library for one typo hardware.

905
00:37:46,830 --> 00:37:48,070
It's very very fragmented,

906
00:37:48,070 --> 00:37:49,729
so everybody is doing their own.

907
00:37:49,729 --> 00:37:54,040
Okay? Um, and in terms
of research direction,

908
00:37:54,040 --> 00:37:55,839
I think there are still a lot

909
00:37:55,839 --> 00:37:57,859
of going on because every
time there's a new model,

910
00:37:57,859 --> 00:37:59,340
when there's a new hardware,

911
00:37:59,340 --> 00:38:02,460
when there's a new kind
of kernel operator,

912
00:38:02,460 --> 00:38:04,999
quanta people is going
to basically tact them.

913
00:38:04,999 --> 00:38:07,139
Yeah, they are going to probably
feel prepared for that.

914
00:38:07,139 --> 00:38:09,799
Okay? And people are
doing a lot in this area,

915
00:38:09,799 --> 00:38:12,420
for example, quantadon pruning,

916
00:38:12,420 --> 00:38:14,039
because you eventually want

917
00:38:14,039 --> 00:38:15,379
to deploy your ID
devices, right,

918
00:38:15,379 --> 00:38:18,700
you start care about energy,
okay, energy efficiency.

919
00:38:18,700 --> 00:38:20,560
And you also do
federation machining,

920
00:38:20,560 --> 00:38:23,180
Okay, which was not
covered in this lecture.

921
00:38:23,180 --> 00:38:25,620
But unfortunately,
we hired a faculty.

922
00:38:25,620 --> 00:38:27,439
He's going to join this fall.

923
00:38:27,439 --> 00:38:29,419
Yeah, he's a quant
editing guy, okay?

924
00:38:29,419 --> 00:38:31,039
And if you're
interested, you can

925
00:38:31,039 --> 00:38:33,540
try to listen to
his classes, okay?

926
00:38:33,540 --> 00:38:36,420
Cool. That's all from
me on quant edition.

927
00:38:36,420 --> 00:38:38,299
Okay. Then let's move on, okay?

928
00:38:38,299 --> 00:38:39,999
Big picture. So we finish

929
00:38:39,999 --> 00:38:43,659
DataFgraph right
AutoDvGraph Oenation.

930
00:38:43,659 --> 00:38:46,744
We also dive deeper
into operator Oenation.

931
00:38:46,744 --> 00:38:48,689
Uh, we also start
talking about run time,

932
00:38:48,689 --> 00:38:51,010
how to schedule, how to do
memory, how to do quantition.

933
00:38:51,010 --> 00:38:53,269
Okay? Now, I think we
are going to enter

934
00:38:53,269 --> 00:38:56,429
our last big lecture
Paradition, right?

935
00:38:56,429 --> 00:39:00,829
Okay. Inspired I want to cover
this many content, okay?

936
00:39:00,829 --> 00:39:02,509
I'm going to justify
why we need to

937
00:39:02,509 --> 00:39:05,909
start why we need to
study paraldition.

938
00:39:05,909 --> 00:39:07,949
Then I'm going to talk about

939
00:39:07,949 --> 00:39:10,589
machine learning parallelism and

940
00:39:10,589 --> 00:39:12,630
how to talk about
connective communication

941
00:39:12,630 --> 00:39:15,130
because it's a cornerstone for
machine learning paralism.

942
00:39:15,130 --> 00:39:18,189
Okay? And then we start
talking about and study

943
00:39:18,189 --> 00:39:19,530
all this data model

944
00:39:19,530 --> 00:39:22,709
interoperator interrupted
palism we wrap up,

945
00:39:22,709 --> 00:39:27,514
uh, by reviewing some latest
work on automatic parallon.

946
00:39:27,514 --> 00:39:31,899
Okay. Cool. So why
we need piston?

947
00:39:31,899 --> 00:39:33,619
I think we basically
need to come back

948
00:39:33,619 --> 00:39:35,700
to our mainstream story.

949
00:39:35,700 --> 00:39:39,280
More slow. So, I think we should

950
00:39:39,280 --> 00:39:40,839
do this when we start
talking about when

951
00:39:40,839 --> 00:39:42,639
we start justify accelerators.

952
00:39:42,639 --> 00:39:46,599
Okay. So 20 years ago, yes,

953
00:39:46,599 --> 00:39:48,159
we are still having
the more slow,

954
00:39:48,159 --> 00:39:50,979
but now it stopped, right?

955
00:39:50,979 --> 00:39:53,900
Okay. Meanwhile in
machinery industry,

956
00:39:53,900 --> 00:39:57,649
what happened is, Okay.

957
00:39:57,649 --> 00:40:00,029
The model size, they
are basically ten times

958
00:40:00,029 --> 00:40:03,030
every 18 months. It's crazy.

959
00:40:03,030 --> 00:40:05,149
And this curve is
still going on.

960
00:40:05,149 --> 00:40:09,730
The latest TipSIg model is
600 biniar perimeters, okay.

961
00:40:09,730 --> 00:40:11,069
So basically the size of

962
00:40:11,069 --> 00:40:13,490
mercenary model is
growing crazily,

963
00:40:13,490 --> 00:40:15,630
why people start developing

964
00:40:15,630 --> 00:40:17,789
all these large models?
Two reasons why.

965
00:40:17,789 --> 00:40:20,919
One is because, um, by

966
00:40:20,919 --> 00:40:22,700
simply skiing the
size of the models,

967
00:40:22,700 --> 00:40:25,020
you are going to have a
way better performance.

968
00:40:25,020 --> 00:40:26,540
So here, the excess

969
00:40:26,540 --> 00:40:28,359
is the number of
parameters of the model.

970
00:40:28,359 --> 00:40:30,760
The Y excess is the
average performance

971
00:40:30,760 --> 00:40:33,660
of 20 benchmarks in
literal language,

972
00:40:33,660 --> 00:40:36,420
uh, uh, literal
language processing.

973
00:40:36,420 --> 00:40:38,080
So you don't care about
what the benchmark.

974
00:40:38,080 --> 00:40:41,299
Basically, we normalize
that into from 0% to 100%.

975
00:40:41,299 --> 00:40:44,700
Okay. 100% is
perfect performance.

976
00:40:44,700 --> 00:40:46,840
Zero is basically the
zero performance.

977
00:40:46,840 --> 00:40:48,659
And once we normalize
it you observe that

978
00:40:48,659 --> 00:40:50,739
when we skill the
model size, okay?

979
00:40:50,739 --> 00:40:51,999
And we try to measure

980
00:40:51,999 --> 00:40:53,920
the average performance on
all these 20 benchmarks,

981
00:40:53,920 --> 00:40:55,800
you will see basically

982
00:40:55,800 --> 00:40:57,820
the performance will
just keep increasing.

983
00:40:57,820 --> 00:41:00,399
Okay. Okay, this
gives hope, right,

984
00:41:00,399 --> 00:41:02,279
because if you can achieve 100%,

985
00:41:02,279 --> 00:41:03,819
you are basically achieving AI.

986
00:41:03,819 --> 00:41:06,979
True AGI. Okay. That's
the first reason.

987
00:41:06,979 --> 00:41:08,879
Okay, Skinny model size
can give you a lot of

988
00:41:08,879 --> 00:41:11,499
performance improvement.
That's simple.

989
00:41:11,499 --> 00:41:14,940
The second is people
start observing

990
00:41:14,940 --> 00:41:18,120
that big models have some
emergent capability,

991
00:41:18,120 --> 00:41:20,460
and I really like
this Jif produced,

992
00:41:20,460 --> 00:41:23,260
um, by Google, okay?

993
00:41:23,260 --> 00:41:27,240
So what is this Jif try to
illustrate is basically,

994
00:41:27,240 --> 00:41:29,825
when you continue to
skill the model sizes,

995
00:41:29,825 --> 00:41:32,510
Be able to do some very
complicated tasks.

996
00:41:32,510 --> 00:41:37,829
Okay. So this is kind of like
a leap pas leap forward.

997
00:41:37,829 --> 00:41:39,769
It's like a baby that
every few months,

998
00:41:39,769 --> 00:41:41,049
it's going to
develop a new skill.

999
00:41:41,049 --> 00:41:44,669
Suddenly, yeah. Okay. And
people indeed observe this.

1000
00:41:44,669 --> 00:41:46,289
Today, I think people
are already talking

1001
00:41:46,289 --> 00:41:49,770
about some very advanced
skills, reasoning,

1002
00:41:49,770 --> 00:41:51,309
When you skill your model in

1003
00:41:51,309 --> 00:41:53,449
some way and your
model start to reason,

1004
00:41:53,449 --> 00:41:58,429
start to solve this kind of
mathematical questions, okay?

1005
00:41:59,340 --> 00:42:02,819
No. I try to do is

1006
00:42:02,819 --> 00:42:05,719
basically I'm going to align
the two curves I should.

1007
00:42:05,719 --> 00:42:08,200
One is the model sizes.

1008
00:42:08,200 --> 00:42:11,339
The other is chips capability.

1009
00:42:11,339 --> 00:42:15,239
This is the model sizes,
ten X every 18 months.

1010
00:42:15,239 --> 00:42:17,979
This is the Morse law, you can

1011
00:42:17,979 --> 00:42:21,780
see the gap is widening,
a lot of widening,

1012
00:42:21,780 --> 00:42:24,539
Which means that Morse
law is not going to work,

1013
00:42:25,619 --> 00:42:27,900
Morse law is basically
characterized

1014
00:42:27,900 --> 00:42:30,179
in the development of CPU, okay.

1015
00:42:32,690 --> 00:42:37,949
Okay. I hope you still
remember my slide this slide,

1016
00:42:37,949 --> 00:42:39,110
right? So we have two options.

1017
00:42:39,110 --> 00:42:41,090
One is accelerator. The
other is quantum computing,

1018
00:42:41,090 --> 00:42:42,649
and quantum computing
is going on,

1019
00:42:42,649 --> 00:42:44,030
and we can basically project

1020
00:42:44,030 --> 00:42:46,689
our accelerator story
into that curve. Okay.

1021
00:42:46,689 --> 00:42:49,210
And if we project

1022
00:42:49,210 --> 00:42:52,589
the growth of the
capability of accelerators,

1023
00:42:52,589 --> 00:42:57,119
what happens is basically
still wide gap, right?

1024
00:42:57,119 --> 00:42:59,339
It's basically in the middle
of what we need and what

1025
00:42:59,339 --> 00:43:01,779
we actually what CPU
current currently offer.

1026
00:43:01,779 --> 00:43:04,560
Okay? So the growth
of the VD GPU,

1027
00:43:04,560 --> 00:43:05,979
the compute flops is still

1028
00:43:05,979 --> 00:43:08,699
not catching up, okay?
That's a problem.

1029
00:43:08,699 --> 00:43:11,420
And the TPU curve green curves

1030
00:43:11,420 --> 00:43:12,600
a little bit better than GPU.

1031
00:43:12,600 --> 00:43:18,460
Okay? Yeah. Cool. And
this gap is increasing,

1032
00:43:18,460 --> 00:43:21,639
uh, 256 times every 18 months.

1033
00:43:21,639 --> 00:43:23,659
Yeah. So basically,
all the accelerators,

1034
00:43:23,659 --> 00:43:24,820
if you use a single accelerator,

1035
00:43:24,820 --> 00:43:25,979
you're not going to catch up

1036
00:43:25,979 --> 00:43:28,540
with the model sizes
we need to develop.

1037
00:43:28,540 --> 00:43:31,900
Okay? And this is from
the compute perspective.

1038
00:43:31,900 --> 00:43:33,499
And you can also draw

1039
00:43:33,499 --> 00:43:35,440
this kind of curve on from
a memory perspective.

1040
00:43:35,440 --> 00:43:36,980
What happens is basically,

1041
00:43:36,980 --> 00:43:41,139
um, the memory increase
even crazier, okay?

1042
00:43:41,139 --> 00:43:42,979
So the memory that we

1043
00:43:42,979 --> 00:43:45,200
need to store the model to
train the model basically,

1044
00:43:45,200 --> 00:43:48,859
uh, it increase 35
times every two years.

1045
00:43:49,660 --> 00:43:53,900
But the GPU memory girls
is much flatter, okay?

1046
00:43:53,900 --> 00:43:57,859
So basically, there's no way
that you can catch up. Okay.

1047
00:43:58,260 --> 00:44:00,399
And the largest model fit on

1048
00:44:00,399 --> 00:44:02,279
single GB basically
we stop at BRT.

1049
00:44:02,279 --> 00:44:04,099
Okay. BRT is basically
the model that

1050
00:44:04,099 --> 00:44:06,940
we can store on single GI, okay?

1051
00:44:08,540 --> 00:44:11,539
And at some point, we
basically face the change.

1052
00:44:11,539 --> 00:44:13,140
That is we need more than 100

1053
00:44:13,140 --> 00:44:14,580
GPOs to just store
the parameters,

1054
00:44:14,580 --> 00:44:16,479
not mentioning those
openmeter states

1055
00:44:16,479 --> 00:44:18,209
and activations, okay?

1056
00:44:18,209 --> 00:44:21,419
Yeah, I hope this figure
this is real data, okay?

1057
00:44:21,419 --> 00:44:22,919
This real data, it's not like

1058
00:44:22,919 --> 00:44:24,679
I made up this, okay, real data.

1059
00:44:24,679 --> 00:44:26,160
And I hope it's basically

1060
00:44:26,160 --> 00:44:28,159
justify that there's
no way out, right?

1061
00:44:28,159 --> 00:44:31,140
The only way that we do is
basically we do part addition.

1062
00:44:31,140 --> 00:44:33,459
That's why we need to
study paraliztion, okay?

1063
00:44:33,459 --> 00:44:36,140
Cool. Then let's talk
about part addition.

1064
00:44:36,140 --> 00:44:38,059
Okay. So I think many

1065
00:44:38,059 --> 00:44:40,359
of you probably already
worked on this part, right?

1066
00:44:40,359 --> 00:44:41,860
Like, if you do Petro

1067
00:44:41,860 --> 00:44:43,719
you do whatever kind
of thing today,

1068
00:44:43,719 --> 00:44:46,359
you probably have already
started working with like

1069
00:44:46,359 --> 00:44:48,059
Petroc distributed
or Tinder flow

1070
00:44:48,059 --> 00:44:49,519
distributed kind
of library, right.

1071
00:44:49,519 --> 00:44:54,119
And, um, and I think most of
you when you work on this,

1072
00:44:54,119 --> 00:44:56,480
you are basically educated
with two concepts.

1073
00:44:56,480 --> 00:44:58,020
One is called data parism.

1074
00:44:58,020 --> 00:44:59,839
The other is called the Mtopism.

1075
00:44:59,839 --> 00:45:03,099
So what is data palism
and topolism? Okay?

1076
00:45:03,099 --> 00:45:06,199
I think the dataparism is very
easy to understand we have

1077
00:45:06,199 --> 00:45:07,099
been talking about that at

1078
00:45:07,099 --> 00:45:09,600
the accelerated levels
basically SIMD.

1079
00:45:09,600 --> 00:45:11,339
Uh I GPU you hold

1080
00:45:11,339 --> 00:45:13,899
many many cores you
give each part of data.

1081
00:45:13,899 --> 00:45:15,560
That is basically data parism.

1082
00:45:15,560 --> 00:45:17,319
And in this part of
the lecture, uh,

1083
00:45:17,319 --> 00:45:18,499
data paralisis means that you

1084
00:45:18,499 --> 00:45:19,700
have many many different GPUs,

1085
00:45:19,700 --> 00:45:22,894
you give each GPU a part
of data. It's the same.

1086
00:45:22,894 --> 00:45:26,670
So in data parism we
have many many GPUs.

1087
00:45:26,670 --> 00:45:29,329
Each GPU is probably installed
on some notes, right?

1088
00:45:29,329 --> 00:45:31,150
And we have a huge dataset.

1089
00:45:31,150 --> 00:45:33,369
And we basically split
the data set and we

1090
00:45:33,369 --> 00:45:36,070
give each GPU a paro dataset,

1091
00:45:36,070 --> 00:45:37,990
and we let them
proceed together,

1092
00:45:37,990 --> 00:45:40,989
okay, in parallel. Okay.

1093
00:45:40,989 --> 00:45:44,889
So here, Datapism actually has
a core assumption that is,

1094
00:45:44,889 --> 00:45:46,509
uh, the entire model needs to

1095
00:45:46,509 --> 00:45:48,515
fit on a single machine or GPU.

1096
00:45:48,515 --> 00:45:52,020
Okay. So what happens,

1097
00:45:52,020 --> 00:45:54,539
if the model does not
fit on a single machine?

1098
00:45:54,539 --> 00:45:56,939
That basically give us multipis

1099
00:45:56,939 --> 00:45:58,219
We need to party in the model.

1100
00:45:58,219 --> 00:46:01,339
Okay? So, meaning that

1101
00:46:01,339 --> 00:46:02,579
we need to paralyze the model

1102
00:46:02,579 --> 00:46:04,759
itself. So what is the model?

1103
00:46:06,150 --> 00:46:09,049
Because in this lecture, I
think we know what this model.

1104
00:46:09,049 --> 00:46:10,569
It's basically computer graph,

1105
00:46:10,569 --> 00:46:12,249
we define that comminute graph,

1106
00:46:12,249 --> 00:46:13,969
and we are going to come
back to that later.

1107
00:46:13,969 --> 00:46:17,590
But however, say,
this is a unla model.

1108
00:46:17,590 --> 00:46:19,229
It's basically a
lot of mat model,

1109
00:46:19,229 --> 00:46:20,850
many many non linear functions,

1110
00:46:20,850 --> 00:46:24,689
uh, like composing
complete computer graph.

1111
00:46:24,689 --> 00:46:28,349
Okay. So when we talk about, uh,

1112
00:46:28,349 --> 00:46:30,050
how we basically
partition these models,

1113
00:46:30,050 --> 00:46:31,430
we are talking about
how we partitioning

1114
00:46:31,430 --> 00:46:33,249
this kind of tensor
operators, right?

1115
00:46:33,249 --> 00:46:36,939
So intuitively, there
are a few ways, okay?

1116
00:46:36,939 --> 00:46:39,329
One is like we can cut
in the middle, right?

1117
00:46:39,329 --> 00:46:41,010
So say this general

1118
00:46:41,010 --> 00:46:42,610
has four layers, we
basically cut in the middle.

1119
00:46:42,610 --> 00:46:44,109
We give the first
GPU two layers,

1120
00:46:44,109 --> 00:46:46,149
the other GPU another
two layers, right?

1121
00:46:46,149 --> 00:46:48,689
This is a model type number one.

1122
00:46:48,689 --> 00:46:51,989
Okay. Another way is, um,

1123
00:46:51,989 --> 00:46:54,330
we can even pipeline execution

1124
00:46:54,330 --> 00:46:56,430
to make it even more
complicated but more efficient,

1125
00:46:56,430 --> 00:46:58,770
but we can talk about
this layer, okay?

1126
00:46:58,890 --> 00:47:04,569
Um, Okay. And what's the
second way of doing mod party?

1127
00:47:07,040 --> 00:47:09,499
So this is vertical cut right.

1128
00:47:09,499 --> 00:47:10,819
I can do horizontal cut.

1129
00:47:10,819 --> 00:47:12,960
Okay? Yeah, so it's
very intuitive.

1130
00:47:12,960 --> 00:47:14,239
I can also horizontal cut.

1131
00:47:14,239 --> 00:47:16,199
So for each layer, I'm
going to cut that layer

1132
00:47:16,199 --> 00:47:18,879
into different parts and
I give each GB a part.

1133
00:47:18,879 --> 00:47:20,879
Okay? This is another part

1134
00:47:20,879 --> 00:47:23,624
of another type of
mod partism, okay?

1135
00:47:23,624 --> 00:47:25,729
And you can see this
can become rather

1136
00:47:25,729 --> 00:47:27,749
complicated because
each layer probably has

1137
00:47:27,749 --> 00:47:29,709
different kind of operators and

1138
00:47:29,709 --> 00:47:31,789
each neural network
probably have

1139
00:47:31,789 --> 00:47:34,529
different kind of layers, right?

1140
00:47:34,529 --> 00:47:37,930
And we also have a
different type of cluster.

1141
00:47:37,930 --> 00:47:39,309
We could have a cluster with

1142
00:47:39,309 --> 00:47:41,450
eight GPUs installed
in one node.

1143
00:47:41,450 --> 00:47:43,509
We can also have a
pretty big cluster where

1144
00:47:43,509 --> 00:47:46,109
we have tens of
thousands of GPUs,

1145
00:47:46,109 --> 00:47:48,289
and they are distributed
in different ways.

1146
00:47:48,289 --> 00:47:49,709
Okay? And that will

1147
00:47:49,709 --> 00:47:51,690
basically make the
problem very complicated.

1148
00:47:51,690 --> 00:47:53,859
And that's what we are
going to dive deeper.

1149
00:47:53,859 --> 00:47:56,109
Okay. But intuitively, I think,

1150
00:47:56,109 --> 00:47:58,249
like I said, uh,

1151
00:47:58,249 --> 00:48:01,110
you have a classic view of
machine learning partism

1152
00:48:01,110 --> 00:48:05,470
where you classify pism into
data parts, Multi partism.

1153
00:48:05,470 --> 00:48:06,949
And I think next,
what I'm going to

1154
00:48:06,949 --> 00:48:08,529
do is I'm going to
map this kind of like

1155
00:48:08,529 --> 00:48:10,550
intuitive explanation of pism

1156
00:48:10,550 --> 00:48:14,549
into our definition of models
and data on compute, right?

1157
00:48:14,549 --> 00:48:16,690
Because we want to
approach this pism

1158
00:48:16,690 --> 00:48:21,110
from sure computation
and system perspective.

1159
00:48:21,110 --> 00:48:24,304
So we know what to do what
to deal with on system side.

1160
00:48:24,304 --> 00:48:26,739
Okay. Still recap, right?

1161
00:48:26,739 --> 00:48:29,540
This is the figure we use
in my second lecture,

1162
00:48:29,540 --> 00:48:32,019
where we have
neural networks and

1163
00:48:32,019 --> 00:48:33,740
this neural network
can be represented

1164
00:48:33,740 --> 00:48:35,039
using this master
equation, right?

1165
00:48:35,039 --> 00:48:37,900
Still remember the
definition of each symbol.

1166
00:48:37,900 --> 00:48:39,039
Okay.

1167
00:48:39,039 --> 00:48:42,420
So from a computational
perspective, what is partsm?

1168
00:48:42,420 --> 00:48:43,799
Okay? I think from

1169
00:48:43,799 --> 00:48:44,800
a computation perspective,

1170
00:48:44,800 --> 00:48:45,879
we care about two things, right?

1171
00:48:45,879 --> 00:48:47,760
One is how to compute.

1172
00:48:47,760 --> 00:48:50,800
How to compute the computer
graph. Second is memory.

1173
00:48:50,800 --> 00:48:52,520
I hope we spend so
many time on memory,

1174
00:48:52,520 --> 00:48:54,479
how to basically make sure

1175
00:48:54,479 --> 00:48:56,699
the computation will
subject to the peak memory.

1176
00:48:56,699 --> 00:48:59,200
Okay? So for computer,

1177
00:48:59,200 --> 00:49:00,639
if you look at
this equation, uh,

1178
00:49:00,639 --> 00:49:03,425
what are the elements
that require compute?

1179
00:49:03,425 --> 00:49:06,090
The first element
is this function,

1180
00:49:06,090 --> 00:49:07,550
which perform forward
and backward.

1181
00:49:07,550 --> 00:49:10,810
Okay? And when we
talk about parism,

1182
00:49:10,810 --> 00:49:11,849
we basically need to parts in

1183
00:49:11,849 --> 00:49:13,030
this compute function across

1184
00:49:13,030 --> 00:49:15,009
different number of dips. Okay?

1185
00:49:15,009 --> 00:49:18,810
A second part is
the F op matters,

1186
00:49:18,810 --> 00:49:23,049
right, where we apply
gradients, uh, two parameters.

1187
00:49:23,049 --> 00:49:24,830
And we need to
apply this function

1188
00:49:24,830 --> 00:49:26,229
F. We need to

1189
00:49:26,229 --> 00:49:28,550
perform this function F
across different dipts.

1190
00:49:28,550 --> 00:49:32,970
Okay? And this is the main
compute from this equation.

1191
00:49:32,970 --> 00:49:36,175
Okay, we are going to
study how to parallelism.

1192
00:49:36,175 --> 00:49:39,420
On the memory part, we
have two main sources

1193
00:49:39,420 --> 00:49:41,499
of consumption is D,

1194
00:49:41,499 --> 00:49:44,760
D is data, number batches.

1195
00:49:44,760 --> 00:49:49,019
And if you partition D, it
becomes data pismV clear.

1196
00:49:49,019 --> 00:49:53,660
Okay? And Theta, Theta is
basically the uh parameters.

1197
00:49:53,660 --> 00:49:55,619
Okay? We need to partition
parameters because we

1198
00:49:55,619 --> 00:49:58,160
cannot put the
model into one GPU.

1199
00:49:58,160 --> 00:50:00,130
The memory is not sufficient.

1200
00:50:00,130 --> 00:50:03,979
Okay. And one part that we
really need to study and take

1201
00:50:03,979 --> 00:50:07,819
care of is, um, communication.

1202
00:50:07,819 --> 00:50:09,620
Why communication is important

1203
00:50:09,620 --> 00:50:11,099
because on a single device,

1204
00:50:11,099 --> 00:50:12,299
you do have communication, but

1205
00:50:12,299 --> 00:50:15,400
communication basically happens
between memory hierarchy.

1206
00:50:15,400 --> 00:50:18,079
You communicate between, uh,

1207
00:50:18,079 --> 00:50:22,900
CPU memory to HPM SRM to
catch it to register.

1208
00:50:22,900 --> 00:50:24,819
Okay. And that is pure local.

1209
00:50:24,819 --> 00:50:28,560
So your device, uh your
GPU library like Coda,

1210
00:50:28,560 --> 00:50:30,339
they provide all this kind
of memory copy for you.

1211
00:50:30,339 --> 00:50:32,505
It's very easy to implement.

1212
00:50:32,505 --> 00:50:35,629
But when you go distributed
this different story,

1213
00:50:35,629 --> 00:50:38,150
you have to communicate
these things across

1214
00:50:38,150 --> 00:50:42,130
different GPUs and how
different dips get connected.

1215
00:50:42,130 --> 00:50:44,610
There is some interconnect
between devices,

1216
00:50:44,610 --> 00:50:46,629
and this interconnect could be,

1217
00:50:46,629 --> 00:50:50,589
medius Vlink if you
put GPUs in one box.

1218
00:50:50,589 --> 00:50:52,729
Or if you want to
continue to scale up,

1219
00:50:52,729 --> 00:50:55,550
you have to go
through the network.

1220
00:50:55,550 --> 00:50:58,450
Network, you have different
kind of networks.

1221
00:50:58,450 --> 00:51:02,810
You have Amazons,
like elastic fabric,

1222
00:51:02,810 --> 00:51:04,590
this kind of network technology.

1223
00:51:04,590 --> 00:51:06,110
You also have infinivn.

1224
00:51:06,110 --> 00:51:08,329
You have TCP IP kind of

1225
00:51:08,329 --> 00:51:11,890
basically many layers if you
have studied networking.

1226
00:51:11,890 --> 00:51:13,849
Then how to handle
communication.

1227
00:51:13,849 --> 00:51:17,029
And that is the main source
of complexity in poison.

1228
00:51:17,029 --> 00:51:19,369
So how to carefully handle
communication because we know

1229
00:51:19,369 --> 00:51:23,490
communication is much
slower than locality,

1230
00:51:23,490 --> 00:51:24,669
because locality, you just

1231
00:51:24,669 --> 00:51:26,290
moves between memory hierarchy.

1232
00:51:26,290 --> 00:51:27,710
But once you start
doing communication,

1233
00:51:27,710 --> 00:51:28,770
you have a high latency.

1234
00:51:28,770 --> 00:51:30,989
Okay? We have to handle that.

1235
00:51:30,989 --> 00:51:35,069
And how to handle communication
parameters and also, uh,

1236
00:51:35,069 --> 00:51:37,989
the communication activations,
and that depends on

1237
00:51:37,989 --> 00:51:41,729
how we partition the new
network, the computation graph.

1238
00:51:41,729 --> 00:51:45,969
Okay? So that'll be inside,

1239
00:51:45,969 --> 00:51:49,249
um, we know what do we
do at a high level.

1240
00:51:49,249 --> 00:51:51,390
Then we can basically
try to understand

1241
00:51:51,390 --> 00:51:53,950
data and multipaism
from this perspective,

1242
00:51:53,950 --> 00:51:55,310
from this computer perspective.

1243
00:51:55,310 --> 00:51:57,049
So for data parism,
what we do is

1244
00:51:57,049 --> 00:52:00,109
basically we are going
to partition the data.

1245
00:52:00,109 --> 00:52:02,650
We are going to piton the
data and we give each GPU

1246
00:52:02,650 --> 00:52:05,529
different data and they
compute on their own, right?

1247
00:52:05,529 --> 00:52:07,754
But the problem is, um,

1248
00:52:07,754 --> 00:52:11,259
each GPU is going to
compute on their own,

1249
00:52:11,259 --> 00:52:13,100
but at some point, they
need to synchronize.

1250
00:52:13,100 --> 00:52:15,959
Because otherwise, GPU
is basically computer on

1251
00:52:15,959 --> 00:52:17,499
their own pace and they are

1252
00:52:17,499 --> 00:52:19,299
not basically contributing
to the same job.

1253
00:52:19,299 --> 00:52:21,740
You are not making progress
through partition.

1254
00:52:21,740 --> 00:52:23,739
So I datapati what kind of thing

1255
00:52:23,739 --> 00:52:26,480
will need to communicate.

1256
00:52:29,160 --> 00:52:31,700
We are going to soon
figure out. So basically

1257
00:52:31,700 --> 00:52:32,779
the part that we
need to figure out

1258
00:52:32,779 --> 00:52:34,220
is how to communicate
the parameters

1259
00:52:34,220 --> 00:52:35,919
and ingredients, okay?

1260
00:52:35,919 --> 00:52:37,480
And the entire algorithm,

1261
00:52:37,480 --> 00:52:41,159
um, so basically, we
part in the data, okay?

1262
00:52:41,159 --> 00:52:43,239
We give HGPU a different data

1263
00:52:43,239 --> 00:52:45,940
where we also replicate
the parameters.

1264
00:52:45,940 --> 00:52:49,040
So we give each GPU a
copy of the parameters.

1265
00:52:49,040 --> 00:52:50,999
Okay? And then GPU

1266
00:52:50,999 --> 00:52:52,940
is going to take the
copy the parameters,

1267
00:52:52,940 --> 00:52:55,899
and also a copy of the model
code, of course, okay.

1268
00:52:55,899 --> 00:52:58,300
And it's going to
apply this computation

1269
00:52:58,300 --> 00:53:01,369
on that specific
designated batch of data.

1270
00:53:01,369 --> 00:53:03,799
And once we do this,
what we'll produce?

1271
00:53:03,799 --> 00:53:05,979
We will produce a copy
of gradients, right?

1272
00:53:05,979 --> 00:53:10,000
And remember in
this optimization,

1273
00:53:10,000 --> 00:53:11,059
what we do is we need to apply

1274
00:53:11,059 --> 00:53:13,659
the gradients into
primeters, right?

1275
00:53:13,659 --> 00:53:15,359
So HTPU could apply

1276
00:53:15,359 --> 00:53:17,080
the gradients independently
to the parameters.

1277
00:53:17,080 --> 00:53:19,300
But it does not make
sense, because otherwise,

1278
00:53:19,300 --> 00:53:21,579
EHPU is doing, um,

1279
00:53:21,579 --> 00:53:24,180
in other way, GPU is basically
doing their own communion.

1280
00:53:24,180 --> 00:53:25,459
There's no
communication. So what

1281
00:53:25,459 --> 00:53:26,800
do we do is basically
in daal purism,

1282
00:53:26,800 --> 00:53:28,240
we are going to
synchronize the gradients.

1283
00:53:28,240 --> 00:53:30,840
Okay? We are going to have
a communication mechanism

1284
00:53:30,840 --> 00:53:32,360
where we are going
to connect gradients

1285
00:53:32,360 --> 00:53:34,019
from all the GPUs.

1286
00:53:34,019 --> 00:53:37,390
We accumulate them, and then
we apply to the parameters.

1287
00:53:37,390 --> 00:53:40,060
Okay. And once we apply
to the parameters,

1288
00:53:40,060 --> 00:53:41,680
we are going to get a new
copy of the parameters

1289
00:53:41,680 --> 00:53:43,719
and we basically redistribute

1290
00:53:43,719 --> 00:53:45,339
that copy of parameters to DPS,

1291
00:53:45,339 --> 00:53:46,640
and we start next iteration.

1292
00:53:46,640 --> 00:53:49,200
Okay? This is data partarism.

1293
00:53:49,200 --> 00:53:52,819
And I Mdparism things becomes
more complicated, okay?

1294
00:53:52,819 --> 00:53:56,319
So, we don't partition
data anymore.

1295
00:53:56,319 --> 00:53:58,620
Okay? We are going to
partition the model.

1296
00:53:58,620 --> 00:54:01,619
Uh Partitioning model is
very, very weak, right?

1297
00:54:01,619 --> 00:54:03,319
So what do I mean
by parison model?

1298
00:54:03,319 --> 00:54:04,599
There are many many
possibilities.

1299
00:54:04,599 --> 00:54:05,720
One is I'm going to partition

1300
00:54:05,720 --> 00:54:07,700
the model parameters,
which is the Theta.

1301
00:54:07,700 --> 00:54:09,380
The other one is I'm
going to petition

1302
00:54:09,380 --> 00:54:11,660
the competition, which
is that function.

1303
00:54:11,660 --> 00:54:14,200
Okay. Lambda L, okay?

1304
00:54:15,450 --> 00:54:18,909
The problem here is how
do we deal with data?

1305
00:54:18,909 --> 00:54:21,109
Right? Because in multiparism,

1306
00:54:21,109 --> 00:54:22,189
I don't know how
to deal with data.

1307
00:54:22,189 --> 00:54:23,550
I could parts or not partition.

1308
00:54:23,550 --> 00:54:25,489
Okay. We are going
to figure it out.

1309
00:54:25,489 --> 00:54:27,250
And, of course, we
need to partition

1310
00:54:27,250 --> 00:54:29,269
optiber states because we
already part in part of

1311
00:54:29,269 --> 00:54:31,329
the model and we have to apply

1312
00:54:31,329 --> 00:54:34,490
the partial gradients into
the part of parameters.

1313
00:54:34,490 --> 00:54:37,789
Okay, we partition. Okay.

1314
00:54:37,789 --> 00:54:39,689
Uh, that'll give
you a big picture,

1315
00:54:39,689 --> 00:54:41,130
right? It's very complicated.

1316
00:54:41,130 --> 00:54:43,090
Okay. You have so many
choices to petition,

1317
00:54:43,090 --> 00:54:46,250
and you also subject
to networking latency.

1318
00:54:46,250 --> 00:54:47,730
You subject to
memory constraints,

1319
00:54:47,730 --> 00:54:48,709
you subject to whatever,

1320
00:54:48,709 --> 00:54:51,789
like, uh, well, make
your job slow, okay?

1321
00:54:51,789 --> 00:54:53,609
You have to make sure, uh,

1322
00:54:53,609 --> 00:54:56,689
you petition in a smart way
that your job is not slow.

1323
00:54:56,689 --> 00:54:59,189
Okay? This is part of the issue.

1324
00:54:59,420 --> 00:55:01,519
Okay, now, I'm going to

1325
00:55:01,519 --> 00:55:03,039
basically start
talking about how we

1326
00:55:03,039 --> 00:55:06,199
make this entire picture
much more clear.

1327
00:55:06,199 --> 00:55:11,459
Okay. So I hope you still
remember this figure, right?

1328
00:55:11,459 --> 00:55:14,779
So we can represent our model
using a computer graph,

1329
00:55:14,779 --> 00:55:16,579
and this computer
graph basically is

1330
00:55:16,579 --> 00:55:18,620
composed of three parts forward,

1331
00:55:18,620 --> 00:55:19,880
backward and with update.

1332
00:55:19,880 --> 00:55:23,620
Okay. And eventually we'll
end up with this graph.

1333
00:55:23,620 --> 00:55:26,180
Okay. So what is paradisi?

1334
00:55:26,180 --> 00:55:29,379
So if you only talk about if you

1335
00:55:29,379 --> 00:55:31,239
think that paradiing
at the model level,

1336
00:55:31,239 --> 00:55:32,459
you're not going
to figure it out.

1337
00:55:32,459 --> 00:55:33,560
Okay, it's too complicated

1338
00:55:33,560 --> 00:55:35,179
because there are so many
different kind of models.

1339
00:55:35,179 --> 00:55:37,340
So now, let's basically
change your mindset.

1340
00:55:37,340 --> 00:55:39,159
We forget about the model.

1341
00:55:39,159 --> 00:55:41,099
Okay? All the model are
basically computer graph.

1342
00:55:41,099 --> 00:55:43,179
Okay. I'm going to give
you a computer graph,

1343
00:55:43,179 --> 00:55:45,579
you just paradi it.
That's paradion.

1344
00:55:45,579 --> 00:55:47,279
Okay? So in this lecture,

1345
00:55:47,279 --> 00:55:49,919
what I'm going to the way
I taught I taught is,

1346
00:55:49,919 --> 00:55:51,340
I'm going to teach parodies

1347
00:55:51,340 --> 00:55:55,539
basically I'm going to give
you this computer graph.

1348
00:55:55,539 --> 00:55:59,359
I'm also going to give you
a device cluster like this.

1349
00:55:59,359 --> 00:56:02,539
I think cluster is
very easy to read,

1350
00:56:02,539 --> 00:56:04,160
especially given
to this cluster.

1351
00:56:04,160 --> 00:56:07,099
For example, this is
the medius DGX box,

1352
00:56:07,099 --> 00:56:08,520
and this is V 100,

1353
00:56:08,520 --> 00:56:10,959
but basically E 100
and H 100 is the same.

1354
00:56:10,959 --> 00:56:15,469
They are connected in a way
where you can see this GPU.

1355
00:56:15,469 --> 00:56:20,219
Okay. And HGPU basically
has a multi way link,

1356
00:56:20,219 --> 00:56:23,660
this Mink is very
high bandwidth.

1357
00:56:23,660 --> 00:56:27,699
Meaning that HGPU when they
talk to another GPU inside of

1358
00:56:27,699 --> 00:56:29,199
this box is

1359
00:56:29,199 --> 00:56:32,119
very high bandwidth
communication. Very very fast.

1360
00:56:32,119 --> 00:56:33,880
It's almost like a
memory hierarchy,

1361
00:56:33,880 --> 00:56:35,584
but just a little bit slower.

1362
00:56:35,584 --> 00:56:38,869
Okay. But if you
draw another box

1363
00:56:38,869 --> 00:56:40,650
here and if this GPU

1364
00:56:40,650 --> 00:56:42,749
wants to communicate
with another GPU here,

1365
00:56:42,749 --> 00:56:44,869
the problem exists, they have

1366
00:56:44,869 --> 00:56:47,270
to go through this Internet.

1367
00:56:47,270 --> 00:56:49,510
They have to move
things from GPU memory

1368
00:56:49,510 --> 00:56:51,089
to CP memory and let

1369
00:56:51,089 --> 00:56:56,010
the CPU to send those contents
through this Internet.

1370
00:56:56,010 --> 00:56:58,370
And you can see the scale of
the communication bandwidth.

1371
00:56:58,370 --> 00:57:00,130
Internet is almost
like ten to 100

1372
00:57:00,130 --> 00:57:02,260
times slower than link.

1373
00:57:02,260 --> 00:57:05,089
Okay. And maybe I

1374
00:57:05,089 --> 00:57:06,810
can make this a little
bit more abstract,

1375
00:57:06,810 --> 00:57:09,250
that is, we have this
kind of cluster.

1376
00:57:09,250 --> 00:57:10,949
Here I have four nodes, okay?

1377
00:57:10,949 --> 00:57:12,949
One, two, three, four, right?

1378
00:57:12,949 --> 00:57:16,229
And basically each node has
a few GPUs here I give four,

1379
00:57:16,229 --> 00:57:18,469
but typically today
is eight, okay.

1380
00:57:18,469 --> 00:57:21,430
These GPUs are basically
have a fast connection,

1381
00:57:21,430 --> 00:57:23,949
which I use green
green line here.

1382
00:57:23,949 --> 00:57:25,749
And if you want to
communicate between

1383
00:57:25,749 --> 00:57:28,710
any GPU that is from
different boxes,

1384
00:57:28,710 --> 00:57:31,250
you have to subject to
the slow connection.

1385
00:57:31,250 --> 00:57:35,949
Okay. And with this repenton
of this device cluster, um,

1386
00:57:35,949 --> 00:57:38,469
then our problem is
become a little bit more,

1387
00:57:38,469 --> 00:57:41,189
I would say, nicer
than previous version.

1388
00:57:41,189 --> 00:57:43,419
That is, uh Okay.

1389
00:57:43,419 --> 00:57:45,060
So part of this basically equals

1390
00:57:45,060 --> 00:57:48,280
to partitioning computational
graph on device Claster.

1391
00:57:48,280 --> 00:57:50,259
Okay? On the left hand side,

1392
00:57:50,259 --> 00:57:51,419
we are given committer graph.

1393
00:57:51,419 --> 00:57:52,839
Okay? On the right hand side,

1394
00:57:52,839 --> 00:57:54,079
we are given another graph, but

1395
00:57:54,079 --> 00:57:55,559
this graph is not
a computer graph.

1396
00:57:55,559 --> 00:57:57,779
It's basically graph
of devices. Okay.

1397
00:57:57,779 --> 00:58:00,559
And here, I don't use graph,

1398
00:58:00,559 --> 00:58:02,119
but you can understand
it that way, okay?

1399
00:58:02,119 --> 00:58:04,339
And this graph has a very

1400
00:58:04,339 --> 00:58:07,499
unique and very determined
characteristics

1401
00:58:07,499 --> 00:58:09,779
that is inside of one box,

1402
00:58:09,779 --> 00:58:13,900
it's fast connection and
between boxes store connection.

1403
00:58:13,900 --> 00:58:15,480
And our problem is basically

1404
00:58:15,480 --> 00:58:18,540
how to partition commuter
graph on the device clasor.

1405
00:58:18,540 --> 00:58:21,060
Okay? Problem defined.

1406
00:58:21,060 --> 00:58:22,719
Okay. Any question?

1407
00:58:22,719 --> 00:58:26,239
Of course,

1408
00:58:26,239 --> 00:58:28,519
you need to subject to
memory constraints, right?

1409
00:58:28,519 --> 00:58:30,599
You need to subject
to, for example,

1410
00:58:30,599 --> 00:58:33,659
communicating bandwis.
And you want your job.

1411
00:58:33,659 --> 00:58:36,240
Once you partition this graph
on this device cluster,

1412
00:58:36,240 --> 00:58:38,059
you want your job to run as

1413
00:58:38,059 --> 00:58:40,300
fast as possible
without auto memory.

1414
00:58:40,300 --> 00:58:44,819
Okay? Cool. With this
defined problem,

1415
00:58:44,819 --> 00:58:46,299
let's start talking about a

1416
00:58:46,299 --> 00:58:48,359
little bit on how
to partin this.

1417
00:58:48,359 --> 00:58:50,279
Okay, here, I'm going to

1418
00:58:50,279 --> 00:58:51,840
start talking about
how to paten graphs.

1419
00:58:51,840 --> 00:58:53,139
To simpfy a little bit,

1420
00:58:53,139 --> 00:58:55,219
I'm going to give you a
very simple graph, okay?

1421
00:58:55,219 --> 00:58:57,649
So here in this
graph, you have, um,

1422
00:58:57,649 --> 00:59:00,059
this is the one we used in

1423
00:59:00,059 --> 00:59:02,620
the previous lecture it
basically single layer MLP.

1424
00:59:02,620 --> 00:59:06,320
Okay? Uh, two met
m1u and one MSC,

1425
00:59:06,320 --> 00:59:09,259
two is W one, W two,
and one input X.

1426
00:59:09,259 --> 00:59:11,999
But compared to my
previous lecture,

1427
00:59:11,999 --> 00:59:14,179
here, I'm going to introduce
a little bit more rotation.

1428
00:59:14,179 --> 00:59:16,899
I'm going to give
each node a color.

1429
00:59:16,899 --> 00:59:19,639
So here I have a
cluster of two nodes,

1430
00:59:19,639 --> 00:59:21,239
Device one and device two,

1431
00:59:21,239 --> 00:59:22,979
and blue is the first device

1432
00:59:22,979 --> 00:59:26,799
and pink is the second device,

1433
00:59:26,799 --> 00:59:30,239
if a node is one color,

1434
00:59:30,239 --> 00:59:31,900
that means that
node that operator,

1435
00:59:31,900 --> 00:59:33,979
that computation
happens on that device.

1436
00:59:33,979 --> 00:59:38,500
Okay? Does that make sense?
Cool. Then this notation,

1437
00:59:38,500 --> 00:59:40,019
we can start
partitioning this graph,

1438
00:59:40,019 --> 00:59:41,840
and we can use
notation to notate,

1439
00:59:41,840 --> 00:59:43,534
what do we do on the
partitioning bar?

1440
00:59:43,534 --> 00:59:47,390
Okay. This is our
first strategy.

1441
00:59:47,390 --> 00:59:49,050
If you still remember

1442
00:59:49,050 --> 00:59:51,590
a few slides before,
I cut in the middle.

1443
00:59:51,590 --> 00:59:54,089
I cut in between
different layers,

1444
00:59:54,089 --> 00:59:55,689
I give the first two layers to

1445
00:59:55,689 --> 00:59:58,990
one device and another two
layers in the second device.

1446
00:59:58,990 --> 01:00:01,929
So here, if I reflect
that part in strategy is

1447
01:00:01,929 --> 01:00:04,890
basically this one.
I cut in the middle.

1448
01:00:04,890 --> 01:00:08,289
I give the first met
to the blue device,

1449
01:00:08,289 --> 01:00:12,750
and I give basically
the due and second memo

1450
01:00:12,750 --> 01:00:14,949
to the second device. I
cut it in the middle.

1451
01:00:14,949 --> 01:00:18,929
I use different color.
Strategy one, okay.

1452
01:00:19,070 --> 01:00:23,350
And I can basically do
slightly different,

1453
01:00:23,350 --> 01:00:26,349
for example, I can
do a horizontal cut.

1454
01:00:26,349 --> 01:00:28,529
I put the weight W one,

1455
01:00:28,529 --> 01:00:30,109
W two on first device,

1456
01:00:30,109 --> 01:00:33,109
and I put the rest
on on second device.

1457
01:00:33,109 --> 01:00:35,309
So remember, when we cut in

1458
01:00:35,309 --> 01:00:39,009
the middle when those two
nodes where the halfway at,

1459
01:00:39,009 --> 01:00:41,090
if they are on different
devices, what would happen?

1460
01:00:41,090 --> 01:00:42,449
They need to communicate, right?

1461
01:00:42,449 --> 01:00:44,649
They need to communicate.
And we're going to,

1462
01:00:44,649 --> 01:00:46,529
with this part again, but

1463
01:00:46,529 --> 01:00:48,745
let's assume that we
know how to communicate.

1464
01:00:48,745 --> 01:00:53,059
Okay. So I would

1465
01:00:53,059 --> 01:00:54,699
say the two strategy
is the same because

1466
01:00:54,699 --> 01:00:56,999
I actually basically
cut those notes,

1467
01:00:56,999 --> 01:00:59,560
I put different notes
on different devices.

1468
01:00:59,560 --> 01:01:02,340
So could you give me
a different strategy?

1469
01:01:09,100 --> 01:01:11,939
So if you remember in
my previous slides

1470
01:01:11,939 --> 01:01:13,639
in intuitive slide,

1471
01:01:13,639 --> 01:01:16,279
I also can cut
horital card I cut

1472
01:01:16,279 --> 01:01:19,660
a layer into half or
one half network.

1473
01:01:19,660 --> 01:01:21,899
So for that card,

1474
01:01:21,899 --> 01:01:25,539
does that card correspond
to one of these two?

1475
01:01:26,590 --> 01:01:28,669
One, you think is one?

1476
01:01:28,669 --> 01:01:31,069
No, it's not one,
because in other cut,

1477
01:01:31,069 --> 01:01:33,749
I basically cut the metmo I
cut the memo into two parts.

1478
01:01:33,749 --> 01:01:35,009
And in these two figures, I

1479
01:01:35,009 --> 01:01:36,709
actually don't cut memo at all.

1480
01:01:36,709 --> 01:01:40,309
The metamo is always perform
as a whole on one device.

1481
01:01:40,309 --> 01:01:42,890
Which means that if I
try to cut operators,

1482
01:01:42,890 --> 01:01:45,749
I actually have a different
strategy like this.

1483
01:01:45,950 --> 01:01:49,329
So here I use this notation a

1484
01:01:49,329 --> 01:01:52,729
little bit more intuitively,

1485
01:01:52,729 --> 01:01:54,869
that is I'm going to cut

1486
01:01:54,869 --> 01:01:56,589
operator I'm going to give

1487
01:01:56,589 --> 01:01:59,090
the operator half and
half different colors,

1488
01:01:59,090 --> 01:02:01,109
and one half is performed on

1489
01:02:01,109 --> 01:02:02,569
one device and then
the other half

1490
01:02:02,569 --> 01:02:04,330
is performing different devices.

1491
01:02:04,330 --> 01:02:07,049
So you figure out how
to parse that one.

1492
01:02:07,049 --> 01:02:08,989
Uh, if you look at this part,

1493
01:02:08,989 --> 01:02:12,169
it means that for
this met more, okay?

1494
01:02:12,169 --> 01:02:13,730
I'm going to cut in the middle.

1495
01:02:13,730 --> 01:02:16,029
I put the first half of the
met more on one device,

1496
01:02:16,029 --> 01:02:18,050
the second half on
second devices.

1497
01:02:18,050 --> 01:02:19,970
Be I cut metam

1498
01:02:19,970 --> 01:02:22,309
the input and output also
need to be cut, right?

1499
01:02:22,309 --> 01:02:24,110
So because I want to perform

1500
01:02:24,110 --> 01:02:26,129
the first half of the
metam which is mat,

1501
01:02:26,129 --> 01:02:28,289
I want to perform mat
on the first device.

1502
01:02:28,289 --> 01:02:30,949
So I have to also cut the
input into two half, right?

1503
01:02:30,949 --> 01:02:32,689
And this part is
going to perform Mt.

1504
01:02:32,689 --> 01:02:34,789
Okay. And the second device

1505
01:02:34,789 --> 01:02:36,229
is going to perform more. Okay.

1506
01:02:36,229 --> 01:02:38,870
So I also need to
cut this output.

1507
01:02:38,870 --> 01:02:40,769
Sorry, another part
of input, right?

1508
01:02:40,769 --> 01:02:42,969
And you're going to
perform the pink bar.

1509
01:02:42,969 --> 01:02:44,669
Okay. Makes sense, right?

1510
01:02:44,669 --> 01:02:48,229
Cool. Is there a different way?

1511
01:02:48,500 --> 01:02:52,339
Of course, I can do a horizontal
cart at operating level.

1512
01:02:52,339 --> 01:02:54,419
So sorry this color
is a little bit,

1513
01:02:54,419 --> 01:02:56,979
uh, not working, but you get it.

1514
01:02:56,979 --> 01:02:58,679
I can basically cut
in this way, right?

1515
01:02:58,679 --> 01:03:01,179
Yeah. And so for

1516
01:03:01,179 --> 01:03:03,840
Metamor I can cut basically
following different axes,

1517
01:03:03,840 --> 01:03:05,559
a, it will result into

1518
01:03:05,559 --> 01:03:08,620
different kind of
parting strategies.

1519
01:03:08,660 --> 01:03:12,419
Okay. Any question
on this slide?

1520
01:03:15,500 --> 01:03:18,659
Cool. So here, I'm going to

1521
01:03:18,659 --> 01:03:21,000
introduce two slightly
different strategy.

1522
01:03:21,000 --> 01:03:23,099
Okay? So in the first row,

1523
01:03:23,099 --> 01:03:24,700
I'm basically
applying a strategy

1524
01:03:24,700 --> 01:03:26,260
where I don't cut operators.

1525
01:03:26,260 --> 01:03:28,539
I only cut the graph. Okay?

1526
01:03:28,539 --> 01:03:32,580
And I call this strategy
interoperatd pism.

1527
01:03:32,580 --> 01:03:35,219
Uh, you can infer the
meaning of this lame, right?

1528
01:03:35,219 --> 01:03:36,739
Interoperator parism
basically means that

1529
01:03:36,739 --> 01:03:38,360
I never cut an operator.

1530
01:03:38,360 --> 01:03:39,579
I basically only cut the graph.

1531
01:03:39,579 --> 01:03:41,239
I assign different nodes

1532
01:03:41,239 --> 01:03:43,080
of the graph to
different devices.

1533
01:03:43,080 --> 01:03:46,340
And in the second row, I called
the interoperative pism.

1534
01:03:46,340 --> 01:03:47,720
That is, I'm going to
touch the operator.

1535
01:03:47,720 --> 01:03:49,679
I'm going to perform
one operator on

1536
01:03:49,679 --> 01:03:51,199
two different
devices and I figure

1537
01:03:51,199 --> 01:03:53,059
out how to aggregate
results later.

1538
01:03:53,059 --> 01:03:55,779
Okay. Um, you can

1539
01:03:55,779 --> 01:03:58,119
see the definition of
interoperative prism and

1540
01:03:58,119 --> 01:04:01,159
inter operative parism is
much I would say much more or

1541
01:04:01,159 --> 01:04:04,879
less uh ambiguous compared
to data and motpoism.

1542
01:04:04,879 --> 01:04:06,679
Why? I motoparism when I

1543
01:04:06,679 --> 01:04:08,539
talk about Mtoparism,
don't know what I mean.

1544
01:04:08,539 --> 01:04:09,399
You probably don't know,

1545
01:04:09,399 --> 01:04:12,019
because I can mean
many many things.

1546
01:04:12,019 --> 01:04:13,399
But when I talk about inter

1547
01:04:13,399 --> 01:04:15,560
and interoperats,
it's very precise.

1548
01:04:15,560 --> 01:04:18,060
Interbasically means, I'm
not going to cut operator,

1549
01:04:18,060 --> 01:04:20,359
but intra means I'm
going to do that.

1550
01:04:20,359 --> 01:04:23,219
Okay. Then let me
ask you a question.

1551
01:04:23,219 --> 01:04:26,800
So is datapism inter or
intraoperative parism?

1552
01:04:26,800 --> 01:04:36,219
Please. Is Intra Inter?

1553
01:04:36,219 --> 01:04:39,759
Intra? Yes, Datab intra Why?

1554
01:04:39,759 --> 01:04:43,179
Because you cut X
and X is your node,

1555
01:04:43,179 --> 01:04:45,899
and you indeed cut
X X is your input.

1556
01:04:45,899 --> 01:04:47,899
You cut batches, so it's intra.

1557
01:04:47,899 --> 01:04:50,679
Okay. We're going to come
back to this again again.

1558
01:04:50,679 --> 01:04:51,840
Okay? We are going to explore

1559
01:04:51,840 --> 01:04:53,819
all different strategies
of Inter intra.

1560
01:04:53,819 --> 01:04:55,099
We are going to reason it trade

1561
01:04:55,099 --> 01:04:57,960
off, how much it communicate.

1562
01:04:57,960 --> 01:05:01,020
How many memory it needs, okay?

1563
01:05:02,790 --> 01:05:05,869
Okay. And given these
definitions, uh,

1564
01:05:05,869 --> 01:05:09,709
let me expand intra and
intraparism a little bit.

1565
01:05:09,709 --> 01:05:12,549
So for matrix, we can
actually partition it along,

1566
01:05:12,549 --> 01:05:15,929
um, we can do row parison, okay?

1567
01:05:15,929 --> 01:05:17,390
We can also do column pion.

1568
01:05:17,390 --> 01:05:19,070
Sorry, this color is different,

1569
01:05:19,070 --> 01:05:21,109
it's not working.
I'm going to fix it.

1570
01:05:21,109 --> 01:05:22,870
But what I mean, basically,

1571
01:05:22,870 --> 01:05:25,369
you can cut it in this way or
you can cut it in this way.

1572
01:05:25,369 --> 01:05:27,750
Okay. I call this row partition,

1573
01:05:27,750 --> 01:05:30,329
I call this column
partition. Okay.

1574
01:05:30,329 --> 01:05:33,429
So I'm going to

1575
01:05:33,429 --> 01:05:35,409
introduce a third definition
and a third color,

1576
01:05:35,409 --> 01:05:37,489
which I could replicate it.

1577
01:05:37,489 --> 01:05:39,609
Which means that
I'm not cutting.

1578
01:05:39,609 --> 01:05:42,150
Okay. I'm basically
replicating this tenser.

1579
01:05:42,150 --> 01:05:43,929
Okay. If this color is green,

1580
01:05:43,929 --> 01:05:46,410
it means that I'm replicating
this note on both devices.

1581
01:05:46,410 --> 01:05:48,629
So each device will
have a copy of it.

1582
01:05:48,629 --> 01:05:52,580
Okay. So following
this location,

1583
01:05:52,580 --> 01:05:54,759
we can visualize more
parlement strategies. Okay.

1584
01:05:54,759 --> 01:05:57,020
And next, I'm going to basically

1585
01:05:57,020 --> 01:06:00,080
visualize a few notable
parliament strategies.

1586
01:06:00,080 --> 01:06:02,679
And basically each
strategy I'm going

1587
01:06:02,679 --> 01:06:05,339
to visualize next is
very famous paper,

1588
01:06:05,339 --> 01:06:07,320
more than 1,000 stations, okay?

1589
01:06:07,320 --> 01:06:09,739
And they are being
adopted in training.

1590
01:06:09,739 --> 01:06:12,639
The reason I do this is
because I want to show you

1591
01:06:12,639 --> 01:06:14,299
that we can use

1592
01:06:14,299 --> 01:06:16,319
this repenton to basically
illustrate many,

1593
01:06:16,319 --> 01:06:18,940
many different concepts, okay?

1594
01:06:20,340 --> 01:06:27,900
So the first one. Um, plea you
look at it for 10 seconds,

1595
01:06:27,900 --> 01:06:31,739
and I'm going to ask
you what what this is.

1596
01:06:36,220 --> 01:06:39,059
Anyone want to answer?

1597
01:06:42,060 --> 01:06:44,699
I actually talked
about this right now.

1598
01:06:44,699 --> 01:06:46,920
What is this? Yeah,
data parison.

1599
01:06:46,920 --> 01:06:49,060
Okay. You got it.
Why is data parison?

1600
01:06:49,060 --> 01:06:51,139
Because I have two
devices, right?

1601
01:06:51,139 --> 01:06:53,139
I replicate the width, right?

1602
01:06:53,139 --> 01:06:56,199
Which means that each
device has a copy of width.

1603
01:06:56,199 --> 01:06:57,959
And I part in the input.

1604
01:06:57,959 --> 01:06:59,480
Because I part in the input,

1605
01:06:59,480 --> 01:07:02,819
so this metamo is going to
be partin as well, right?

1606
01:07:02,819 --> 01:07:05,620
All the way like propagated
through this neural network.

1607
01:07:05,620 --> 01:07:08,699
So each device is going to
calculate different loss.

1608
01:07:08,699 --> 01:07:12,979
And I skip the update
green update part,

1609
01:07:12,979 --> 01:07:14,379
but we'll come back
to that later.

1610
01:07:14,379 --> 01:07:16,759
Okay? This is data
parison, okay.

1611
01:07:16,759 --> 01:07:19,279
I hope you basically
after my lecture,

1612
01:07:19,279 --> 01:07:20,939
you go back and try to

1613
01:07:20,939 --> 01:07:23,520
internalize this repetition
because it's very important.

1614
01:07:23,520 --> 01:07:26,200
We are going to make this
arbitrarily complicated.

1615
01:07:26,200 --> 01:07:29,499
Okay? How about this one?

1616
01:07:32,460 --> 01:07:36,259
Anyone work on M No, this one.

1617
01:07:37,100 --> 01:07:41,519
Sorry. Is is one
type of modoparism.

1618
01:07:41,519 --> 01:07:46,239
What is that type? Okay? This is

1619
01:07:46,239 --> 01:07:48,639
the one I give it to
you in reading megaton.

1620
01:07:48,639 --> 01:07:50,539
Okay. And typically, people call

1621
01:07:50,539 --> 01:07:52,760
this kind of modopoism
tensor partism.

1622
01:07:52,760 --> 01:07:55,239
Okay? It's called
tensor partism it is

1623
01:07:55,239 --> 01:07:58,939
a typical pism used in today's
language model training.

1624
01:07:58,939 --> 01:08:01,299
And one thing you'll
notice that is

1625
01:08:01,299 --> 01:08:03,060
here instead of replicating

1626
01:08:03,060 --> 01:08:05,080
weights we are
replicating the input.

1627
01:08:05,080 --> 01:08:07,319
And basically, each device

1628
01:08:07,319 --> 01:08:09,300
is going to get a
copy of the input,

1629
01:08:09,300 --> 01:08:11,979
and we are going to
patina weight. Okay?

1630
01:08:11,979 --> 01:08:13,799
And also because
we patina weight,

1631
01:08:13,799 --> 01:08:16,099
we are also going to
part in the meto, okay?

1632
01:08:16,099 --> 01:08:18,179
And then we also

1633
01:08:18,179 --> 01:08:21,199
propagate all the partitions
all the way to next met moo.

1634
01:08:21,199 --> 01:08:23,639
But for the next Mtmo we are
going to replicate the memo.

1635
01:08:23,639 --> 01:08:26,399
Okay. Yeah. Okay. I want

1636
01:08:26,399 --> 01:08:27,559
you to cross boons

1637
01:08:27,559 --> 01:08:29,139
this into the reading
I give it to you,

1638
01:08:29,139 --> 01:08:32,159
the required reading,
which is microtome paper.

1639
01:08:32,360 --> 01:08:40,839
How about this
one? Okay. This is

1640
01:08:40,839 --> 01:08:42,479
a little bit more abstract,
but I can tell you.

1641
01:08:42,479 --> 01:08:46,849
This one is essentially
pipeline parism

1642
01:08:46,849 --> 01:08:49,569
the thing that you notice
here is basically,

1643
01:08:49,569 --> 01:08:51,190
I don't do intraperparism,

1644
01:08:51,190 --> 01:08:52,329
I never parting operator.

1645
01:08:52,329 --> 01:08:54,889
But what I do is basically, um,

1646
01:08:54,889 --> 01:08:57,889
I part in this met M.
I cut in the middle.

1647
01:08:57,889 --> 01:08:59,749
I give each device
a different metmo.

1648
01:08:59,749 --> 01:09:01,449
Meanwhile, I pipeline execution

1649
01:09:01,449 --> 01:09:03,509
of metam using a long time.

1650
01:09:03,509 --> 01:09:05,749
Each time I'm going
to give a batch and

1651
01:09:05,749 --> 01:09:08,029
this batch first comes
into the first device.

1652
01:09:08,029 --> 01:09:10,489
And then once the first
device metm finished,

1653
01:09:10,489 --> 01:09:11,890
it will forward the computation

1654
01:09:11,890 --> 01:09:13,729
or the activation to
the second device.

1655
01:09:13,729 --> 01:09:15,249
Meanwhile, in the
next time step,

1656
01:09:15,249 --> 01:09:16,389
this device is going to fetch

1657
01:09:16,389 --> 01:09:18,849
another batch to the
parallel competion.

1658
01:09:18,849 --> 01:09:22,019
This is basically
pipeline parism. Okay.

1659
01:09:22,019 --> 01:09:25,139
How about this one? This
is even crazier, right?

1660
01:09:25,139 --> 01:09:26,779
No, I have four devices.

1661
01:09:26,779 --> 01:09:29,339
Okay. And device three
and device four,

1662
01:09:29,339 --> 01:09:30,959
they are using different colors.

1663
01:09:30,959 --> 01:09:34,179
Okay. This one is

1664
01:09:34,179 --> 01:09:38,379
called I wouldn't call
it three G parison,

1665
01:09:38,379 --> 01:09:40,059
but it's very complicated.

1666
01:09:40,059 --> 01:09:41,639
Indeed, this is
something that we

1667
01:09:41,639 --> 01:09:45,499
use today for language model
training for GBD training.

1668
01:09:45,499 --> 01:09:46,979
The thing you'll notice that is

1669
01:09:46,979 --> 01:09:48,739
I hold four devices, right?

1670
01:09:48,739 --> 01:09:51,539
What I do is I first cut

1671
01:09:51,539 --> 01:09:53,419
my devices group
into two groups.

1672
01:09:53,419 --> 01:09:56,119
The first group is this
group, blue and red.

1673
01:09:56,119 --> 01:09:59,399
Second is green and yellow.

1674
01:09:59,399 --> 01:10:01,379
And what I do is if you think of

1675
01:10:01,379 --> 01:10:06,179
these two as one mega
device or device group,

1676
01:10:06,179 --> 01:10:08,174
what I do is I first to enter

1677
01:10:08,174 --> 01:10:12,349
of inter operative partism,
I cut in the middle.

1678
01:10:12,349 --> 01:10:14,429
I give this part of Nork to

1679
01:10:14,429 --> 01:10:15,729
the first group and I give

1680
01:10:15,729 --> 01:10:17,469
this part of N to
the second group.

1681
01:10:17,469 --> 01:10:19,809
Okay. And if you

1682
01:10:19,809 --> 01:10:21,929
think this two device
are one meta device,

1683
01:10:21,929 --> 01:10:23,830
this is pure interpartim.

1684
01:10:23,830 --> 01:10:25,609
But instead of
this device group,

1685
01:10:25,609 --> 01:10:28,469
what I do is I continue
to do intraop partism.

1686
01:10:28,469 --> 01:10:32,369
Okay? I basically cut
in column parison,

1687
01:10:32,369 --> 01:10:35,709
for the first group, the two
device in the first group,

1688
01:10:35,709 --> 01:10:39,089
and I do another column party
in the second group. Okay?

1689
01:10:39,089 --> 01:10:42,849
This is rather complicated
because it essentially

1690
01:10:42,849 --> 01:10:46,789
combines inter and intra
op pismoO more devices.

1691
01:10:46,789 --> 01:10:50,809
And today, people give the
name that is parism, okay?

1692
01:10:50,809 --> 01:10:54,419
And um if you want to
train really large model,

1693
01:10:54,419 --> 01:10:58,539
for example, on more than 1,000
GPUs, this is your to go.

1694
01:10:58,539 --> 01:11:00,819
I'm going to explain next.

1695
01:11:00,819 --> 01:11:03,619
Okay. Then I have

1696
01:11:03,619 --> 01:11:05,659
one last slide to

1697
01:11:05,659 --> 01:11:07,479
explain why I spent

1698
01:11:07,479 --> 01:11:09,799
so much time to onboard
you on this orientation.

1699
01:11:09,799 --> 01:11:12,519
Why we care about
intra and intra.

1700
01:11:13,040 --> 01:11:16,019
But before that, I want to
summarie a little bit okay.

1701
01:11:16,019 --> 01:11:18,179
For interrupted
partism we basically

1702
01:11:18,179 --> 01:11:21,039
assign different operators
to different devices.

1703
01:11:21,039 --> 01:11:24,099
For interrupted
partism we assign

1704
01:11:24,099 --> 01:11:25,479
different regions of a single

1705
01:11:25,479 --> 01:11:28,159
operator to different devices.

1706
01:11:28,159 --> 01:11:31,319
And that's easy to understand.

1707
01:11:32,810 --> 01:11:35,529
Okay. Now, let's try

1708
01:11:35,529 --> 01:11:38,929
to see what the
computational pattern

1709
01:11:38,929 --> 01:11:40,669
really looks like if we apply

1710
01:11:40,669 --> 01:11:44,469
these two types of
partism to a small graph.

1711
01:11:44,469 --> 01:11:46,589
Okay, we are going to dive
deeper into this partition and

1712
01:11:46,589 --> 01:11:49,409
we basically thinking in a
way that what happens on GPU.

1713
01:11:49,409 --> 01:11:52,229
Okay? And this is one of
the most important slides.

1714
01:11:52,229 --> 01:11:54,129
So make sure you understand it.

1715
01:11:54,129 --> 01:11:55,709
And if you don't
understand, come

1716
01:11:55,709 --> 01:11:57,869
to ask me, I try
to explain this.

1717
01:11:57,869 --> 01:12:01,349
Okay. Consider this
graph with five notes.

1718
01:12:01,349 --> 01:12:03,349
Okay. I got this
graph by basically

1719
01:12:03,349 --> 01:12:05,949
skipping the al and the MSE.

1720
01:12:05,949 --> 01:12:07,189
I make it even simpler.

1721
01:12:07,189 --> 01:12:09,174
Okay. Um,

1722
01:12:09,174 --> 01:12:11,419
So in fact, this graph
is essentially doing

1723
01:12:11,419 --> 01:12:15,819
two matrix modifications
as this equation shows.

1724
01:12:16,460 --> 01:12:19,159
Okay, I want you to
basically internalize

1725
01:12:19,159 --> 01:12:21,679
this equation on
this graph, right?

1726
01:12:21,679 --> 01:12:24,039
So here, I basically partition

1727
01:12:24,039 --> 01:12:26,659
the W into two devices
into two parts, right?

1728
01:12:26,659 --> 01:12:28,699
And I replicate X.

1729
01:12:28,699 --> 01:12:30,499
And I like each copy of

1730
01:12:30,499 --> 01:12:35,919
X column partition the W
which will basically give me,

1731
01:12:35,919 --> 01:12:41,599
also I do partition
another uh W and W two,

1732
01:12:41,599 --> 01:12:43,779
and I tie them together
on different devices.

1733
01:12:43,779 --> 01:12:45,859
I make each part from
different devices.

1734
01:12:45,859 --> 01:12:48,599
Okay? Okay, with this
equation, right?

1735
01:12:48,599 --> 01:12:52,519
Cool. Okay.

1736
01:12:52,519 --> 01:12:54,159
Let's see what's
happening on device.

1737
01:12:54,159 --> 01:12:56,119
Okay. Let's start
from the first memo.

1738
01:12:56,119 --> 01:13:00,039
So let's start with
this one. So here

1739
01:13:00,039 --> 01:13:02,119
I know X is replicated, right?

1740
01:13:02,119 --> 01:13:06,159
So which means both device
one and two will have copy x.

1741
01:13:06,159 --> 01:13:09,459
Okay? So the way W one,

1742
01:13:09,459 --> 01:13:10,999
it is column partition, right?

1743
01:13:10,999 --> 01:13:13,659
So which means that device

1744
01:13:13,659 --> 01:13:16,959
one has the left part
of W one, right?

1745
01:13:16,959 --> 01:13:19,079
And Device two has the
right part of W one.

1746
01:13:19,079 --> 01:13:24,479
Okay? So X is replicated, right?

1747
01:13:24,479 --> 01:13:31,239
So each device has X
Left par W par, W. Okay?

1748
01:13:31,239 --> 01:13:34,139
We are going to perform at
the mall. So what do we get?

1749
01:13:36,290 --> 01:13:38,669
We tie them together, right?

1750
01:13:38,669 --> 01:13:40,609
For the first device,
I'm going to get

1751
01:13:40,609 --> 01:13:42,569
the first half of
the Mdmal result,

1752
01:13:42,569 --> 01:13:45,649
which is met, for
the second device,

1753
01:13:45,649 --> 01:13:49,209
I'm going to get the
second half of Mdm M,

1754
01:13:49,890 --> 01:13:52,949
that's why basically
in this notation,

1755
01:13:52,949 --> 01:13:57,129
I get this met and
M. It's our result.

1756
01:13:59,170 --> 01:14:01,769
And we can basically check

1757
01:14:01,769 --> 01:14:04,709
the result here the
partition here.

1758
01:14:04,709 --> 01:14:06,489
We find that basically this

1759
01:14:06,489 --> 01:14:07,889
met corresponds to this

1760
01:14:07,889 --> 01:14:09,369
met and this mo
correspond with small,

1761
01:14:09,369 --> 01:14:13,049
there's no problem it
aligns. We're good.

1762
01:14:13,049 --> 01:14:15,149
We are going to proceed
to the next meth mod.

1763
01:14:15,149 --> 01:14:22,249
Okay. Okay. In the next met
mode, we start with this.

1764
01:14:22,249 --> 01:14:25,209
Okay? No, let's move
right into here.

1765
01:14:25,209 --> 01:14:27,049
Okay. Look at this one.

1766
01:14:27,049 --> 01:14:28,649
Okay. So in this figure,

1767
01:14:28,649 --> 01:14:31,309
we said that we are going
to for the first device,

1768
01:14:31,309 --> 01:14:32,989
we are going to have met
for the second device,

1769
01:14:32,989 --> 01:14:36,429
we are going to have
mod, meanwhile, uh,

1770
01:14:36,429 --> 01:14:38,189
for this W two, right, we

1771
01:14:38,189 --> 01:14:40,489
basically do a low
parting, okay?

1772
01:14:40,489 --> 01:14:42,589
Which means that for
the first device,

1773
01:14:42,589 --> 01:14:44,889
we have the upper
part of W two, right?

1774
01:14:44,889 --> 01:14:45,809
For the second device,

1775
01:14:45,809 --> 01:14:48,029
we have the lower
part of W two, okay?

1776
01:14:48,029 --> 01:14:52,109
And what do we do we are
going to do the multiplier.

1777
01:14:52,109 --> 01:14:55,169
Okay? So what are
happen if we do this?

1778
01:14:56,850 --> 01:14:59,969
So we all get a matrix met M,

1779
01:14:59,969 --> 01:15:01,289
which is the full shape of

1780
01:15:01,289 --> 01:15:03,229
the same shape of
the original metamo.

1781
01:15:03,229 --> 01:15:06,229
But their values are different.

1782
01:15:06,229 --> 01:15:09,889
Because if you still
remember our Mdm cross,

1783
01:15:09,889 --> 01:15:13,729
this is called
partial sum, right?

1784
01:15:13,729 --> 01:15:16,409
So the invintu metal
value should be we

1785
01:15:16,409 --> 01:15:17,609
should add these two

1786
01:15:17,609 --> 01:15:19,949
together together the
invent memo, right?

1787
01:15:19,949 --> 01:15:21,609
Okay. Then what we do is,

1788
01:15:21,609 --> 01:15:23,149
again, we're going
to check this.

1789
01:15:23,149 --> 01:15:25,249
This is what we get if
we execute it right.

1790
01:15:25,249 --> 01:15:27,629
And this is what we
expect on graph, right?

1791
01:15:27,629 --> 01:15:30,369
Okay? We are going to
check this on this.

1792
01:15:30,369 --> 01:15:33,209
So let's check it, okay?

1793
01:15:33,209 --> 01:15:38,109
So on this one, each device
basically get a partial sum.

1794
01:15:38,109 --> 01:15:40,229
But on this graph, we
want each device to

1795
01:15:40,229 --> 01:15:43,109
get a replica of the met.

1796
01:15:43,109 --> 01:15:46,649
So how we can transition
from here to here,

1797
01:15:48,220 --> 01:15:52,119
how we can satisfy the
constraint on life handset.

1798
01:15:52,119 --> 01:15:56,779
We add them together, right?
So how to add them together?

1799
01:15:57,980 --> 01:16:00,779
So in order to add
them together,

1800
01:16:00,779 --> 01:16:02,679
I need the first device to send

1801
01:16:02,679 --> 01:16:05,599
this pure sum to the
second device, right?

1802
01:16:05,599 --> 01:16:07,599
And meanwhile, I need
to second device

1803
01:16:07,599 --> 01:16:09,579
to send this pum to
the first device.

1804
01:16:09,579 --> 01:16:12,339
That is, I need this
kind of operation.

1805
01:16:12,380 --> 01:16:15,659
That is using to me
and using to me.

1806
01:16:15,659 --> 01:16:18,139
And then I do the
summation myself, okay?

1807
01:16:18,139 --> 01:16:20,699
Right? That's the way that
I minimize communication.

1808
01:16:20,699 --> 01:16:27,469
Okay? What operation basically
perform this operation?

1809
01:16:27,469 --> 01:16:29,249
What kind of
communication operation

1810
01:16:29,249 --> 01:16:31,670
will satisfy this constraint?
It's called reduce.

1811
01:16:31,670 --> 01:16:34,089
Okay? You guys know
reduce, right?

1812
01:16:34,089 --> 01:16:35,809
Okay. If you don't
know, it's fine.

1813
01:16:35,809 --> 01:16:37,369
I'm going to teach you
next quarter. Okay.

1814
01:16:37,369 --> 01:16:40,269
But not next
quarter, next class.

1815
01:16:40,269 --> 01:16:43,169
Okay. But essentially,
here, you can see.

1816
01:16:43,169 --> 01:16:45,829
When I do this
kind of partition,

1817
01:16:45,829 --> 01:16:48,049
I derive what execution,

1818
01:16:48,049 --> 01:16:51,569
I try to see how I can satisfy
this kind of partitioning.

1819
01:16:51,569 --> 01:16:54,469
And I eventually found that
I actually need reduce,

1820
01:16:54,469 --> 01:16:56,489
this reduce is basically how

1821
01:16:56,489 --> 01:17:01,149
pardon handles this
kind of partitioning.

1822
01:17:01,149 --> 01:17:04,229
Yeah. Okay? Let's look
at second example.

1823
01:17:04,229 --> 01:17:06,299
Yeah, please. Strategy.

1824
01:17:06,299 --> 01:17:08,819
Sorry. This strategy.

1825
01:17:08,819 --> 01:17:10,619
Is this a good strategy.

1826
01:17:10,619 --> 01:17:13,519
This is a good strategy.
This is the Mctron.

1827
01:17:13,519 --> 01:17:18,059
Yeah. Yeah. It's pretty slow,

1828
01:17:18,059 --> 01:17:19,859
because you need to
reduce and reduce is

1829
01:17:19,859 --> 01:17:21,719
a very expensive operation,

1830
01:17:21,719 --> 01:17:23,519
but there's no cheaper way.

1831
01:17:23,519 --> 01:17:25,719
Because eventually, I'm going to

1832
01:17:25,719 --> 01:17:27,639
convert this into
optimal problem.

1833
01:17:27,639 --> 01:17:30,099
I will prove this
is the optimal way.

1834
01:17:30,099 --> 01:17:34,219
You cannot do better.
Okay? Doesn't make sense.

1835
01:17:41,670 --> 01:17:45,649
Yeah, let's assume that
this is what we want.

1836
01:17:45,649 --> 01:17:47,749
But I want to explain
why we need this because

1837
01:17:47,749 --> 01:17:50,409
in order to perform
the next palm,

1838
01:17:50,409 --> 01:17:53,849
how to have this met
following this replica.

1839
01:17:53,849 --> 01:17:58,329
Yeah. Okay. Then
I'm going to finish

1840
01:17:58,329 --> 01:18:01,089
my lecture using
one final example

1841
01:18:01,089 --> 01:18:03,709
and the next example
is going to be easier.

1842
01:18:05,350 --> 01:18:07,669
This is another part of history,

1843
01:18:07,669 --> 01:18:11,209
which I call inter paralization.
It's pretty simple.

1844
01:18:11,209 --> 01:18:14,389
What happens on first
device is basically uh,

1845
01:18:14,389 --> 01:18:15,729
for following scholar, I know

1846
01:18:15,729 --> 01:18:17,809
this memo is going to
perform on the first device.

1847
01:18:17,809 --> 01:18:21,169
And it basically has these
two input X on W one local.

1848
01:18:21,169 --> 01:18:23,289
So what happens on the
first device is X,

1849
01:18:23,289 --> 01:18:27,009
types one, I get the memo
results on the first device.

1850
01:18:27,009 --> 01:18:30,209
And then I start moving my
I into the second memo.

1851
01:18:30,209 --> 01:18:32,049
Here I want my second
memo to happen on

1852
01:18:32,049 --> 01:18:34,609
the second device,
the red device.

1853
01:18:34,609 --> 01:18:37,069
So here, in the second device,

1854
01:18:37,069 --> 01:18:39,289
I have this W two, which
is also local, right?

1855
01:18:39,289 --> 01:18:42,009
And I want my results to be
written in the second device.

1856
01:18:42,009 --> 01:18:44,449
But I find that I'm
missing my input, right?

1857
01:18:44,449 --> 01:18:46,089
And this input where it is,

1858
01:18:46,089 --> 01:18:47,869
is on the first
device here, right?

1859
01:18:47,869 --> 01:18:51,124
So in order to satisfy this
kind of competion, what I do?

1860
01:18:51,124 --> 01:18:53,039
I also communicate by

1861
01:18:53,039 --> 01:18:54,899
the communicating
pattern changes here.

1862
01:18:54,899 --> 01:18:56,899
What I do is I ask
the first device

1863
01:18:56,899 --> 01:18:59,599
to send the more results
directly to the second device.

1864
01:18:59,599 --> 01:19:03,339
Okay? That's what I did, right?

1865
01:19:03,339 --> 01:19:07,639
This is called P to
P communication.

1866
01:19:07,639 --> 01:19:09,399
This is a slightly different

1867
01:19:09,399 --> 01:19:10,799
communicating pattern
from this one.

1868
01:19:10,799 --> 01:19:12,519
In this one, I need every device

1869
01:19:12,519 --> 01:19:14,839
to communicate with
the other device.

1870
01:19:14,839 --> 01:19:16,479
But in this one, I only

1871
01:19:16,479 --> 01:19:18,059
need one device to
communicate to the other.

1872
01:19:18,059 --> 01:19:20,819
One is sender, the
other is receiver.

1873
01:19:21,220 --> 01:19:23,999
I think now you already
spot the difference.

1874
01:19:23,999 --> 01:19:25,859
I'm going to give
you a statement.

1875
01:19:25,859 --> 01:19:27,879
So for intraoperative partism,

1876
01:19:27,879 --> 01:19:30,059
as long as you do this kind
of operator partitioning,

1877
01:19:30,059 --> 01:19:34,019
you are going to deduce some
connective communication.

1878
01:19:34,020 --> 01:19:36,540
But for this interoperatism,

1879
01:19:36,540 --> 01:19:37,759
if you don't party in operator,

1880
01:19:37,759 --> 01:19:39,139
you always part in a graph,

1881
01:19:39,139 --> 01:19:41,919
you only need PTP communication.

1882
01:19:41,919 --> 01:19:44,439
That basically give
you a key difference

1883
01:19:44,439 --> 01:19:47,699
because different communication
have very different cost.

1884
01:19:47,699 --> 01:19:52,059
A reduce is much, much
more expensive than PTP,

1885
01:19:52,059 --> 01:19:53,799
that need to be mapped into

1886
01:19:53,799 --> 01:19:56,279
different network hierarchies.

1887
01:19:56,279 --> 01:19:57,719
For example, if you
have high bandwidth,

1888
01:19:57,719 --> 01:19:59,759
you should prefer to reduce.

1889
01:19:59,759 --> 01:20:01,119
And if you have low bandwidth,

1890
01:20:01,119 --> 01:20:03,299
you should prefer to use
you should avoid radius.

1891
01:20:03,299 --> 01:20:04,859
You should prefer using PTP.

1892
01:20:04,859 --> 01:20:06,259
In the next lecture,
I'm going to

1893
01:20:06,259 --> 01:20:07,399
introduce this to communicating

1894
01:20:07,399 --> 01:20:08,839
primitives to make
sure we understand

1895
01:20:08,839 --> 01:20:11,499
the mathematics
behind it. Thank you.
