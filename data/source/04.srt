1
00:00:08,480 --> 00:00:12,559
Okay. Thanks for coming back.

2
00:00:12,559 --> 00:00:14,879
Yeah, let's get started.

3
00:00:15,440 --> 00:00:18,859
Cool. Enrollment updates.

4
00:00:18,859 --> 00:00:22,899
We have sent out two
batches of invitations.

5
00:00:22,899 --> 00:00:25,319
One on Wednesday, the
other on Thursday.

6
00:00:25,319 --> 00:00:28,499
I think we try to enroll
another 100 students.

7
00:00:28,499 --> 00:00:31,600
And if you want to take this
class, you need to accept.

8
00:00:31,600 --> 00:00:34,320
You have a 24 hours due.

9
00:00:34,520 --> 00:00:36,779
After the deal, we
are going to enroll

10
00:00:36,779 --> 00:00:38,160
the next batch tomorrow morning.

11
00:00:38,160 --> 00:00:40,239
Yeah. Okay.

12
00:00:43,260 --> 00:00:49,979
Um, let's get started
with today's contents.

13
00:00:49,979 --> 00:00:52,320
So I put that graph

14
00:00:52,320 --> 00:00:54,039
there so make sure you
understand where you are now.

15
00:00:54,039 --> 00:00:56,200
Okay? Well, at the
bottom layer, okay?

16
00:00:56,200 --> 00:00:59,909
Operators. So last lecture,

17
00:00:59,909 --> 00:01:01,769
I think we start
talking about how to

18
00:01:01,769 --> 00:01:04,450
make operators fast
in general, right?

19
00:01:04,450 --> 00:01:08,150
And we said we can vectorize
them by leveraging

20
00:01:08,150 --> 00:01:14,690
the torized functions
from platforms,

21
00:01:14,690 --> 00:01:16,370
accelerator platforms.

22
00:01:16,370 --> 00:01:18,209
And we also touch based

23
00:01:18,209 --> 00:01:21,350
on data layout and
haven't finished yet.

24
00:01:21,350 --> 00:01:23,650
Okay. Today we are
going to finish that,

25
00:01:23,650 --> 00:01:25,350
and then we are
going to talk about

26
00:01:25,350 --> 00:01:27,570
something that is
very, very deep.

27
00:01:27,570 --> 00:01:31,169
And that is our
first metam lesson.

28
00:01:31,169 --> 00:01:32,590
Okay? We are going to have many,

29
00:01:32,590 --> 00:01:33,390
many these lessons, but

30
00:01:33,390 --> 00:01:35,185
today we are going
to take the first.

31
00:01:35,185 --> 00:01:37,480
And if we can finish it part,

32
00:01:37,480 --> 00:01:39,439
and then we are going
to talk about something

33
00:01:39,439 --> 00:01:42,659
that's really interesting that
is the current GPO market.

34
00:01:42,659 --> 00:01:45,219
We are going to repurpose

35
00:01:45,219 --> 00:01:47,639
that class as a economics class.

36
00:01:47,639 --> 00:01:48,940
We try to understand
what's going

37
00:01:48,940 --> 00:01:51,115
on in Silicon Valley. Okay?

38
00:01:51,115 --> 00:01:54,529
Yeah, let's resume
with data layout.

39
00:01:54,529 --> 00:01:58,150
Um, so still remember
this one, right?

40
00:01:58,150 --> 00:02:00,989
So more than
deploying frameworks,

41
00:02:00,989 --> 00:02:03,969
they basically use
stress format.

42
00:02:03,969 --> 00:02:07,609
We like compared to column
major or row major,

43
00:02:07,609 --> 00:02:09,249
we introduce two
more parameters.

44
00:02:09,249 --> 00:02:12,129
One is offset, right?

45
00:02:12,129 --> 00:02:13,550
The other is the stress,

46
00:02:13,550 --> 00:02:15,110
which is a list,

47
00:02:15,110 --> 00:02:19,669
okay with a dimension equal
to the shape of the tensor.

48
00:02:19,669 --> 00:02:23,149
And this visualization
basically give you

49
00:02:23,149 --> 00:02:26,635
a concept of what does
strat means, okay?

50
00:02:26,635 --> 00:02:29,780
And we also did a
very small exercise,

51
00:02:29,780 --> 00:02:31,420
if we have a four
dimensional tensor

52
00:02:31,420 --> 00:02:33,019
with a shape one,
two, three, four.

53
00:02:33,019 --> 00:02:34,479
So what stress?

54
00:02:34,479 --> 00:02:37,279
I hope you guys actually
understand that. Okay?

55
00:02:37,279 --> 00:02:40,399
And then we start
asking the question,

56
00:02:40,399 --> 00:02:43,519
column major and row major
are actually pretty good.

57
00:02:43,519 --> 00:02:47,215
Why do we care about saving
stress when saving tensors?

58
00:02:47,215 --> 00:02:48,989
So in order to answer
this question,

59
00:02:48,989 --> 00:02:53,870
I think most of you when you
write petard or programs,

60
00:02:53,870 --> 00:02:57,409
you probably often
use this API view.

61
00:02:57,409 --> 00:02:59,690
You can view a tensor
in different shapes.

62
00:02:59,690 --> 00:03:01,649
For example, the original shape
is one, two, three, four,

63
00:03:01,649 --> 00:03:04,249
but you can view it as 24
by one or something, right?

64
00:03:04,249 --> 00:03:08,089
And the reason we have
this stress because it can

65
00:03:08,089 --> 00:03:10,030
facilitate a lot of this kind

66
00:03:10,030 --> 00:03:12,929
of array manipulation operators.

67
00:03:12,929 --> 00:03:15,050
And view is such an example.

68
00:03:15,050 --> 00:03:18,789
Um, the core concept
here is stress

69
00:03:18,789 --> 00:03:22,895
can separate the underlying
storage and the view tensor.

70
00:03:22,895 --> 00:03:25,799
So basically when we store
the values of the ray,

71
00:03:25,799 --> 00:03:28,520
we are always stored
sequentially in memory.

72
00:03:28,520 --> 00:03:30,999
But we can view it in
many many different ways.

73
00:03:30,999 --> 00:03:35,979
And, um, and because of this,

74
00:03:35,979 --> 00:03:37,619
we can basically every
time we try to view

75
00:03:37,619 --> 00:03:39,479
the test in a different
shape in a different way,

76
00:03:39,479 --> 00:03:40,880
we don't have to
copy the content.

77
00:03:40,880 --> 00:03:42,319
We don't have to copy
the content from

78
00:03:42,319 --> 00:03:44,599
the original array in

79
00:03:44,599 --> 00:03:47,280
the lower level memory into
another part of the memory.

80
00:03:47,280 --> 00:03:48,659
We just need to we can do

81
00:03:48,659 --> 00:03:51,180
a zero copy for this
kind of operators.

82
00:03:51,180 --> 00:03:53,620
Okay? And in the
next few slides,

83
00:03:53,620 --> 00:03:56,839
I'm going to show you some
very common operators

84
00:03:56,839 --> 00:03:58,540
you use to manipulate arrays.

85
00:03:58,540 --> 00:04:00,220
And I'm going to
tell you, actually,

86
00:04:00,220 --> 00:04:02,199
in petrich there or zero copy.

87
00:04:02,199 --> 00:04:05,239
That is when you perform
this kind of operators,

88
00:04:05,239 --> 00:04:06,340
you don't have to copy anything.

89
00:04:06,340 --> 00:04:08,560
It's zero overhead,
very fast, okay?

90
00:04:08,560 --> 00:04:10,960
And all this could contribute

91
00:04:10,960 --> 00:04:15,180
basically attribute to
this stress format, okay?

92
00:04:15,660 --> 00:04:17,800
When I preoperat I want

93
00:04:17,800 --> 00:04:19,100
you to first spin
off your second to

94
00:04:19,100 --> 00:04:20,560
think about how to
implement that when

95
00:04:20,560 --> 00:04:22,299
I give you this stress format.

96
00:04:22,299 --> 00:04:25,779
The first one slice, how
we can slice the tensor?

97
00:04:25,779 --> 00:04:27,340
For example, here, I have four

98
00:04:27,340 --> 00:04:29,559
by four tensor four
by four matrix,

99
00:04:29,559 --> 00:04:32,180
and I want to slice that piece.

100
00:04:35,600 --> 00:04:40,059
So rudimentary way,
you probably just

101
00:04:40,059 --> 00:04:41,820
copy that pyro content
into another array

102
00:04:41,820 --> 00:04:44,040
and you get that one. But
we don't want to copy.

103
00:04:44,040 --> 00:04:46,739
So one way to do that is
we can just manipulate

104
00:04:46,739 --> 00:04:49,879
the values of the
stress and offset.

105
00:04:49,879 --> 00:04:53,219
So we first add the
offset by one, right?

106
00:04:53,219 --> 00:04:56,900
And then we rewrite the
shape into three by two,

107
00:04:56,900 --> 00:04:58,959
and we pass this
offset and shape

108
00:04:58,959 --> 00:05:02,299
into the stress
and offset values,

109
00:05:02,299 --> 00:05:04,799
and you basically
can slice the sensor

110
00:05:04,799 --> 00:05:07,924
out from this bigger tenser.
Okay? Does that make sense?

111
00:05:07,924 --> 00:05:09,729
Cool.

112
00:05:09,729 --> 00:05:14,330
I think the key part here I
want to make is the copy.

113
00:05:14,330 --> 00:05:16,749
Basically, the sliced tensor,

114
00:05:16,749 --> 00:05:19,329
it shares memory with
this original tensor.

115
00:05:19,329 --> 00:05:21,709
But you still get a kind

116
00:05:21,709 --> 00:05:24,690
of a new tensor that with
a different shape, right?

117
00:05:25,850 --> 00:05:28,629
And slice is a pretty
common operator,

118
00:05:28,629 --> 00:05:32,070
but another very common
operator is transpose.

119
00:05:32,070 --> 00:05:35,809
Here, I think everyone
understands trasposeOPermute.

120
00:05:35,809 --> 00:05:37,470
So here you have a tensor with

121
00:05:37,470 --> 00:05:39,250
original shape of two by three.

122
00:05:39,250 --> 00:05:41,130
You want to transpose
it into a three by two.

123
00:05:41,130 --> 00:05:48,530
How does that? Okay, let's see,

124
00:05:48,530 --> 00:05:51,090
we can run through a very,
very simple example.

125
00:05:51,090 --> 00:05:54,569
So here I have a tensor
four dimensional tensor,

126
00:05:54,569 --> 00:05:56,810
and I directly print it stress.

127
00:05:56,810 --> 00:05:58,709
And as we said in
the last lecture,

128
00:05:58,709 --> 00:06:02,610
the stress is basically
24 12 for one, right?

129
00:06:02,610 --> 00:06:05,969
And then this permute
is basically,

130
00:06:05,969 --> 00:06:08,990
uh transpose but it's in
high dimensional, okay?

131
00:06:08,990 --> 00:06:10,629
And we try to permute
the dimension,

132
00:06:10,629 --> 00:06:13,470
we basically shift the
first dimension to the end,

133
00:06:13,470 --> 00:06:16,749
and we put the next
three dimensions

134
00:06:16,749 --> 00:06:19,249
one position ahead, okay?

135
00:06:19,580 --> 00:06:24,380
And you can see if we
print the permuted value,

136
00:06:24,380 --> 00:06:26,880
the underlying storage is
still the same, right?

137
00:06:26,880 --> 00:06:28,780
Sequentially from
one all the way

138
00:06:28,780 --> 00:06:32,359
to 23 from there to
all the way to 23.

139
00:06:32,359 --> 00:06:35,220
But what changes
here is basically we

140
00:06:35,220 --> 00:06:38,360
kind of modify the stress
a little bit, right?

141
00:06:38,360 --> 00:06:39,999
But only modifying the stress.

142
00:06:39,999 --> 00:06:42,200
Remember stress is
a scatter values

143
00:06:42,200 --> 00:06:44,279
not going to be a lot
of memory spaces.

144
00:06:44,279 --> 00:06:46,759
You just need to
modify a small value,

145
00:06:46,759 --> 00:06:49,580
and you can actually achieve
this kind of transpose.

146
00:06:49,580 --> 00:06:53,679
In fact, in all petroloNpi
when you do transpose,

147
00:06:53,679 --> 00:06:56,369
you are also doing
they will copy.

148
00:06:56,369 --> 00:06:57,730
You don't have to copy anything.

149
00:06:57,730 --> 00:07:00,049
You are still sharing the
original nye storage.

150
00:07:00,049 --> 00:07:02,350
That's why transpose
is super fast, okay?

151
00:07:02,350 --> 00:07:06,970
All because of this
stress format. Okay.

152
00:07:07,290 --> 00:07:09,790
Then let's do one more, okay?

153
00:07:09,790 --> 00:07:11,090
And the third operator is

154
00:07:11,090 --> 00:07:13,409
some operator you
use a lot, okay?

155
00:07:13,409 --> 00:07:15,069
Broadcast.

156
00:07:15,069 --> 00:07:17,090
This broadcast operating
is something that

157
00:07:17,090 --> 00:07:18,950
you always face when you try to

158
00:07:18,950 --> 00:07:21,110
multiply or when you
try to add a race

159
00:07:21,110 --> 00:07:23,549
with only one
dimensional matched,

160
00:07:23,549 --> 00:07:25,089
but the other two
dimension proper one

161
00:07:25,089 --> 00:07:26,789
is higher dimension and the
other is lower dimension.

162
00:07:26,789 --> 00:07:28,829
So how do you do
that? And magically,

163
00:07:28,829 --> 00:07:31,190
when you do Lum pi, uh,
they will do that for you.

164
00:07:31,190 --> 00:07:34,389
When you multiply these
two tens together, uh,

165
00:07:34,389 --> 00:07:36,869
there's an implicit, broadcast

166
00:07:36,869 --> 00:07:38,350
happening behind
the scene, right?

167
00:07:38,350 --> 00:07:40,625
But how you achieve
this kind of thing.

168
00:07:40,625 --> 00:07:43,000
Like one rudimentary way is

169
00:07:43,000 --> 00:07:45,479
basically in order to
broadcast the second tensor,

170
00:07:45,479 --> 00:07:47,040
you just copy the
row four times,

171
00:07:47,040 --> 00:07:48,579
and you get a target sensor.

172
00:07:48,579 --> 00:07:50,920
But like I said, it is not
good because copy is slow.

173
00:07:50,920 --> 00:07:54,299
I don't want to copy. And copy
will consume some memory.

174
00:07:54,299 --> 00:07:56,639
Okay? I don't want
to waste memory.

175
00:07:56,639 --> 00:07:58,020
Okay? Let's think about how we

176
00:07:58,020 --> 00:07:59,619
can achieve this using stress.

177
00:07:59,619 --> 00:08:02,519
I'll let you think
about 10 seconds.

178
00:08:09,040 --> 00:08:11,779
Okay. Anyone want to answer?

179
00:08:11,779 --> 00:08:15,790
Yeah. Yeah, yeah, exactly. Yeah.

180
00:08:15,790 --> 00:08:17,989
We just insert one strut

181
00:08:17,989 --> 00:08:19,869
that is zero. Let's
see how this works.

182
00:08:19,869 --> 00:08:20,410
Okay?

183
00:08:20,410 --> 00:08:25,850
So here, let's inspect
the original tensor.

184
00:08:25,850 --> 00:08:28,370
The original tensor has a
stress equal to one, right?

185
00:08:28,370 --> 00:08:32,769
Because it's one dimensional,
and its shape is 13,

186
00:08:32,769 --> 00:08:35,269
this data is
sequentially in memory,

187
00:08:35,269 --> 00:08:37,610
which is one, two,
three, straightforward.

188
00:08:37,610 --> 00:08:40,009
Okay? And the target
tensor one is

189
00:08:40,009 --> 00:08:43,190
basically with a shape
or equals for three,

190
00:08:43,190 --> 00:08:45,630
because I want to
furs three columns.

191
00:08:45,630 --> 00:08:47,450
Like I said, I don't
want to copy data.

192
00:08:47,450 --> 00:08:49,410
I want zero copy. The data

193
00:08:49,410 --> 00:08:51,509
should be kept in memory as one,

194
00:08:51,509 --> 00:08:53,490
two, three, only three elements.

195
00:08:53,490 --> 00:08:57,529
Right. So how to basically
modify the stress.

196
00:08:57,529 --> 00:08:59,709
So remember the
definition of stress is

197
00:08:59,709 --> 00:09:02,170
basically like how many elements

198
00:09:02,170 --> 00:09:04,489
I move along that
dimension, right?

199
00:09:04,489 --> 00:09:06,150
So in order to get that tensor,

200
00:09:06,150 --> 00:09:07,350
what I do is basically I insert

201
00:09:07,350 --> 00:09:09,549
one more thread at the
beginning at zero, right?

202
00:09:09,549 --> 00:09:13,829
That means along the fake
dimension I added here four,

203
00:09:13,829 --> 00:09:15,310
right, the board carts sensor.

204
00:09:15,310 --> 00:09:17,329
Every time I move
along that dimension,

205
00:09:17,329 --> 00:09:19,924
I just time zero, and it will
go back to the beginning.

206
00:09:19,924 --> 00:09:22,920
Okay. That's how we
basically do this kind of,

207
00:09:22,920 --> 00:09:28,160
uh, array manipulations
in behind patters, yeah.

208
00:09:28,160 --> 00:09:30,839
Um, okay, that's all I have.

209
00:09:30,839 --> 00:09:32,639
These operators
actually are very

210
00:09:32,639 --> 00:09:36,060
important and they all
benefit from stress.

211
00:09:36,540 --> 00:09:40,080
Okay, let's do something
that's more hardcore.

212
00:09:40,080 --> 00:09:42,479
Uh, here, in some cases,

213
00:09:42,479 --> 00:09:43,760
I want to swap the tiles.

214
00:09:43,760 --> 00:09:48,879
Okay, I have a tensor where
you can see it still 0-15,

215
00:09:48,879 --> 00:09:50,000
but I want to swap

216
00:09:50,000 --> 00:09:52,024
the pink part with the
blue part, how I do that.

217
00:09:52,024 --> 00:09:54,289
Okay. I wrote a little program

218
00:09:54,289 --> 00:09:56,729
here and you can see my
programming is pretty stupid.

219
00:09:56,729 --> 00:09:58,249
I need to copy a lot of things,

220
00:09:58,249 --> 00:10:01,429
I basically need to slide some
tensor and swap elements.

221
00:10:01,429 --> 00:10:02,789
And I will leave to you

222
00:10:02,789 --> 00:10:04,990
how to implement
this with zero copy.

223
00:10:04,990 --> 00:10:07,089
Okay? I'm not going

224
00:10:07,089 --> 00:10:09,329
to give you the answer.
Think about that. Okay.

225
00:10:09,329 --> 00:10:11,829
But here, the important thing

226
00:10:11,829 --> 00:10:14,730
is whenever you write this
kind of program in Patric,

227
00:10:14,730 --> 00:10:16,550
you need to think about if
you are doing zero copy.

228
00:10:16,550 --> 00:10:17,690
If you can't do zero copy,

229
00:10:17,690 --> 00:10:19,069
you should always do
zero copy, right?

230
00:10:19,069 --> 00:10:20,910
Like I said, zero copy
is zero overhead,

231
00:10:20,910 --> 00:10:22,929
and you don't have to
consume any memory.

232
00:10:22,929 --> 00:10:24,294
Yeah. Okay?

233
00:10:24,294 --> 00:10:31,480
Um, with that, I think I
finished introducing the stress.

234
00:10:31,480 --> 00:10:33,780
And then let's think about, um,

235
00:10:33,780 --> 00:10:35,960
so what is the problem stress?

236
00:10:35,960 --> 00:10:38,834
I think the problem stress
is quite obvious, right?

237
00:10:38,834 --> 00:10:41,049
Uh, say, if we view

238
00:10:41,049 --> 00:10:43,670
a tensor or select a
tensor or whatever,

239
00:10:43,670 --> 00:10:46,450
um, yeah, indeed, we
can get a tensor,

240
00:10:46,450 --> 00:10:49,909
but, uh, the new tensor,

241
00:10:49,909 --> 00:10:51,969
returned by the operator, uh,

242
00:10:51,969 --> 00:10:53,689
under its values are

243
00:10:53,689 --> 00:10:55,529
not continuous in
memory anymore, right?

244
00:10:55,529 --> 00:10:57,809
Because only the original
tensor is continuous in memory.

245
00:10:57,809 --> 00:10:59,269
Once we change the stress or

246
00:10:59,269 --> 00:11:01,069
whatever, it's not continuous.

247
00:11:01,069 --> 00:11:04,070
But like I said, in
many, many, for example,

248
00:11:04,070 --> 00:11:07,029
element wise or is
multiplication or

249
00:11:07,029 --> 00:11:10,070
many many other
accelerator platforms

250
00:11:10,070 --> 00:11:11,669
when you do this
kind of operator,

251
00:11:11,669 --> 00:11:13,109
you have to make sure tensor

252
00:11:13,109 --> 00:11:14,849
has a continuous storage, right?

253
00:11:14,849 --> 00:11:17,249
So which means that
every time when

254
00:11:17,249 --> 00:11:20,759
you apply this kind
of or manipulation.

255
00:11:20,759 --> 00:11:22,159
And if you proceed to next

256
00:11:22,159 --> 00:11:23,439
operation and the
next Operation,

257
00:11:23,439 --> 00:11:25,720
have a requirement
on the continuity

258
00:11:25,720 --> 00:11:27,080
of the underlying storage,

259
00:11:27,080 --> 00:11:29,199
what do you do is you
sometimes need to call

260
00:11:29,199 --> 00:11:31,859
this operator torch
or contiguous,

261
00:11:31,859 --> 00:11:35,160
and it will basically
rearrange the um,

262
00:11:35,160 --> 00:11:37,139
underlying storage
for you. Okay?

263
00:11:37,139 --> 00:11:38,620
So this one sounds similar.

264
00:11:38,620 --> 00:11:40,020
And if you don't do
this, you probably

265
00:11:40,020 --> 00:11:41,439
made this error
many many times in

266
00:11:41,439 --> 00:11:42,839
torching you don't
want to complain that

267
00:11:42,839 --> 00:11:44,920
your tensor is not
continuous in memory,

268
00:11:44,920 --> 00:11:46,739
so I cannot proceed
next operation.

269
00:11:46,739 --> 00:11:53,139
Okay? Cool. Any question? No.

270
00:11:53,139 --> 00:11:55,679
Okay, then we finish this layout

271
00:11:55,679 --> 00:11:58,319
and so the next thing

272
00:11:58,319 --> 00:12:00,340
that we want to
introduce is basically,

273
00:12:00,340 --> 00:12:04,739
we can also paralyze
um, operators, right?

274
00:12:04,739 --> 00:12:07,120
So here, I think the palsiton

275
00:12:07,120 --> 00:12:09,020
especially for this
kind of operation.

276
00:12:09,020 --> 00:12:11,340
So here, I hope you still

277
00:12:11,340 --> 00:12:12,480
remember the operation here is

278
00:12:12,480 --> 00:12:14,059
basically element
we side, right?

279
00:12:14,059 --> 00:12:15,919
We add A and B and rather to

280
00:12:15,919 --> 00:12:19,094
C. We do that element by
element from left to right,

281
00:12:19,094 --> 00:12:21,769
in this kind of element
wise operations,

282
00:12:21,769 --> 00:12:23,369
it's very easy to
paralyze it, right?

283
00:12:23,369 --> 00:12:25,690
For example, in
most CPU platforms,

284
00:12:25,690 --> 00:12:27,370
you have this decorator

285
00:12:27,370 --> 00:12:28,930
which you can decorate
on top of your loop,

286
00:12:28,930 --> 00:12:31,330
and it will basically split

287
00:12:31,330 --> 00:12:33,289
the addition operations to

288
00:12:33,289 --> 00:12:37,370
different CPU course and it
all perform in parallel.

289
00:12:37,370 --> 00:12:41,850
The key thing here
is the partition or

290
00:12:41,850 --> 00:12:44,209
the split is
straightforward because it

291
00:12:44,209 --> 00:12:46,889
is a element wise
at each element,

292
00:12:46,889 --> 00:12:49,970
addition is independent
from other elements.

293
00:12:49,970 --> 00:12:52,610
So you can easily
split the repetition.

294
00:12:52,610 --> 00:12:54,349
This is a very simple way

295
00:12:54,349 --> 00:12:58,055
to paralyze operators and
leverage mark course.

296
00:12:58,055 --> 00:13:02,099
We'll talk more about this
because suppose there are

297
00:13:02,099 --> 00:13:04,100
some operations
where one elements

298
00:13:04,100 --> 00:13:07,219
add is dependent on
another elements add,

299
00:13:07,219 --> 00:13:10,120
how can we basically
split these operators,

300
00:13:10,120 --> 00:13:12,579
and we are going to dive
deeper into that part later.

301
00:13:12,579 --> 00:13:14,459
That basically will
cover our lecture

302
00:13:14,459 --> 00:13:16,680
in how to paralyze the operator.

303
00:13:16,680 --> 00:13:18,280
But this one is too
straightforward,

304
00:13:18,280 --> 00:13:20,670
so I put it here.
You understand.

305
00:13:20,670 --> 00:13:24,359
Okay. To summarize how we
can make operator faster,

306
00:13:24,359 --> 00:13:27,020
we can do arization
We can leverage

307
00:13:27,020 --> 00:13:29,640
the platform specific
Corize functions,

308
00:13:29,640 --> 00:13:30,979
for example, the
load floor four.

309
00:13:30,979 --> 00:13:34,719
Okay? We can also
manipulate the data layout.

310
00:13:34,719 --> 00:13:37,479
Um, we introduce
the strat format,

311
00:13:37,479 --> 00:13:41,120
which allows us to do copy
for many, many operators.

312
00:13:41,120 --> 00:13:42,880
And for some very simple

313
00:13:42,880 --> 00:13:44,979
straightforward animal
wise operators,

314
00:13:44,979 --> 00:13:48,519
we do very simple parison, okay.

315
00:13:49,550 --> 00:13:50,829
Okay.

316
00:13:50,829 --> 00:13:53,110
Now, let's go into

317
00:13:53,110 --> 00:13:54,789
the next part and

318
00:13:54,789 --> 00:13:57,729
probably a difficult
part of this lecture.

319
00:13:57,729 --> 00:13:59,850
So how we can make
met moo faster.

320
00:13:59,850 --> 00:14:03,889
Previously, we have to talk
about, array manipulations.

321
00:14:03,889 --> 00:14:05,150
We talk about element wise,

322
00:14:05,150 --> 00:14:07,610
but met moo is apparently
not animal wise,

323
00:14:07,610 --> 00:14:09,469
you have to multiply the role of

324
00:14:09,469 --> 00:14:11,930
the first matrix of the
common second matrix,

325
00:14:11,930 --> 00:14:14,050
and it's not performed
element by element.

326
00:14:14,050 --> 00:14:17,169
So how we can make these
kind of operators fast.

327
00:14:17,169 --> 00:14:18,929
And this is so important
because we know

328
00:14:18,929 --> 00:14:22,150
marching system is all
about met moo, okay?

329
00:14:22,800 --> 00:14:26,540
Um, there are in
general, two ideas.

330
00:14:26,540 --> 00:14:28,119
One is, we have some
algorithm for that,

331
00:14:28,119 --> 00:14:30,360
and we are going to dive
deep into the algorithm.

332
00:14:30,360 --> 00:14:32,580
The other is we can use
some more ace hardware.

333
00:14:32,580 --> 00:14:34,459
That's why we start talking
about accelerators,

334
00:14:34,459 --> 00:14:36,320
okay, in the final
part of this lecture.

335
00:14:36,320 --> 00:14:37,279
Okay?

336
00:14:37,279 --> 00:14:39,439
Um, before we talk

337
00:14:39,439 --> 00:14:42,120
about the optimization
techniques, so what is Mtmo?

338
00:14:42,120 --> 00:14:44,880
Okay. What is essentially
a Mthmo in code?

339
00:14:44,880 --> 00:14:46,499
So I believe most of you

340
00:14:46,499 --> 00:14:48,139
have implement Mthmo
by yourself, right?

341
00:14:48,139 --> 00:14:53,510
Metamo is essentially a
a three layer loop, IJK,

342
00:14:53,510 --> 00:14:58,389
here we are trying to multiply
two matrices A and B,

343
00:14:58,389 --> 00:14:59,710
and we try to write
the results into

344
00:14:59,710 --> 00:15:03,410
C and each of them
are two dimensional.

345
00:15:03,410 --> 00:15:05,070
Here, I simplify the problem,

346
00:15:05,070 --> 00:15:09,090
their dimension or equal
to N, both dimensions.

347
00:15:09,090 --> 00:15:10,909
In order to implement
this met mo,

348
00:15:10,909 --> 00:15:14,490
we basically write this
loop we loop over I, J,

349
00:15:14,490 --> 00:15:17,929
and then we take a
row and column from

350
00:15:17,929 --> 00:15:19,809
two matrices and we do

351
00:15:19,809 --> 00:15:23,029
a dot product and write the
result back to C, right?

352
00:15:23,350 --> 00:15:28,469
No question, right?
Okay. What is

353
00:15:28,469 --> 00:15:31,350
the complexity of this loop?

354
00:15:32,150 --> 00:15:34,330
Cube, right? A cube.

355
00:15:34,330 --> 00:15:37,149
Okay. So what is

356
00:15:37,149 --> 00:15:41,650
the best complexity
the human society

357
00:15:41,650 --> 00:15:44,270
can achieve? Do you guys know?

358
00:15:46,190 --> 00:15:48,530
It's definitely
better than cube,

359
00:15:48,530 --> 00:15:50,630
because you guys
probably know a lot

360
00:15:50,630 --> 00:15:52,730
of researchers pH
networking in this area.

361
00:15:52,730 --> 00:15:55,410
Okay. Yeah. So
here's the number.

362
00:15:55,410 --> 00:16:00,309
It's roughly 2.37 1552. Okay?

363
00:16:00,870 --> 00:16:06,229
But no one can reduce it to
square, of course, okay?

364
00:16:06,340 --> 00:16:10,540
And mere complexity
is actually very,

365
00:16:10,540 --> 00:16:13,359
very, um, I wouldn't say active,

366
00:16:13,359 --> 00:16:15,179
but old line of research, right?

367
00:16:15,179 --> 00:16:19,800
People work on that from
50, 60 years ago, okay?

368
00:16:19,800 --> 00:16:21,299
And you can see, there are many,

369
00:16:21,299 --> 00:16:23,799
many notable papers
published along the line.

370
00:16:23,799 --> 00:16:26,419
And this is how we
do on this research.

371
00:16:26,419 --> 00:16:29,260
Okay you can see
originally we are at cube,

372
00:16:29,260 --> 00:16:31,759
right, and we
gradually reduce it.

373
00:16:31,759 --> 00:16:34,360
We try to invent one
algorithm to make that fast.

374
00:16:34,360 --> 00:16:36,959
Um, but as you can see,

375
00:16:36,959 --> 00:16:40,780
the Y access is actually
very fine green,

376
00:16:40,780 --> 00:16:44,440
which means that it's
already saturated.

377
00:16:44,440 --> 00:16:46,339
So I conclude that metamo is not

378
00:16:46,339 --> 00:16:48,979
a good area to research
because if you do this,

379
00:16:48,979 --> 00:16:50,880
it's unlikely you can
publish a good paper.

380
00:16:50,880 --> 00:16:53,480
Okay? Yeah, it's very saturated.

381
00:16:53,480 --> 00:16:55,620
Um, but require our goal, okay?

382
00:16:55,620 --> 00:16:57,419
We try to make memo fast, right?

383
00:16:57,419 --> 00:16:59,480
There are two ways
to make it fast.

384
00:16:59,480 --> 00:17:01,565
Our ultimate goal
is, yeah, please.

385
00:17:01,565 --> 00:17:04,829
So from 1990,

386
00:17:11,430 --> 00:17:13,929
very little, very little. Yeah.

387
00:17:13,929 --> 00:17:16,929
Yeah, you can think there's
almost no difference.

388
00:17:16,929 --> 00:17:19,169
Especially when you put
this into real system,

389
00:17:19,169 --> 00:17:21,530
those kind of improvement
will diminish.

390
00:17:21,530 --> 00:17:24,689
Yeah, yeah. Yeah. Okay.

391
00:17:24,689 --> 00:17:26,590
Let's go back to
this slide, okay.

392
00:17:26,590 --> 00:17:29,349
Our goal maximize arithmetic
intensity, right?

393
00:17:29,349 --> 00:17:31,769
And apparently this AI equals

394
00:17:31,769 --> 00:17:36,550
to compute divided
by memory, memory o.

395
00:17:36,550 --> 00:17:39,149
So what these people are
doing basically they try to

396
00:17:39,149 --> 00:17:41,589
increase the number
of the compute,

397
00:17:41,589 --> 00:17:43,169
right? Make it faster.

398
00:17:43,169 --> 00:17:45,950
Okay. Okay. And like I
said, it's saturated.

399
00:17:45,950 --> 00:17:47,609
So we have to take an
alternative, right?

400
00:17:47,609 --> 00:17:51,550
We try to reduce the IOs so
we can still make it fast.

401
00:17:51,550 --> 00:17:51,889
Okay?

402
00:17:51,889 --> 00:17:53,550
Does that make
sense? Big picture.

403
00:17:53,550 --> 00:17:55,329
Okay. Okay.

404
00:17:55,329 --> 00:17:57,029
Then in the rest
lecture, what are we

405
00:17:57,029 --> 00:17:58,809
going to talk about how

406
00:17:58,809 --> 00:18:00,609
we can basically
make the second term

407
00:18:00,609 --> 00:18:02,529
smaller when we perform at them.

408
00:18:02,529 --> 00:18:03,729
Okay.

409
00:18:04,020 --> 00:18:07,159
Uh, in order to talk about that,

410
00:18:07,159 --> 00:18:09,279
I think I need to remind you of

411
00:18:09,279 --> 00:18:11,819
something you learned
from operating systems.

412
00:18:11,819 --> 00:18:13,820
Okay? Remember in computers,

413
00:18:13,820 --> 00:18:16,399
we have memory hierarchy, right?

414
00:18:16,399 --> 00:18:18,379
The reason we have
memory hierarchy is

415
00:18:18,379 --> 00:18:22,439
because memory becomes
more and more expensive,

416
00:18:22,439 --> 00:18:26,259
right when it is closer to
the processing units, right?

417
00:18:26,259 --> 00:18:29,779
For example, in the
processors, we have registers.

418
00:18:29,779 --> 00:18:31,460
It's super fast, likely fast.

419
00:18:31,460 --> 00:18:33,599
It's basically
local, but we cannot

420
00:18:33,599 --> 00:18:36,020
make many of them because
it's so expensive.

421
00:18:36,020 --> 00:18:38,440
That's why we add layers
and layers and layers,

422
00:18:38,440 --> 00:18:40,939
and we ended up with
memory hierarchy.

423
00:18:40,939 --> 00:18:44,964
One catch two catch
then memory DRM.

424
00:18:44,964 --> 00:18:47,249
What is behind dram?

425
00:18:47,249 --> 00:18:49,909
Disks, right? And in discs,

426
00:18:49,909 --> 00:18:51,449
we have different kind of disks.

427
00:18:51,449 --> 00:18:54,230
We have SSD, we have different
kind of disks anyway.

428
00:18:54,230 --> 00:18:56,670
Yeah. Okay. But we don't
care about that part, okay?

429
00:18:56,670 --> 00:18:59,050
And here, it is a rough speed

430
00:18:59,050 --> 00:19:02,069
read run speed for
each layer, okay?

431
00:19:02,230 --> 00:19:05,769
But we want to put memo
on top of this, okay?

432
00:19:05,769 --> 00:19:08,310
So apparently, this
is too complicated.

433
00:19:08,310 --> 00:19:10,844
Okay, we are going to
simplify a little bit.

434
00:19:10,844 --> 00:19:12,959
We only have three layers.

435
00:19:12,959 --> 00:19:15,859
The first layer is the
course ALU, right?

436
00:19:15,859 --> 00:19:19,759
Course which are
performing competition.

437
00:19:19,759 --> 00:19:21,459
The second layer are basically

438
00:19:21,459 --> 00:19:23,260
those storage that are
local to the course,

439
00:19:23,260 --> 00:19:24,720
which are basically registers.

440
00:19:24,720 --> 00:19:29,199
Okay? The third layer is a DRAM.

441
00:19:29,199 --> 00:19:32,119
And we know this is
computing units, right?

442
00:19:32,119 --> 00:19:35,120
And we also know registers
are local to cross,

443
00:19:35,120 --> 00:19:37,179
so they are like to invest,

444
00:19:37,179 --> 00:19:39,620
compared to registers,
apparently,

445
00:19:39,620 --> 00:19:41,420
DRAM is so slow,

446
00:19:41,420 --> 00:19:43,800
it's turtle is slow,

447
00:19:43,800 --> 00:19:46,119
we are going to apply Mm

448
00:19:46,119 --> 00:19:48,860
on this and see how
we can do better.

449
00:19:49,400 --> 00:19:52,300
Before we do that, I
still want to remind

450
00:19:52,300 --> 00:19:55,379
you to calculate how to
estimate aismont intensity.

451
00:19:55,379 --> 00:19:57,380
We just count how
many red and reds,

452
00:19:57,380 --> 00:19:59,180
still remember this loop,

453
00:19:59,180 --> 00:20:03,719
and the aismon intensity
of the first loop is 1/3,

454
00:20:03,719 --> 00:20:05,520
and the second one
is three by five.

455
00:20:05,520 --> 00:20:08,060
Because in the second loop
we fuse operating together.

456
00:20:08,060 --> 00:20:09,199
Okay.

457
00:20:10,360 --> 00:20:13,500
Then I'm going to rewrite

458
00:20:13,500 --> 00:20:16,839
the three layer loop of
metamol into this format.

459
00:20:16,839 --> 00:20:19,380
I will let you
appreciate this loop

460
00:20:19,380 --> 00:20:22,560
for 10 seconds, then
I'm going to explain.

461
00:20:29,930 --> 00:20:31,689
Okay.

462
00:20:31,689 --> 00:20:34,349
Nothing changes,
but I just noted

463
00:20:34,349 --> 00:20:37,250
the origin of the arrays.

464
00:20:37,250 --> 00:20:40,189
So here, uh, I noted

465
00:20:40,189 --> 00:20:43,190
that originally the array
was stored on DRAM,

466
00:20:43,190 --> 00:20:44,529
which is the second layer, if

467
00:20:44,529 --> 00:20:46,109
you remember the
memory hierarchy.

468
00:20:46,109 --> 00:20:48,570
In order to perform
computer on ALU,

469
00:20:48,570 --> 00:20:50,969
I have to move things
from DRAM to ALU

470
00:20:50,969 --> 00:20:54,090
to local storage,
which is register.

471
00:20:54,090 --> 00:20:56,390
So I have to basically create

472
00:20:56,390 --> 00:20:58,090
a tempor storage here and I move

473
00:20:58,090 --> 00:20:59,969
things from DRAM to register,

474
00:20:59,969 --> 00:21:02,770
and this will trigger IO ad.

475
00:21:02,770 --> 00:21:03,749
Okay.

476
00:21:03,749 --> 00:21:06,490
That's all. Nothing changes.

477
00:21:06,490 --> 00:21:08,929
Now let's try to
count how many ads we

478
00:21:08,929 --> 00:21:10,370
did in this loop

479
00:21:10,370 --> 00:21:12,670
because this is the
most original version,

480
00:21:12,670 --> 00:21:15,289
original flavor of metamo,

481
00:21:15,970 --> 00:21:18,329
we basically count
how many times we

482
00:21:18,329 --> 00:21:20,150
read A read B and how many times

483
00:21:20,150 --> 00:21:24,129
we write to C Bet
A, B, C are on DRM.

484
00:21:24,129 --> 00:21:27,269
So when we count the
number of times we read A,

485
00:21:27,269 --> 00:21:29,390
we basically see how many times

486
00:21:29,390 --> 00:21:32,750
this line was executed
and it's pretty easy.

487
00:21:32,750 --> 00:21:34,890
This line is inside of
this three layer loop,

488
00:21:34,890 --> 00:21:40,615
so it's cube, It was
executed cube times, okay?

489
00:21:40,615 --> 00:21:44,739
And similarly for B, it was
executed in cube times.

490
00:21:44,739 --> 00:21:46,879
When we read the results back,

491
00:21:46,879 --> 00:21:49,319
you can see this line was
outside of this loop, right?

492
00:21:49,319 --> 00:21:51,360
Because, yeah,

493
00:21:51,360 --> 00:21:54,739
we basically multiply this in
order to get one results in

494
00:21:54,739 --> 00:21:57,999
the resulting array C. So we

495
00:21:57,999 --> 00:21:59,739
can get the readon right
which is basically

496
00:21:59,739 --> 00:22:01,739
in cube and cube
and square. Okay?

497
00:22:01,739 --> 00:22:05,260
Does that make sense? Cool.

498
00:22:05,630 --> 00:22:10,109
And this is the number
of read and red we have.

499
00:22:10,109 --> 00:22:13,849
And the second question is,
how many space we use here.

500
00:22:13,849 --> 00:22:16,109
Of course, we use
some space in dim,

501
00:22:16,109 --> 00:22:18,729
but dim is pretty large, so
we don't care about that.

502
00:22:18,729 --> 00:22:21,489
So we care more about how
many registers we use.

503
00:22:21,489 --> 00:22:22,790
And from this loop,

504
00:22:22,790 --> 00:22:25,930
you can see we actually only
allocated three registers,

505
00:22:25,930 --> 00:22:28,889
one to store the
intermediate value C

506
00:22:28,889 --> 00:22:31,429
and the other two small ANB,

507
00:22:31,429 --> 00:22:33,949
which basically read
things from dim. Okay?

508
00:22:33,949 --> 00:22:37,229
So the space we use
is three, okay.

509
00:22:38,530 --> 00:22:41,389
Basically the reading
cost is basically

510
00:22:41,389 --> 00:22:43,829
two times cube times the speed

511
00:22:43,829 --> 00:22:46,449
of moving one element from

512
00:22:46,449 --> 00:22:50,970
dram to register. This
is the time cost.

513
00:22:50,970 --> 00:22:56,189
Does that make
sense? Cool. Now I'm

514
00:22:56,189 --> 00:22:59,689
going to introduce a more
advanced version of Mm.

515
00:23:03,930 --> 00:23:14,359
Sorry. Like I don't think
I get the question.

516
00:23:19,960 --> 00:23:22,980
Because the original
value was stored in DR,

517
00:23:22,980 --> 00:23:25,400
we have to move it to register.

518
00:23:32,280 --> 00:23:35,000
Move it manually?

519
00:23:42,040 --> 00:23:48,459
Yeah, we

520
00:23:48,459 --> 00:23:50,960
can do that. I will
talk about that later.

521
00:23:50,960 --> 00:23:53,600
Yeah. I think you
already spot point.

522
00:23:53,600 --> 00:23:56,140
It's not efficient. You don't

523
00:23:56,140 --> 00:23:58,039
have to move it once. You
can just put it there.

524
00:23:58,039 --> 00:24:00,659
You don't move it again,
right. That's why

525
00:24:00,659 --> 00:24:02,520
here we only use
three registers.

526
00:24:02,520 --> 00:24:04,459
In your case, you are going
to use more than three

527
00:24:04,459 --> 00:24:06,219
because next time
you need to move.

528
00:24:06,219 --> 00:24:09,439
Okay. Okay.

529
00:24:09,439 --> 00:24:11,799
Then I'm going to introduce
something that is

530
00:24:11,799 --> 00:24:13,140
so fundamental that I hope

531
00:24:13,140 --> 00:24:14,559
all of you guys can understand?

532
00:24:14,559 --> 00:24:16,160
Because this is so fundamental

533
00:24:16,160 --> 00:24:17,319
that if you don't understand,

534
00:24:17,319 --> 00:24:19,500
you are not going to understand
the flash attention,

535
00:24:19,500 --> 00:24:20,939
and you are not going to

536
00:24:20,939 --> 00:24:23,699
understand all the
attenon related stuff.

537
00:24:23,699 --> 00:24:27,219
This is called registered
tiled metamor.

538
00:24:27,219 --> 00:24:30,249
This is a slightly improved
version of met Mo.

539
00:24:30,249 --> 00:24:33,740
Um, and I'm going to give you
a very short description,

540
00:24:33,740 --> 00:24:35,079
and I'm going to
give you 10 seconds

541
00:24:35,079 --> 00:24:36,480
to appreciate this loop again.

542
00:24:36,480 --> 00:24:39,420
Okay. What we do is
basically we reship

543
00:24:39,420 --> 00:24:40,779
the original three arrays

544
00:24:40,779 --> 00:24:43,840
by from the two dimension
to four dimension.

545
00:24:43,840 --> 00:24:45,299
Here, what we do is basically we

546
00:24:45,299 --> 00:24:47,099
introduce two smaller numbers,

547
00:24:47,099 --> 00:24:49,540
definitely smaller than
the original dimension.

548
00:24:49,540 --> 00:24:51,700
And we tell the array.

549
00:24:51,700 --> 00:24:55,359
We have so many of these kind
of smaller matrices, right?

550
00:24:55,359 --> 00:24:57,419
And we put the rest
of dimension here.

551
00:24:57,419 --> 00:24:59,240
Okay. And here we apparently

552
00:24:59,240 --> 00:25:02,439
introduce three uh
telling factors.

553
00:25:02,439 --> 00:25:04,784
V one, V two and V three.

554
00:25:04,784 --> 00:25:07,649
And then what we do is we

555
00:25:07,649 --> 00:25:10,949
rewrite the original
loop into this. Okay?

556
00:25:10,949 --> 00:25:14,589
I would like to appreciate
this loop a little bit.

557
00:25:16,270 --> 00:25:19,009
I want to make sure
that this loop is

558
00:25:19,009 --> 00:25:22,470
equivalent to the original
loop I wrote, okay?

559
00:25:32,880 --> 00:25:35,800
Okay. Indeed, it's
equivalent, why?

560
00:25:35,800 --> 00:25:38,000
Because we are
basically transforming

561
00:25:38,000 --> 00:25:42,320
the original memo into a
block wise memo, here.

562
00:25:42,320 --> 00:25:45,479
Every time I basically
will take a row

563
00:25:45,479 --> 00:25:49,539
with the number of original
rows equal to one,

564
00:25:49,539 --> 00:25:51,899
and take a column from
here with umber of

565
00:25:51,899 --> 00:25:54,999
original columns from
B equal to weight two.

566
00:25:54,999 --> 00:25:57,099
And then I have a
third loop which

567
00:25:57,099 --> 00:25:59,559
will basically scan
from left to right,

568
00:25:59,559 --> 00:26:01,159
from top to bottom,

569
00:26:01,159 --> 00:26:03,659
and calculate the results in

570
00:26:03,659 --> 00:26:07,860
this small block and write the
result back to this block.

571
00:26:07,860 --> 00:26:12,760
Does that make
sense? Cool, cool.

572
00:26:12,760 --> 00:26:16,080
Okay. Now, you understand
this loop, okay?

573
00:26:16,080 --> 00:26:20,000
And this loop is so magical
that we can basically again,

574
00:26:20,000 --> 00:26:23,594
count the red and red
and see what happens.

575
00:26:23,594 --> 00:26:25,329
We are going to first,

576
00:26:25,329 --> 00:26:27,150
like what we did for
the original loop,

577
00:26:27,150 --> 00:26:29,029
we are going to count
how many times we

578
00:26:29,029 --> 00:26:33,490
read A from DRM to register.

579
00:26:33,490 --> 00:26:36,190
I think it's straightforward.

580
00:26:36,190 --> 00:26:38,129
So from this line,

581
00:26:38,129 --> 00:26:40,150
you can see every time we read

582
00:26:40,150 --> 00:26:44,149
array of size one by way three.

583
00:26:44,149 --> 00:26:46,370
We write it to this small array.

584
00:26:46,370 --> 00:26:47,909
We just count how many times we

585
00:26:47,909 --> 00:26:50,010
ask you this line.
How many times?

586
00:26:50,010 --> 00:26:52,889
The first loop is
undivided by we one.

587
00:26:52,889 --> 00:26:54,589
Second is undivided by way two,

588
00:26:54,589 --> 00:26:57,059
and third is undivided
by way three. Right.

589
00:26:57,059 --> 00:26:59,019
We time all these
three together,

590
00:26:59,019 --> 00:27:01,220
and we also time the
shape of the array.

591
00:27:01,220 --> 00:27:03,579
So how many times
we read? You can

592
00:27:03,579 --> 00:27:06,139
see this we one will be
canceled here, right?

593
00:27:06,139 --> 00:27:08,459
And this week three
will be canceled here.

594
00:27:08,459 --> 00:27:13,039
So it's basically cube
by week two, right?

595
00:27:13,240 --> 00:27:15,900
And you found magic here, okay?

596
00:27:15,900 --> 00:27:18,140
But just rewrite the loop.

597
00:27:18,140 --> 00:27:20,959
I already reduced the
time I read A from

598
00:27:20,959 --> 00:27:25,279
cube to a factor that
is divided by we two.

599
00:27:25,279 --> 00:27:31,470
Okay? Any question? Cool.

600
00:27:31,470 --> 00:27:34,349
Similarly, we can do that for B,

601
00:27:34,349 --> 00:27:36,069
we just time this shape with

602
00:27:36,069 --> 00:27:39,249
the loop factors and
loop iterations and

603
00:27:39,249 --> 00:27:41,989
we get the times

604
00:27:41,989 --> 00:27:45,089
we read B is basically
cubed divided by w one.

605
00:27:45,089 --> 00:27:48,489
We're already doing pretty
good because we already reduce

606
00:27:48,489 --> 00:27:52,469
both BofactorF it's the

607
00:27:52,469 --> 00:27:56,150
same because results back
from register to DRAM,

608
00:27:56,150 --> 00:27:59,170
so it's still square.

609
00:28:01,770 --> 00:28:06,209
Then the next question is how
many register we use here?

610
00:28:08,450 --> 00:28:10,530
So in order to count

611
00:28:10,530 --> 00:28:11,889
the lumber register
we use, basically,

612
00:28:11,889 --> 00:28:14,429
we just count how many
intermediate memory

613
00:28:14,429 --> 00:28:16,270
we need to store when
performing this loop.

614
00:28:16,270 --> 00:28:18,089
And you can easily see

615
00:28:18,089 --> 00:28:20,049
the lumber register we use here

616
00:28:20,049 --> 00:28:22,289
is this one, this
one, and this one.

617
00:28:22,289 --> 00:28:25,029
And this one is of shape
way one times w two.

618
00:28:25,029 --> 00:28:27,649
This is we one we three,
we two with three.

619
00:28:27,649 --> 00:28:30,310
So lumber register we
use is this number.

620
00:28:30,310 --> 00:28:32,489
We just add them together, okay?

621
00:28:32,489 --> 00:28:35,130
And at a high level, you
basically get the point.

622
00:28:35,130 --> 00:28:37,649
Here we are basically treating
space for time, right?

623
00:28:37,649 --> 00:28:40,070
And we slightly use
more registers,

624
00:28:40,070 --> 00:28:45,009
but we will reduce the
lumber risk we did, okay?

625
00:28:45,990 --> 00:28:50,989
And we also summarize the
ad cost here is cube by

626
00:28:50,989 --> 00:28:53,869
two plus cb did
but we want times

627
00:28:53,869 --> 00:28:58,029
the speed of moving contents
from DM to register.

628
00:28:58,029 --> 00:29:00,530
And this is already doing
better than the previous loop.

629
00:29:00,530 --> 00:29:01,850
And you can verify by yourself.

630
00:29:01,850 --> 00:29:05,050
You go back and bring
up C plus plus,

631
00:29:05,050 --> 00:29:07,090
editor and try to
register loops.

632
00:29:07,090 --> 00:29:08,329
You'll find the
second one actually

633
00:29:08,329 --> 00:29:11,269
actually much faster than
the first one, okay?

634
00:29:16,270 --> 00:29:17,749
Yeah.

635
00:29:17,749 --> 00:29:19,770
So here, in order to
perform this loop,

636
00:29:19,770 --> 00:29:22,529
I need to allocate
something register, right?

637
00:29:22,529 --> 00:29:25,389
And we just look at this loop
and see where we allocate.

638
00:29:25,389 --> 00:29:27,430
So in this line,
we allocate array

639
00:29:27,430 --> 00:29:30,709
of shape w by way too, right?

640
00:29:30,709 --> 00:29:33,229
And in these two
lines, we allocate

641
00:29:33,229 --> 00:29:36,270
another small array A and B.
We just add them together.

642
00:29:36,270 --> 00:29:39,470
Okay. Clear? Cool.

643
00:29:40,820 --> 00:29:45,120
Okay, before I move into
something that's even deeper.

644
00:29:45,120 --> 00:29:48,339
Let's try to understand
reveal what's going on here.

645
00:29:48,339 --> 00:29:51,919
Okay? So the first thing

646
00:29:51,919 --> 00:29:53,840
you noted by looking
at these terms,

647
00:29:53,840 --> 00:29:55,420
the first thing you
notice is you'll

648
00:29:55,420 --> 00:29:57,199
find that when I do telling,

649
00:29:57,199 --> 00:29:59,299
actually I introduce
I introduced

650
00:29:59,299 --> 00:30:02,179
three tell factors, we
want me with three.

651
00:30:02,179 --> 00:30:04,060
But it ended up with a cost

652
00:30:04,060 --> 00:30:07,659
that is not related
to with three, right?

653
00:30:07,659 --> 00:30:10,739
Okay This is something
really interesting.

654
00:30:10,739 --> 00:30:12,119
That means that when

655
00:30:12,119 --> 00:30:13,999
I try to optimize
this kind of metamor,

656
00:30:13,999 --> 00:30:15,619
I can arbitrarily choose with

657
00:30:15,619 --> 00:30:18,980
three and that will not
affect my performance.

658
00:30:18,980 --> 00:30:22,500
Does that make sense? Cool.

659
00:30:22,780 --> 00:30:25,299
The second question
I want to ask is how

660
00:30:25,299 --> 00:30:27,959
we can set the value for.

661
00:30:27,959 --> 00:30:31,200
Apparently, we want to
influence a lot of things.

662
00:30:31,200 --> 00:30:33,260
I influence our speed.

663
00:30:34,640 --> 00:30:38,899
Apparently, we want to be as
large as possible, right?

664
00:30:38,899 --> 00:30:41,640
Because otherwise, um,
because if they are large,

665
00:30:41,640 --> 00:30:43,760
then our read will be smaller.

666
00:30:43,760 --> 00:30:45,560
But we are subject
to constraints.

667
00:30:45,560 --> 00:30:48,359
That is, we have limited
number of registers.

668
00:30:48,359 --> 00:30:51,159
So we are subject to
a constraint that is,

669
00:30:51,159 --> 00:30:56,740
um, basically, this number,

670
00:30:56,740 --> 00:30:58,859
this line should be smaller than

671
00:30:58,859 --> 00:31:02,799
the total registers
offered in ALU CPU,

672
00:31:02,799 --> 00:31:07,079
right in the core that's
one constraint we have to,

673
00:31:07,079 --> 00:31:10,000
uh, you know, respect, okay?

674
00:31:11,170 --> 00:31:13,669
Let me ask a
fundamental question.

675
00:31:13,669 --> 00:31:16,910
Why essentially can
telling reduce the cost?

676
00:31:16,910 --> 00:31:19,029
Yeah, at a high level, yes,

677
00:31:19,029 --> 00:31:23,850
we are essentially treat
space for IO, but why?

678
00:31:23,850 --> 00:31:26,290
Can anyone explain?

679
00:31:26,330 --> 00:31:36,169
Yeah. Yeah, exactly.

680
00:31:36,169 --> 00:31:39,949
In the original cube
implementation,

681
00:31:39,949 --> 00:31:42,130
every time we try
to perform element,

682
00:31:42,130 --> 00:31:45,289
we have to move them from
memory from DRAM to register.

683
00:31:45,289 --> 00:31:47,670
But here, we first move a block,

684
00:31:47,670 --> 00:31:50,229
but we fix them and
we read the rest of

685
00:31:50,229 --> 00:31:52,890
things which means that when
we read the rest of things,

686
00:31:52,890 --> 00:31:55,150
actually we are using
the previous load

687
00:31:55,150 --> 00:31:58,949
from DRAM to register.

688
00:31:58,990 --> 00:32:01,969
I hope everyone get this one.

689
00:32:01,969 --> 00:32:06,350
This is called
register tut Mm, okay?

690
00:32:06,510 --> 00:32:08,869
Okay, let's go deeper.

691
00:32:08,869 --> 00:32:11,909
Okay? So you know, reality
is not this right.

692
00:32:11,909 --> 00:32:13,129
In reality, we don't have

693
00:32:13,129 --> 00:32:14,749
three layers of
memory hierarchy.

694
00:32:14,749 --> 00:32:17,969
We have CPU A, we have
registered, we have DRAM,

695
00:32:17,969 --> 00:32:21,270
but we actually have another
layer that is called

696
00:32:21,270 --> 00:32:25,270
LON catch and I think LON catch
is fast, is rapidly fast.

697
00:32:25,270 --> 00:32:27,009
Okay. It's definitely
faster than DRM.

698
00:32:27,009 --> 00:32:28,909
Okay. So in reality,

699
00:32:28,909 --> 00:32:30,150
we probably have three layers.

700
00:32:30,150 --> 00:32:32,349
Okay. So how we can
make this happen,

701
00:32:32,349 --> 00:32:33,889
how we can basically put metma

702
00:32:33,889 --> 00:32:35,355
on top of this memory hierarchy.

703
00:32:35,355 --> 00:32:36,219
Okay.

704
00:32:36,219 --> 00:32:40,720
I'm going to introduce
another telling technique

705
00:32:40,720 --> 00:32:43,679
that is called
catch away telling.

706
00:32:45,360 --> 00:32:50,119
Um, so here, there's nothing
significantly different.

707
00:32:50,119 --> 00:32:52,200
Okay. What we do
is basically now,

708
00:32:52,200 --> 00:32:55,219
we cannot directly move
contents from DRAM to register.

709
00:32:55,219 --> 00:32:58,899
We have to move from DRAM to
LON catch then Oca register.

710
00:32:58,899 --> 00:33:01,919
And we're going to embed
these two loops together.

711
00:33:01,919 --> 00:33:07,500
Uh here we first tell
between DRAM and ON catch.

712
00:33:07,500 --> 00:33:09,810
The way I do telling
is the same.

713
00:33:09,810 --> 00:33:13,120
Um, I just basically tell
one dimension, okay?

714
00:33:13,120 --> 00:33:15,719
I introduce two parameters
B one and B two.

715
00:33:15,719 --> 00:33:17,659
You know, B three
doesn't matter, right?

716
00:33:17,659 --> 00:33:20,380
So I already explained that
B three doesn't matter.

717
00:33:20,380 --> 00:33:22,859
I simply said B
three as one here.

718
00:33:22,859 --> 00:33:24,799
Because B three is not going to,

719
00:33:24,799 --> 00:33:27,459
um, influence our
performance. Okay.

720
00:33:27,459 --> 00:33:29,639
And what I do is basically,

721
00:33:29,639 --> 00:33:31,519
um, I do this kind of tilling.

722
00:33:31,519 --> 00:33:34,539
So for here, every time I
pick up a row for this,

723
00:33:34,539 --> 00:33:36,859
every time I pick up
a column and then I

724
00:33:36,859 --> 00:33:39,624
do a dot program and try
to get the results, okay?

725
00:33:39,624 --> 00:33:45,469
And this happens between
DRAM and RNCchm you know,

726
00:33:45,469 --> 00:33:48,629
once I read them, to
perform computer,

727
00:33:48,629 --> 00:33:51,310
that computer won't happen
in RN cache so you still

728
00:33:51,310 --> 00:33:54,469
need to move contents
from OCach to register.

729
00:33:54,469 --> 00:33:56,349
So at that layer, you also

730
00:33:56,349 --> 00:33:59,709
perform metam but in a
much smaller size, right?

731
00:33:59,709 --> 00:34:02,429
And what I do is I'm going to
apply the technique I just

732
00:34:02,429 --> 00:34:04,970
introduced um in
the previous slide,

733
00:34:04,970 --> 00:34:06,549
that is, I'm going to do that

734
00:34:06,549 --> 00:34:09,410
register wire telling, okay?

735
00:34:09,410 --> 00:34:14,489
Does that make sense? Cool.
And this will ended up with,

736
00:34:14,489 --> 00:34:19,669
um, so I already explained this.

737
00:34:19,669 --> 00:34:21,410
The data movement
passes basically

738
00:34:21,410 --> 00:34:23,069
it first starts from DRAM and

739
00:34:23,069 --> 00:34:24,649
then move content from DRAM to

740
00:34:24,649 --> 00:34:27,350
L one cache and then
one cache to register,

741
00:34:27,350 --> 00:34:31,389
This ended up with this loop.

742
00:34:31,860 --> 00:34:36,839
But before we proceed to
the ultimate loop, okay?

743
00:34:36,839 --> 00:34:39,219
We're first analyzing
the cost here, okay?

744
00:34:39,219 --> 00:34:41,179
And I think you guys
already get the idea.

745
00:34:41,179 --> 00:34:43,699
It's very easy to count
how many rates it

746
00:34:43,699 --> 00:34:47,099
happens between DRAM
and catch, right?

747
00:34:47,099 --> 00:34:50,019
So in order to move
A's content from

748
00:34:50,019 --> 00:34:53,460
DRAM to ON CAT what
we do is basically,

749
00:34:53,460 --> 00:34:55,939
um, A happens here, right?

750
00:34:55,939 --> 00:34:58,339
And there's one outer loop.

751
00:34:58,339 --> 00:35:02,379
So we just time this
number divided by B one,

752
00:35:02,379 --> 00:35:04,679
and we time the shape
of the we move,

753
00:35:04,679 --> 00:35:07,404
which is times B one.

754
00:35:07,404 --> 00:35:09,830
So the lumber IO happens

755
00:35:09,830 --> 00:35:11,629
between these two layers

756
00:35:11,629 --> 00:35:14,549
is basically square
for A, right?

757
00:35:14,549 --> 00:35:18,929
And here, what do we do for B,

758
00:35:18,929 --> 00:35:21,609
it has one more
loop compared to A.

759
00:35:21,609 --> 00:35:24,190
So what happens if
we time this lumber,

760
00:35:24,190 --> 00:35:27,129
this lumber, and this lumber
and the shape of the array.

761
00:35:27,129 --> 00:35:30,849
So we get the cost
as Nub div by B.

762
00:35:30,849 --> 00:35:39,250
Okay? And the total cost is
The total c is basically,

763
00:35:39,250 --> 00:35:41,589
um, these two number together.

764
00:35:41,589 --> 00:35:44,570
And as I said, you still
subject to constraint.

765
00:35:44,570 --> 00:35:47,189
That is your LO catch size
cannot be greater than

766
00:35:47,189 --> 00:35:52,029
the available L one catch
offered on your course, right?

767
00:35:52,029 --> 00:35:53,729
So you also care
about the space,

768
00:35:53,729 --> 00:35:55,850
and the space we use is
basically B one Tison

769
00:35:55,850 --> 00:35:58,309
and B two Timson and
we sum them together,

770
00:35:58,309 --> 00:36:01,109
and they should be smaller
than one catch size.

771
00:36:01,109 --> 00:36:03,029
Okay? Exactly the
same thing. Okay.

772
00:36:03,029 --> 00:36:05,270
I just reduce one dimension.

773
00:36:06,280 --> 00:36:11,059
Good with this one,
right? Okay. And then

774
00:36:11,059 --> 00:36:13,900
we are going to put these
two loops together.

775
00:36:13,900 --> 00:36:18,060
And this is a more realistic
more implementation

776
00:36:18,060 --> 00:36:21,599
in today's architectures,
for example,

777
00:36:21,599 --> 00:36:24,700
in GPU and CPU, apparently,

778
00:36:24,700 --> 00:36:26,419
this is still a simplified
version because,

779
00:36:26,419 --> 00:36:29,149
you know, there's
still to catch, right.

780
00:36:29,149 --> 00:36:30,979
Yeah. Okay.

781
00:36:30,980 --> 00:36:34,159
So in the outside of
this loop, we are doing,

782
00:36:34,159 --> 00:36:37,459
catch a wire telling and
inside of this loop,

783
00:36:37,459 --> 00:36:40,219
we are doing register
aware telling, right?

784
00:36:40,219 --> 00:36:43,159
That's what I said
before. And here,

785
00:36:43,159 --> 00:36:45,079
we simply said with
three equal to

786
00:36:45,079 --> 00:36:47,099
one because we know we
three does not matter.

787
00:36:47,099 --> 00:36:49,620
It won't affect performance.

788
00:36:50,930 --> 00:36:54,649
And in order for you to
understand this loop,

789
00:36:54,649 --> 00:36:56,329
we can see on this box,

790
00:36:56,329 --> 00:36:57,429
basically we are doing catch

791
00:36:57,429 --> 00:36:59,069
telling using B one and B two,

792
00:36:59,069 --> 00:37:02,309
and it moves things
from DRAM to OCach.

793
00:37:02,309 --> 00:37:03,869
And then in this green box,

794
00:37:03,869 --> 00:37:06,149
we are doing register
telling using V and V two,

795
00:37:06,149 --> 00:37:08,510
we are moving things
from one to register.

796
00:37:08,510 --> 00:37:10,490
And finally, things
are in register,

797
00:37:10,490 --> 00:37:12,730
we actually proceed
the competition.

798
00:37:12,730 --> 00:37:17,770
Okay? And let's try to
analyze the cost again, okay?

799
00:37:19,590 --> 00:37:22,309
The cost of moving things from

800
00:37:22,309 --> 00:37:25,069
dm to one can be analyzed
in this way, right?

801
00:37:25,069 --> 00:37:26,629
We roughly just look at this.

802
00:37:26,629 --> 00:37:29,669
This is the operation
where we move things.

803
00:37:29,910 --> 00:37:35,289
And the first term is
basically moving this array.

804
00:37:35,289 --> 00:37:39,889
And this array was executed
divided by B one times.

805
00:37:39,889 --> 00:37:42,889
Very tEveryt excuse it

806
00:37:42,889 --> 00:37:47,089
move something in this shape
basically time together.

807
00:37:47,089 --> 00:37:50,509
And we get the cost of moving

808
00:37:50,509 --> 00:37:54,029
things moving A
from dram to one,

809
00:37:54,029 --> 00:37:56,610
which is square, okay?

810
00:37:56,610 --> 00:37:59,670
Similarly, when we move things

811
00:37:59,670 --> 00:38:03,089
from DRM to catch and move B,

812
00:38:03,089 --> 00:38:04,850
we just time this factor,

813
00:38:04,850 --> 00:38:06,690
this factor and the shape

814
00:38:06,690 --> 00:38:08,449
together and we get this number.

815
00:38:08,449 --> 00:38:12,749
And then we proceed to the
inner loop and we can also

816
00:38:12,749 --> 00:38:14,829
count uh the times we move

817
00:38:14,829 --> 00:38:17,230
things between one and register.

818
00:38:17,230 --> 00:38:18,789
And you can double check.

819
00:38:18,789 --> 00:38:25,809
This is correct.
Okay. Yeah, that's

820
00:38:25,809 --> 00:38:27,949
all we have for this loop. Okay?

821
00:38:27,949 --> 00:38:29,470
And you can see in reality,

822
00:38:29,470 --> 00:38:31,949
this is super
complicated, right?

823
00:38:31,949 --> 00:38:34,670
So what makes it complicated?

824
00:38:34,670 --> 00:38:38,970
So in practice, um,
for example, on CPU,

825
00:38:38,970 --> 00:38:41,209
we have a disc to dram to

826
00:38:41,209 --> 00:38:43,910
one to one to two
to one to register.

827
00:38:43,910 --> 00:38:45,370
That means we are doing telling

828
00:38:45,370 --> 00:38:47,729
across at least three layers.

829
00:38:47,729 --> 00:38:49,769
Probably we can ignore disc.

830
00:38:49,769 --> 00:38:52,770
That's one thing that
makes things complicated.

831
00:38:52,770 --> 00:38:54,889
The second thing is, we need to

832
00:38:54,889 --> 00:38:58,789
determine the values for 1b2b1,

833
00:38:58,789 --> 00:39:02,689
b2c1 and C two, Because if
you still remember, uh,

834
00:39:02,689 --> 00:39:05,029
these telling factors
need to be subject to

835
00:39:05,029 --> 00:39:07,769
the space constraints
at each layer.

836
00:39:07,769 --> 00:39:09,569
We cannot be more than that.

837
00:39:09,569 --> 00:39:12,709
Otherwise, this loop is
not going to run, okay?

838
00:39:14,050 --> 00:39:16,990
And the third thing that you
can make this even faster

839
00:39:16,990 --> 00:39:20,429
is when we are reading
things from DRM to L two,

840
00:39:20,429 --> 00:39:24,229
for example, you can also
pipeline in the reading, right?

841
00:39:24,229 --> 00:39:26,930
You can also let the
CPU to read scenes

842
00:39:26,930 --> 00:39:31,530
2-1 or concurrently
one to register,

843
00:39:31,530 --> 00:39:33,130
and all this can be pipeline,

844
00:39:33,130 --> 00:39:34,969
so you can basically even

845
00:39:34,969 --> 00:39:37,489
reduce the time that you
use to read scenes, right?

846
00:39:37,489 --> 00:39:40,530
You transfer contents
between memory hierarchies.

847
00:39:43,130 --> 00:39:45,269
And all these, like I said,

848
00:39:45,269 --> 00:39:47,049
need to subject to
the size of L two,

849
00:39:47,049 --> 00:39:50,289
L one cache and registers. Okay?

850
00:39:50,289 --> 00:39:52,509
That basically finish most of

851
00:39:52,509 --> 00:39:55,369
the contents about
telling, please.

852
00:40:00,050 --> 00:40:02,230
Yeah. Most cases.

853
00:40:02,230 --> 00:40:11,609
Yeah. Yeah. Please. D was what?

854
00:40:17,880 --> 00:40:19,699
That's a really good question.

855
00:40:19,699 --> 00:40:21,299
I don't know. Yeah,
I don't know.

856
00:40:21,299 --> 00:40:23,479
I think those line
of implementation,

857
00:40:23,479 --> 00:40:24,959
they focus on different parts.

858
00:40:24,959 --> 00:40:26,240
They focus on reducing

859
00:40:26,240 --> 00:40:29,280
the compute complexity,
if you remember.

860
00:40:29,280 --> 00:40:32,539
This telling is essential
in reducing the IO.

861
00:40:32,539 --> 00:40:35,359
So they are fungon
theoretically,

862
00:40:35,359 --> 00:40:37,859
you can implement more
advanced method that has

863
00:40:37,859 --> 00:40:41,159
a lower complexity while
you still implement this.

864
00:40:41,159 --> 00:40:47,079
Yeah, there are
some limitations,

865
00:40:47,079 --> 00:40:48,859
but in Koda, I think

866
00:40:48,859 --> 00:40:52,220
most people still
adopt cube algorithm.

867
00:40:52,220 --> 00:40:55,719
Yeah. Okay. Yeah.

868
00:40:57,680 --> 00:41:01,999
Yeah, we are going to do
this on GPO again next week.

869
00:41:01,999 --> 00:41:04,140
And that will be
more complicated.

870
00:41:04,140 --> 00:41:06,959
So make sure you spend time
to understand these loops.

871
00:41:06,959 --> 00:41:08,760
Okay? This is so essential.

872
00:41:08,760 --> 00:41:11,299
And I can't see more on this.

873
00:41:11,299 --> 00:41:14,339
Okay? So flash attention is
basically based on this.

874
00:41:14,339 --> 00:41:15,820
But in flash attention,

875
00:41:15,820 --> 00:41:18,279
what do we do is on
top of that met M we

876
00:41:18,279 --> 00:41:21,799
add another operator
which is even guess.

877
00:41:21,799 --> 00:41:24,079
Softmax. We just put

878
00:41:24,079 --> 00:41:26,420
softmax and Mtmo together,
and we do telling.

879
00:41:26,420 --> 00:41:28,669
That's basically flash
attention. Okay?

880
00:41:28,669 --> 00:41:30,219
Oh.

881
00:41:35,140 --> 00:41:39,759
They, they do this. Yeah.
Yeah. Yeah. This was

882
00:41:39,759 --> 00:41:41,019
not something super new,

883
00:41:41,019 --> 00:41:42,280
but it is so fundamental.

884
00:41:42,280 --> 00:41:46,899
Yeah, yeah. Okay. Okay.

885
00:41:46,899 --> 00:41:48,679
Now, let's review
the final question.

886
00:41:48,679 --> 00:41:52,879
While telling works? The reason

887
00:41:52,879 --> 00:41:54,440
telling works
basically you reuse

888
00:41:54,440 --> 00:41:57,259
some loading many of you
already understand this.

889
00:41:57,259 --> 00:42:00,859
If you look at the
mathematical format of

890
00:42:00,859 --> 00:42:03,499
Met M you'll find

891
00:42:03,499 --> 00:42:06,699
that there are some independent
access of the array.

892
00:42:06,699 --> 00:42:08,779
So the acess of the array

893
00:42:08,779 --> 00:42:10,859
A is independent of
the dimension of

894
00:42:10,859 --> 00:42:12,959
J B J is

895
00:42:12,959 --> 00:42:14,099
only one dimension of B

896
00:42:14,099 --> 00:42:15,660
and A does not have
that dimension.

897
00:42:15,660 --> 00:42:17,619
You can always read something in

898
00:42:17,619 --> 00:42:19,680
the other irrelevant dimension

899
00:42:19,680 --> 00:42:23,099
and you try to reuse some
loading from the first array,

900
00:42:23,099 --> 00:42:25,699
we can tell the J
dimension by one,

901
00:42:25,699 --> 00:42:28,639
and it will enable us
reuse of A for one times.

902
00:42:28,639 --> 00:42:31,459
That's why we can reduce
the load by one times.

903
00:42:31,459 --> 00:42:38,139
Okay. Okay. After
finishing this,

904
00:42:38,139 --> 00:42:39,399
I'm going to introduce a very,

905
00:42:39,399 --> 00:42:42,200
very good homework candidate,

906
00:42:43,360 --> 00:42:47,399
is this. This is pretty good.

907
00:42:47,399 --> 00:42:48,899
This is what I did
for my homework when

908
00:42:48,899 --> 00:42:50,800
I learned machinery systems.

909
00:42:50,800 --> 00:42:52,999
Can anyone guess what is?

910
00:42:53,590 --> 00:42:56,729
This is a operator that we

911
00:42:56,729 --> 00:42:59,229
all are very familiar with
it's count two D. Okay.

912
00:42:59,229 --> 00:43:01,849
Count two is essentially
seven layer loop.

913
00:43:01,849 --> 00:43:04,269
It's much more complicated
metamor, okay?

914
00:43:04,269 --> 00:43:06,329
That's a joke. I'm not
going to let you do this.

915
00:43:06,329 --> 00:43:08,509
Okay. The reason I do
this is because when

916
00:43:08,509 --> 00:43:11,389
I learned this count is
the chosen one, right?

917
00:43:11,389 --> 00:43:13,669
But when you learn this,
count is not the chosen one.

918
00:43:13,669 --> 00:43:15,969
I think transformer and
tenting is the chosen one.

919
00:43:15,969 --> 00:43:20,459
I'm going to let you do this
telling for a tenting, okay.

920
00:43:20,459 --> 00:43:24,389
Because it's more
relevant. Okay. And we

921
00:43:24,389 --> 00:43:25,529
can go through this a little

922
00:43:25,529 --> 00:43:27,269
bit because we are going
to talk about this later.

923
00:43:27,269 --> 00:43:28,989
But essentially count two is

924
00:43:28,989 --> 00:43:30,929
basically a seven layer loop,

925
00:43:30,929 --> 00:43:32,989
and you have a
simple spatial loop.

926
00:43:32,989 --> 00:43:35,929
You have some stencil
computation loop,

927
00:43:35,929 --> 00:43:37,389
and you have a reduction loop.

928
00:43:37,389 --> 00:43:40,130
Remember, this reduction
loop also exists

929
00:43:40,130 --> 00:43:44,729
in met Mol Metamlthird loop
is basically reduction loop,

930
00:43:44,930 --> 00:43:47,389
you have another reduction loop.

931
00:43:47,389 --> 00:43:50,169
But usually this KH and

932
00:43:50,169 --> 00:43:51,330
Kw is basically

933
00:43:51,330 --> 00:43:53,809
the filter size, so you
don't care about that.

934
00:43:53,809 --> 00:43:56,229
It's so small that you
just do it as it is.

935
00:43:56,229 --> 00:44:00,689
Okay. And in order
to do telling, uh,

936
00:44:00,689 --> 00:44:02,589
you basically need to tell

937
00:44:02,589 --> 00:44:06,970
some layer loop against four
layer memory hierarchy.

938
00:44:06,970 --> 00:44:08,769
A lot of possibilities.

939
00:44:08,769 --> 00:44:12,209
And this is really hard
and that's why you know,

940
00:44:12,209 --> 00:44:17,350
when people do this, you
can get lost, sometimes.

941
00:44:17,350 --> 00:44:20,909
Okay. Cool, cool.

942
00:44:20,909 --> 00:44:23,009
Yeah, that's our first lesson on

943
00:44:23,009 --> 00:44:24,829
Mtmo and I hope you
understand this tell

944
00:44:24,829 --> 00:44:29,470
the Mtmooa that's the deepest
part of this lecture.

945
00:44:29,470 --> 00:44:31,470
And next, we are
going to proceed

946
00:44:31,470 --> 00:44:34,389
into something that is
really interesting. Okay.

947
00:44:34,390 --> 00:44:37,289
So one way, like I said,

948
00:44:37,289 --> 00:44:40,150
one way to make Mtmo is
basically you do this ting.

949
00:44:40,150 --> 00:44:45,229
The other way is you
utilize DPU accelerators,

950
00:44:45,229 --> 00:44:48,469
and those accelerator devices
will make this super fast.

951
00:44:48,469 --> 00:44:50,709
And next we are trying
to understand why

952
00:44:50,709 --> 00:44:52,709
this can make more fast.

953
00:44:52,709 --> 00:44:55,350
Okay, how does that
accelerator work, okay?

954
00:44:55,350 --> 00:44:58,509
So before we talk
about why, I think,

955
00:44:58,509 --> 00:45:02,529
um, at a high level, you
probably already know.

956
00:45:02,529 --> 00:45:05,249
Essentially GPU is trying to
paralyze this loop, right?

957
00:45:05,249 --> 00:45:07,249
And GPU has so many courses that

958
00:45:07,249 --> 00:45:09,629
each can work on different
part of this loop.

959
00:45:09,629 --> 00:45:13,130
So essentially the same
fundamental to CPU.

960
00:45:13,130 --> 00:45:14,450
Like you allocate more threads,

961
00:45:14,450 --> 00:45:16,690
more curs to work on different
parts of the competition.

962
00:45:16,690 --> 00:45:18,389
But how exactly that works?

963
00:45:18,389 --> 00:45:21,729
Before I introduce
GPUS I think, uh,

964
00:45:21,729 --> 00:45:23,550
I need to bring up a concept

965
00:45:23,550 --> 00:45:24,769
that most of you should be very

966
00:45:24,769 --> 00:45:26,070
familiar with is basically

967
00:45:26,070 --> 00:45:28,609
single instruction,
multiple data, right?

968
00:45:28,609 --> 00:45:31,769
That is the, um, um,

969
00:45:31,769 --> 00:45:33,329
essential part of multi

970
00:45:33,329 --> 00:45:35,190
core computing,
parallel computing.

971
00:45:35,190 --> 00:45:37,589
So in single instruction
multiple data,

972
00:45:37,589 --> 00:45:39,750
um, what you do is essentially,

973
00:45:39,750 --> 00:45:43,089
um, you have a different course

974
00:45:43,089 --> 00:45:45,110
and each core will ask
you the same program,

975
00:45:45,110 --> 00:45:46,489
and you part in the data.

976
00:45:46,489 --> 00:45:48,449
You give each core a part
of the data and you will

977
00:45:48,449 --> 00:45:50,590
proceed with the same
program and output results,

978
00:45:50,590 --> 00:45:53,130
and then you grade results,
you get the final results.

979
00:45:53,130 --> 00:45:54,549
Okay. Because all these course

980
00:45:54,549 --> 00:45:57,029
process together and they
only take a part of data,

981
00:45:57,029 --> 00:46:00,229
so you can you enjoy the
benefit of paralization,

982
00:46:00,229 --> 00:46:02,830
right, which is speed ups, okay?

983
00:46:03,930 --> 00:46:06,909
And as you probably know,

984
00:46:06,909 --> 00:46:08,689
CPO is not pretty good at this

985
00:46:08,689 --> 00:46:11,170
because CPU has a limited
number of course.

986
00:46:11,170 --> 00:46:13,549
The reason is because, um,

987
00:46:13,549 --> 00:46:16,609
the reason is how ChiP
was designed, okay?

988
00:46:16,609 --> 00:46:18,269
Like, originally, we try to

989
00:46:18,269 --> 00:46:20,530
in order to design
a chip like CPU,

990
00:46:20,530 --> 00:46:22,569
we need to put a lot
of components in

991
00:46:22,569 --> 00:46:24,969
addition to compute
units, right?

992
00:46:24,969 --> 00:46:26,750
So we need to put some control.

993
00:46:26,750 --> 00:46:29,989
For example, CPU needs to
understand the IL, okay?

994
00:46:29,989 --> 00:46:31,690
We also need to
put some catches,

995
00:46:31,690 --> 00:46:33,449
L cache to cache.

996
00:46:33,449 --> 00:46:36,649
And we only have a limited
area because we need to

997
00:46:36,649 --> 00:46:39,869
make CPU small and it cannot
consume a lot of power,

998
00:46:39,869 --> 00:46:41,709
otherwise, it cannot be

999
00:46:41,709 --> 00:46:43,329
deployed on laptop
or phones, right?

1000
00:46:43,329 --> 00:46:46,609
So we need to have a
physical limitation on area.

1001
00:46:46,609 --> 00:46:48,310
So if you put some controls,

1002
00:46:48,310 --> 00:46:50,579
you put some catches then

1003
00:46:50,579 --> 00:46:53,480
you only have that blank area
where you can put cross,

1004
00:46:53,480 --> 00:46:56,259
so we put four ALERs here.

1005
00:46:56,259 --> 00:46:59,199
Then people try to make this
faster and faster faster.

1006
00:46:59,199 --> 00:47:01,239
That's what happened in
the last 20 years in

1007
00:47:01,239 --> 00:47:04,800
Cilic Valley people are making
better and better chips.

1008
00:47:04,800 --> 00:47:07,500
They are able to still
put controls and catches,

1009
00:47:07,500 --> 00:47:11,100
but where the technology

1010
00:47:11,100 --> 00:47:12,639
advances is basically
they are able to

1011
00:47:12,639 --> 00:47:15,299
make smaller and
smaller ALUs that

1012
00:47:15,299 --> 00:47:17,099
will basically have the
same computing power,

1013
00:47:17,099 --> 00:47:18,740
but much smaller.

1014
00:47:18,740 --> 00:47:20,260
Because it's much smaller,

1015
00:47:20,260 --> 00:47:23,739
so you can put more of
them in this small area.

1016
00:47:23,739 --> 00:47:27,240
That's why this CPU
becomes much faster.

1017
00:47:27,680 --> 00:47:36,059
Makes sense? Okay. Um,
but at some point,

1018
00:47:36,059 --> 00:47:37,780
due to physical limitation,

1019
00:47:37,780 --> 00:47:41,359
we cannot further reduce
the size of ALU, right?

1020
00:47:42,000 --> 00:47:45,859
So basically, we are
basically reducing it

1021
00:47:45,859 --> 00:47:49,199
from 70 anomeres to 60 to 50.

1022
00:47:49,199 --> 00:47:51,679
And this is basically the
history of value, right?

1023
00:47:51,679 --> 00:47:53,759
And at this point, what we

1024
00:47:53,759 --> 00:47:55,799
can do best is basically
this one, right?

1025
00:47:55,799 --> 00:47:57,700
By Apple, our favorite company,

1026
00:47:57,700 --> 00:47:59,719
and they can do 300 meter, okay?

1027
00:47:59,719 --> 00:48:02,259
That means you can put
a three indremere here.

1028
00:48:02,259 --> 00:48:03,799
And apparently, compared to

1029
00:48:03,799 --> 00:48:05,539
70 anomere you can
put a lot, right?

1030
00:48:05,539 --> 00:48:07,879
That's why Apple's chips
are so good, right?

1031
00:48:07,879 --> 00:48:12,310
Okay. But we cannot

1032
00:48:12,310 --> 00:48:14,109
do better because we are

1033
00:48:14,109 --> 00:48:16,169
subject to physical
limitations, okay?

1034
00:48:16,169 --> 00:48:19,069
So if you observe this trend,

1035
00:48:19,069 --> 00:48:20,789
you can see, uh,

1036
00:48:20,789 --> 00:48:22,669
this is Morse law,
by the way, okay?

1037
00:48:22,669 --> 00:48:24,089
Everyone understands this.

1038
00:48:24,089 --> 00:48:26,569
And apparently Morris
law is ending.

1039
00:48:26,569 --> 00:48:28,790
Uh, we cannot reduce
the sight of ARU,

1040
00:48:28,790 --> 00:48:30,709
so the computing power

1041
00:48:30,709 --> 00:48:33,949
of CPUs or similar chips
cannot increase anymore.

1042
00:48:33,949 --> 00:48:35,609
You can see the
plateaus at some point.

1043
00:48:35,609 --> 00:48:38,169
Okay? So how to
break this limit.

1044
00:48:38,169 --> 00:48:41,789
And you can see this
already plateau as 2015.

1045
00:48:41,789 --> 00:48:44,890
But that's not a reality.

1046
00:48:44,890 --> 00:48:45,929
If you writing check,

1047
00:48:45,929 --> 00:48:47,909
our computing power is
still increasing today.

1048
00:48:47,909 --> 00:48:49,729
Okay? There are two ways,

1049
00:48:49,729 --> 00:48:51,270
in my opinion, you can disagree.

1050
00:48:51,270 --> 00:48:52,569
Okay, feel free to comment.

1051
00:48:52,569 --> 00:48:55,429
One way is basically you
go to the quantum world,

1052
00:48:55,429 --> 00:48:57,309
I'll become admin and you

1053
00:48:57,309 --> 00:48:59,590
try to develop
quantum computing,

1054
00:48:59,590 --> 00:49:01,250
quantum computing is definitely

1055
00:49:01,250 --> 00:49:03,530
a active area of research.

1056
00:49:03,530 --> 00:49:04,789
I think in UCSD, there are

1057
00:49:04,789 --> 00:49:06,469
quite a few factory doing this.

1058
00:49:06,469 --> 00:49:08,749
But this is too far away for me.

1059
00:49:08,749 --> 00:49:11,049
I want something that
is closer, okay?

1060
00:49:11,049 --> 00:49:12,550
So the other way is basically

1061
00:49:12,550 --> 00:49:13,689
we try to build
something that is

1062
00:49:13,689 --> 00:49:17,170
called specialized
hardware, okay?

1063
00:49:17,170 --> 00:49:20,329
Basically, we can
convert the Morse law

1064
00:49:20,329 --> 00:49:21,509
into horns law, okay?

1065
00:49:21,509 --> 00:49:24,739
I'm going to explain
this. Makes sense, right?

1066
00:49:24,739 --> 00:49:28,039
Okay. By the way,

1067
00:49:28,039 --> 00:49:30,280
quantum computing is a pretty
good area of research.

1068
00:49:30,280 --> 00:49:32,940
If you are a person
with patients,

1069
00:49:32,940 --> 00:49:35,379
you can invest into the
field for ten years,

1070
00:49:35,379 --> 00:49:36,539
maybe after ten years,

1071
00:49:36,539 --> 00:49:39,279
even something that's
so good, okay?

1072
00:49:40,030 --> 00:49:43,069
The reason specialized
hardware works

1073
00:49:43,069 --> 00:49:46,870
that instead of basically
putting a lot of controls,

1074
00:49:46,870 --> 00:49:48,429
putting a lot of
catches, putting a lot

1075
00:49:48,429 --> 00:49:52,530
of very complex
logic into the ALU,

1076
00:49:52,530 --> 00:49:54,549
I try to simplify the
core a little bit.

1077
00:49:54,549 --> 00:49:57,189
Okay? The CPO core
is so was told that

1078
00:49:57,189 --> 00:50:00,209
it can do many many things
like EFLs blah, blah, blah.

1079
00:50:00,209 --> 00:50:01,789
But I don't care about
that because now,

1080
00:50:01,789 --> 00:50:03,189
I only care about
machine learning.

1081
00:50:03,189 --> 00:50:04,529
Like I said, in
machine learning,

1082
00:50:04,529 --> 00:50:05,770
I only care about metmo.

1083
00:50:05,770 --> 00:50:08,825
So how about I just make
course that can only do Mtmo?

1084
00:50:08,825 --> 00:50:10,999
But nothing else, right?

1085
00:50:10,999 --> 00:50:13,839
And if I simplify that a
little bit in this way,

1086
00:50:13,839 --> 00:50:15,879
we can put so many
cores and we can make

1087
00:50:15,879 --> 00:50:18,459
the core even smaller and we
can put so many cores here.

1088
00:50:18,459 --> 00:50:20,819
Two reasons one reason

1089
00:50:20,819 --> 00:50:22,560
is because we can make
the core much smaller.

1090
00:50:22,560 --> 00:50:24,580
Second is we can
basically remove

1091
00:50:24,580 --> 00:50:26,759
those control units
from the board.

1092
00:50:26,759 --> 00:50:28,299
So we can basically take

1093
00:50:28,299 --> 00:50:30,564
their space and put more
and more cores, okay?

1094
00:50:30,564 --> 00:50:35,169
Yeah. Yeah, yeah, computing
units, yeah. Okay.

1095
00:50:35,169 --> 00:50:37,609
But if you compare the crew
on the left and right,

1096
00:50:37,609 --> 00:50:40,310
the red crew is
actually more stupid,

1097
00:50:40,310 --> 00:50:42,009
I can only do met mo.

1098
00:50:42,009 --> 00:50:45,329
But that's fine. Okay. We
can make it smart, right?

1099
00:50:45,329 --> 00:50:48,249
Okay. That is at
the lower level,

1100
00:50:48,249 --> 00:50:49,829
this is basically how GPU works.

1101
00:50:49,829 --> 00:50:53,269
Okay? And because of this,

1102
00:50:53,269 --> 00:50:56,129
we are basically from
some point in 2015,

1103
00:50:56,129 --> 00:50:59,550
we started massively
manufacturing GPUs, okay?

1104
00:50:59,550 --> 00:51:03,569
And one notable or
accelerators. Okay.

1105
00:51:03,569 --> 00:51:05,510
And GPU is one type
of accelerator.

1106
00:51:05,510 --> 00:51:09,559
So, um, uh the
idea, like I said,

1107
00:51:09,559 --> 00:51:12,039
is basically use tons we
use but weak and more

1108
00:51:12,039 --> 00:51:17,140
specialized and we do
massive data parism or SIMD,

1109
00:51:17,140 --> 00:51:21,120
it was first popularized
by media in early 2000

1110
00:51:21,120 --> 00:51:25,000
for video games for graphics
because originally,

1111
00:51:25,000 --> 00:51:26,299
they only find application in

1112
00:51:26,299 --> 00:51:27,719
video games because video games,

1113
00:51:27,719 --> 00:51:29,419
you also do mathmo a lot.

1114
00:51:29,419 --> 00:51:31,599
But at some point,
deploing takes off,

1115
00:51:31,599 --> 00:51:35,360
so this is fit with
deploying pretty well, okay?

1116
00:51:35,380 --> 00:51:37,539
Media has been investing in

1117
00:51:37,539 --> 00:51:39,299
this area for so long and they

1118
00:51:39,299 --> 00:51:40,799
also develop some software and

1119
00:51:40,799 --> 00:51:42,800
programming language
to make it faster,

1120
00:51:42,800 --> 00:51:45,019
to make sure people can
program on top of it.

1121
00:51:45,019 --> 00:51:46,540
One notable programming language

1122
00:51:46,540 --> 00:51:47,760
we are going to learn is Koda.

1123
00:51:47,760 --> 00:51:49,859
That's how Koda comes from.

1124
00:51:49,859 --> 00:51:52,399
Okay. Does that make sense?

1125
00:51:52,399 --> 00:51:55,639
Cool. Um, uh,

1126
00:51:55,639 --> 00:51:56,799
usually you need to take

1127
00:51:56,799 --> 00:51:58,399
architectural class
to learn this,

1128
00:51:58,399 --> 00:51:59,659
but in this class, we are

1129
00:51:59,659 --> 00:52:00,940
going to cover it in 20 minutes,

1130
00:52:00,940 --> 00:52:03,479
and we are not going to
do super deep, okay.

1131
00:52:03,479 --> 00:52:07,399
Um, but I want to add a
little bit more here.

1132
00:52:07,399 --> 00:52:10,199
So there are usually
type three ways to

1133
00:52:10,199 --> 00:52:13,819
basically build specialized
hardware or accelerators,

1134
00:52:13,819 --> 00:52:17,439
which I summarize, next.

1135
00:52:17,439 --> 00:52:20,659
But here, I list some very,

1136
00:52:20,659 --> 00:52:25,039
uh famous, uh, what I
call accelerator, Okay.

1137
00:52:25,039 --> 00:52:28,120
The first one, TPU, you already
know Google Divided TPU.

1138
00:52:28,120 --> 00:52:30,059
The reason Google
Divided TPO because they

1139
00:52:30,059 --> 00:52:32,640
want to pay a media,
quite apparently,

1140
00:52:32,640 --> 00:52:37,659
TPU is one type of accelerator
and TPU can only do,

1141
00:52:37,659 --> 00:52:39,619
as you can infer from the lame,

1142
00:52:39,619 --> 00:52:44,644
tensor related operations,
mostly just memo, okay?

1143
00:52:44,644 --> 00:52:47,830
And the second one is basically,

1144
00:52:47,830 --> 00:52:50,109
Nvidia is trying
to manufacture in

1145
00:52:50,109 --> 00:52:52,489
their next generation
GPO is called

1146
00:52:52,489 --> 00:52:56,249
B 201 feature of B

1147
00:52:56,249 --> 00:53:00,350
200 is that they can do pretty
well on certain precision.

1148
00:53:00,350 --> 00:53:03,190
For example,
originally we do P 32,

1149
00:53:03,190 --> 00:53:05,970
in transformer, we do P 16.

1150
00:53:05,970 --> 00:53:08,129
At this point, media

1151
00:53:08,129 --> 00:53:09,909
is saying you should
do LP four P eight.

1152
00:53:09,909 --> 00:53:13,109
Okay. And specializing in

1153
00:53:13,109 --> 00:53:16,184
precision is also a type
of specialization, okay?

1154
00:53:16,184 --> 00:53:18,779
And the third one,
of course, M three.

1155
00:53:18,779 --> 00:53:21,339
That is what we have in
my loft laptop, okay.

1156
00:53:21,339 --> 00:53:22,720
It's also a
specialized hardware,

1157
00:53:22,720 --> 00:53:25,159
and we are going to dive
deep into that, okay?

1158
00:53:25,159 --> 00:53:26,859
And to summarize a little bit,

1159
00:53:26,859 --> 00:53:28,699
so what s value are
building today,

1160
00:53:28,699 --> 00:53:31,160
there are major three
directions, okay?

1161
00:53:31,160 --> 00:53:33,419
The first one is we try to

1162
00:53:33,780 --> 00:53:38,519
First we try to specialize
the functionality, okay?

1163
00:53:38,519 --> 00:53:41,099
For example, we
produce chips that

1164
00:53:41,099 --> 00:53:44,080
are only good at
metam that is TPU,

1165
00:53:44,080 --> 00:53:49,140
or only good at certain
computation with sparsity.

1166
00:53:49,140 --> 00:53:52,559
Okay? Or we can probably make

1167
00:53:52,559 --> 00:53:55,680
a chip that is not so specialized,
but still specialized.

1168
00:53:55,680 --> 00:53:58,660
For example, I can
mix CPUs with GPUs,

1169
00:53:58,660 --> 00:54:01,579
and that is essentially
what Apple does, right?

1170
00:54:01,579 --> 00:54:04,880
Because if you see the
marketing material from Apple,

1171
00:54:04,880 --> 00:54:07,500
every time they
release a new laptop,

1172
00:54:07,500 --> 00:54:09,899
they're saying, I
have so many cores.

1173
00:54:09,899 --> 00:54:11,940
Some cars are so good
at video processing.

1174
00:54:11,940 --> 00:54:13,239
So cores are good
at deep learning.

1175
00:54:13,239 --> 00:54:14,540
Essentially, they're
building different

1176
00:54:14,540 --> 00:54:16,009
cs together into one board.

1177
00:54:16,009 --> 00:54:20,600
Okay. Makes sense, right.
Functionality specialized.

1178
00:54:20,600 --> 00:54:22,779
I call it. Okay?
The second way to

1179
00:54:22,779 --> 00:54:24,099
specialize is basically reduce

1180
00:54:24,099 --> 00:54:25,459
precision, I already mentioned.

1181
00:54:25,459 --> 00:54:28,880
So I don't I probably

1182
00:54:28,880 --> 00:54:32,480
want to I can make it faster
by computing on a shorter,

1183
00:54:32,480 --> 00:54:35,880
like rate shorter floating
point reprendton.

1184
00:54:35,880 --> 00:54:37,519
Yeah. Okay. This is

1185
00:54:37,519 --> 00:54:39,839
what media is strongly
advocating, right?

1186
00:54:39,839 --> 00:54:42,239
And I think ten years ago,

1187
00:54:42,239 --> 00:54:43,419
when I studied machine
learning, people

1188
00:54:43,419 --> 00:54:45,300
only operate on LP 32.

1189
00:54:45,300 --> 00:54:47,999
But today, the up to date model,

1190
00:54:47,999 --> 00:54:49,539
for example, the
deep sik model with

1191
00:54:49,539 --> 00:54:51,379
three trained on P eight.

1192
00:54:51,379 --> 00:54:52,559
You can see the
precision already

1193
00:54:52,559 --> 00:54:54,559
reduced by four times, okay?

1194
00:54:54,559 --> 00:54:58,220
And the third way you
can specialize hardware

1195
00:54:58,220 --> 00:55:00,000
is you try to tune

1196
00:55:00,000 --> 00:55:02,929
the distribution or
configuration of the board.

1197
00:55:02,929 --> 00:55:07,160
Remember, when we
do met telling,

1198
00:55:07,160 --> 00:55:11,840
we have a few parameters
from the from hardware.

1199
00:55:11,840 --> 00:55:13,679
That is how many L
one cache you have,

1200
00:55:13,679 --> 00:55:15,519
how many L two cache you have.

1201
00:55:15,519 --> 00:55:18,619
How many memory you have and
how many cores you have.

1202
00:55:18,619 --> 00:55:22,019
Okay? In general, people have to

1203
00:55:22,019 --> 00:55:25,660
try to strike a balance
between these parameters.

1204
00:55:25,660 --> 00:55:27,499
But there are companies
in Silicon Valley.

1205
00:55:27,499 --> 00:55:29,520
They try to specialize
the hardware.

1206
00:55:29,520 --> 00:55:31,360
For example, they
just give up memory.

1207
00:55:31,360 --> 00:55:34,140
I just build a chip with
L one cache no memory.

1208
00:55:34,140 --> 00:55:37,060
And this kind of chip can do
so well on certain tasks.

1209
00:55:37,060 --> 00:55:38,699
That's another way that you can

1210
00:55:38,699 --> 00:55:41,819
specialize your hardware, Okay?

1211
00:55:42,050 --> 00:55:46,370
Now, I'm going to teach you a
very important thing, okay?

1212
00:55:46,370 --> 00:55:48,730
You should be able and you
should have the ability

1213
00:55:48,730 --> 00:55:51,630
to read the sheet,

1214
00:55:51,630 --> 00:55:53,889
product specification
provided by

1215
00:55:53,889 --> 00:55:55,649
MDia AMD and by

1216
00:55:55,649 --> 00:55:57,690
whatever chip company because
I think it's so important.

1217
00:55:57,690 --> 00:55:59,809
That's why sin value is
called siting value, right.

1218
00:55:59,809 --> 00:56:01,769
So this is a pronoun sheet

1219
00:56:01,769 --> 00:56:05,109
that we are going to
take as is study, okay?

1220
00:56:05,109 --> 00:56:08,269
And this is GPU spec provided by

1221
00:56:08,269 --> 00:56:12,370
NVDIa for their GPU current
generating GPU 100,

1222
00:56:12,370 --> 00:56:15,569
my lab has a few, and my
students really love that, okay?

1223
00:56:15,569 --> 00:56:19,469
And by looking at
this pronoun sheet,

1224
00:56:19,469 --> 00:56:21,370
you can see, the
first few rules,

1225
00:56:21,370 --> 00:56:23,249
what are you talking about?

1226
00:56:24,330 --> 00:56:28,850
It's talking about
the computing power

1227
00:56:28,850 --> 00:56:30,849
of this card, right?

1228
00:56:30,849 --> 00:56:33,729
One thing that you notice that

1229
00:56:33,729 --> 00:56:38,830
the computing power on
different cores are different.

1230
00:56:38,830 --> 00:56:42,849
That's why one core in immediate
is called tensor core.

1231
00:56:42,849 --> 00:56:44,769
That means they build
a core that is just

1232
00:56:44,769 --> 00:56:48,510
good for tensor related
operations, for example, metamo.

1233
00:56:48,510 --> 00:56:52,309
If we perform the same type
of precision, for example,

1234
00:56:52,309 --> 00:56:55,310
here, both of these
two rules basically

1235
00:56:55,310 --> 00:56:58,849
perform competition on 32
bits of floating point.

1236
00:56:58,849 --> 00:57:00,989
But if you use tensor core,

1237
00:57:00,989 --> 00:57:06,480
you get a number of
almost 1,000 Terra flops.

1238
00:57:06,480 --> 00:57:07,579
By the way Terra flops is

1239
00:57:07,579 --> 00:57:09,559
the unit that describe
the computing power.

1240
00:57:09,559 --> 00:57:11,339
We are going to
cover that later.

1241
00:57:11,339 --> 00:57:13,899
But basically number
higher is better, o.

1242
00:57:13,899 --> 00:57:15,960
But if you basically perform

1243
00:57:15,960 --> 00:57:17,159
the computing on
the normal core you

1244
00:57:17,159 --> 00:57:19,060
can see, it's ten times slower.

1245
00:57:19,060 --> 00:57:20,940
That's one type of
specialization.

1246
00:57:20,940 --> 00:57:28,680
Okay. GPU. Yeah, but
majority is centerc.

1247
00:57:28,680 --> 00:57:31,899
For this GPU, the
majority is center core.

1248
00:57:31,899 --> 00:57:34,979
Because this GPU is
manufactured for deep learning.

1249
00:57:34,979 --> 00:57:37,560
There are different
types of GPUs.

1250
00:57:38,000 --> 00:57:42,279
And it spends quite a few
rows to describe, um,

1251
00:57:42,279 --> 00:57:46,680
uh, the flops that is
for different precision.

1252
00:57:46,680 --> 00:57:50,240
And then it also specify
something that is important,

1253
00:57:50,240 --> 00:57:52,100
for example, the
memory bandwidth.

1254
00:57:52,100 --> 00:57:54,160
What is the memory bandwidth?

1255
00:57:56,390 --> 00:58:00,009
Yeah, essentially
moving things from

1256
00:58:00,009 --> 00:58:04,150
the GPS memory to
catch or to register.

1257
00:58:04,150 --> 00:58:06,330
That's the speed
term that we applied

1258
00:58:06,330 --> 00:58:08,910
when we do that memo,

1259
00:58:08,910 --> 00:58:11,609
and that number basically
determine how fast

1260
00:58:11,609 --> 00:58:15,470
it is to move a floating point
between memory hierarchy.

1261
00:58:15,670 --> 00:58:19,750
One interesting factor is,
if you read these flops,

1262
00:58:19,750 --> 00:58:21,829
you will see all these terms

1263
00:58:21,829 --> 00:58:25,589
comes with a star. So
what is that star?

1264
00:58:29,380 --> 00:58:31,839
It's a very small font,

1265
00:58:31,839 --> 00:58:34,139
you can see, marketing

1266
00:58:34,139 --> 00:58:36,719
people really cheating.
They're lying.

1267
00:58:36,719 --> 00:58:40,160
Okay. The star means
with sparsity.

1268
00:58:40,160 --> 00:58:43,060
That means if you want to
achieve the peak flops,

1269
00:58:43,060 --> 00:58:46,700
you have to apply a
sparsified competon.

1270
00:58:46,700 --> 00:58:50,459
Okay? And I don't know
why media does this,

1271
00:58:50,459 --> 00:58:51,779
but this is so confusing, right?

1272
00:58:51,779 --> 00:58:53,459
Because when you try
to buy a GPU you saw

1273
00:58:53,459 --> 00:58:55,579
your GPU can achieve
1,000 flops,

1274
00:58:55,579 --> 00:58:57,700
but it's only applied
with sparsity.

1275
00:58:57,700 --> 00:59:01,980
Does anyone know how many
it has without sparsity?

1276
00:59:03,270 --> 00:59:09,609
Sorry? Yeah, yeah.
Yeah, it's only half.

1277
00:59:09,609 --> 00:59:11,829
Yeah. If you apply

1278
00:59:11,829 --> 00:59:14,730
compution without sparsity,
it's only half of the flops.

1279
00:59:14,730 --> 00:59:17,729
That's why you need to learn
how to read product set.

1280
00:59:17,729 --> 00:59:20,249
Otherwise, when you spend
money, you need to be wise.

1281
00:59:20,249 --> 00:59:24,589
Please. Yeah, I why
explain that later.

1282
00:59:24,589 --> 00:59:28,709
Okay. So what does spars
mean? This is sparsity means.

1283
00:59:28,709 --> 00:59:31,709
I would like to appreciate
this plot again

1284
00:59:31,709 --> 00:59:33,189
for 10 seconds and try to

1285
00:59:33,189 --> 00:59:35,749
find the patterns
on the left plot.

1286
00:59:43,770 --> 00:59:51,730
Okay. Any pattern you you
found. Yeah, exactly.

1287
00:59:51,730 --> 00:59:54,090
In media, if you
are able to write

1288
00:59:54,090 --> 00:59:57,090
a kernel or a function,

1289
00:59:57,090 --> 01:00:00,010
that always perform computation,

1290
01:00:00,010 --> 01:00:02,749
subject to a constraint that
is average will only have

1291
01:00:02,749 --> 01:00:06,570
half elements and the rest
of them are basically empty,

1292
01:00:06,570 --> 01:00:09,150
then you basically
get these flops.

1293
01:00:09,150 --> 01:00:11,649
That is what sparsity means,

1294
01:00:11,649 --> 01:00:15,689
you can see this is
pretty hard to to do,

1295
01:00:15,689 --> 01:00:18,310
at least for me, I feel
it's very hard to convert

1296
01:00:18,310 --> 01:00:19,749
my original competition into

1297
01:00:19,749 --> 01:00:21,669
something that fits into this.

1298
01:00:21,669 --> 01:00:23,709
Cool. This is specially means.

1299
01:00:23,709 --> 01:00:26,010
This basically echoes what
I said at the beginning.

1300
01:00:26,010 --> 01:00:29,590
So one way you can
specialize your hardware

1301
01:00:29,590 --> 01:00:31,709
you make some course
that is so good at

1302
01:00:31,709 --> 01:00:34,309
some type of specified
competition.

1303
01:00:34,309 --> 01:00:36,559
That is what media does.

1304
01:00:36,559 --> 01:00:39,949
Okay, um, to continue
this case study,

1305
01:00:39,949 --> 01:00:41,829
you can also see if you

1306
01:00:41,829 --> 01:00:46,009
compare different
precisions the computation,

1307
01:00:46,009 --> 01:00:47,450
uh, the peak flops,

1308
01:00:47,450 --> 01:00:50,389
the peak computing power
also differ, right?

1309
01:00:50,430 --> 01:00:52,969
And we are going to
talk about this later,

1310
01:00:52,969 --> 01:00:56,469
and this is the fundamental
for quantization.

1311
01:00:56,469 --> 01:00:58,609
So what do we do in machinery
in quantization is we

1312
01:00:58,609 --> 01:01:00,650
try to quantize the
original precision,

1313
01:01:00,650 --> 01:01:03,409
for example, FP 32 all
the way down to FP eight.

1314
01:01:03,409 --> 01:01:05,610
Okay? And in media,

1315
01:01:05,610 --> 01:01:07,310
the reason people
really love quantity

1316
01:01:07,310 --> 01:01:09,389
is because of this,

1317
01:01:09,389 --> 01:01:11,350
if you are able to quantize

1318
01:01:11,350 --> 01:01:13,750
your original computation
into a lower precision,

1319
01:01:13,750 --> 01:01:17,050
you can benefit from more
powerful computing PC flops.

1320
01:01:17,050 --> 01:01:20,569
It will become
faster. Okay? Why it

1321
01:01:20,569 --> 01:01:22,629
can become faster, is very
easy to understand, right?

1322
01:01:22,629 --> 01:01:23,890
So original IP 32,

1323
01:01:23,890 --> 01:01:25,369
you have to operate on 32 bits,

1324
01:01:25,369 --> 01:01:27,189
but now you only
operate on eight bits.

1325
01:01:27,189 --> 01:01:29,469
Of course, it's four
times faster, right?

1326
01:01:29,469 --> 01:01:33,349
Okay. But the question
here I want to ask,

1327
01:01:33,349 --> 01:01:34,649
and I'm not going to give you

1328
01:01:34,649 --> 01:01:36,229
answer is why this could work in

1329
01:01:36,229 --> 01:01:40,509
machine learning programs.
Okay, think about that, right?

1330
01:01:40,509 --> 01:01:42,850
This is magical, right?
Because in many other areas,

1331
01:01:42,850 --> 01:01:45,850
which is not machine learning,
you reduce precision.

1332
01:01:45,850 --> 01:01:48,770
You are going to have some
errors, right on your results.

1333
01:01:48,770 --> 01:01:51,329
Yeah. But in margine learning,
this can be made work.

1334
01:01:51,329 --> 01:01:52,409
That's why immerge learning.

1335
01:01:52,409 --> 01:01:54,205
There's such big
field called qtison.

1336
01:01:54,205 --> 01:02:00,159
Okay. Cool. I think we finish
our kids study one, okay?

1337
01:02:00,159 --> 01:02:02,020
The next study is basically,

1338
01:02:02,020 --> 01:02:06,200
we try to read the product
sheet of Apple silicon.

1339
01:02:06,200 --> 01:02:08,340
What is Apple cooking?

1340
01:02:08,340 --> 01:02:11,419
So this is what Apple
cooking, right?

1341
01:02:11,420 --> 01:02:14,439
By looking at this,
you're basically saying

1342
01:02:14,439 --> 01:02:15,499
they are conveying
a message that

1343
01:02:15,499 --> 01:02:17,139
Michip can't do anything.

1344
01:02:17,139 --> 01:02:19,200
It can do video, it
can do machinery,

1345
01:02:19,200 --> 01:02:20,640
it can do graphics,

1346
01:02:20,640 --> 01:02:22,599
it can do anything, but
how Apple achieves that.

1347
01:02:22,599 --> 01:02:25,500
Okay? The way Apple
achieves that is basically,

1348
01:02:25,500 --> 01:02:27,759
you can see, this is a por okay?

1349
01:02:27,759 --> 01:02:32,530
And they are going to put a
16 core CPO in this corner.

1350
01:02:32,530 --> 01:02:35,060
Okay. And then for
the rest of areas,

1351
01:02:35,060 --> 01:02:37,320
this area are basically
put some control

1352
01:02:37,320 --> 01:02:40,099
or catches or whatever,
you don't care about it.

1353
01:02:40,099 --> 01:02:42,479
Okay. And in the area below,

1354
01:02:42,479 --> 01:02:45,450
what they do is they
put a 40 chord GPU.

1355
01:02:45,450 --> 01:02:47,860
So what happens is essentially

1356
01:02:47,860 --> 01:02:50,579
this M three Max is
mixing some CPU course,

1357
01:02:50,579 --> 01:02:52,180
which are super versatile

1358
01:02:52,180 --> 01:02:54,340
with some GPU course
which are not versatile,

1359
01:02:54,340 --> 01:02:55,920
but really good at memo.

1360
01:02:55,920 --> 01:02:58,979
So why Apple can do this?

1361
01:02:58,979 --> 01:03:01,140
Because Apple is a
software hardware company.

1362
01:03:01,140 --> 01:03:02,800
So they can optimize

1363
01:03:02,800 --> 01:03:04,179
their operating system in a way

1364
01:03:04,179 --> 01:03:06,079
that if there's
something that is

1365
01:03:06,079 --> 01:03:08,919
related with machine learning
with Mtmo I'm going to

1366
01:03:08,919 --> 01:03:12,259
dispatch my program to
run on this course.

1367
01:03:12,259 --> 01:03:14,299
But if something that
is normal, for example,

1368
01:03:14,299 --> 01:03:15,519
dit a doc, I'm going to

1369
01:03:15,519 --> 01:03:17,699
dispatch the computer
here, right.

1370
01:03:17,699 --> 01:03:19,339
No one else in the world can

1371
01:03:19,339 --> 01:03:20,860
do this because they
cannot do software,

1372
01:03:20,860 --> 01:03:23,614
hardware co opplementation.
Does that make sense?

1373
01:03:23,614 --> 01:03:26,730
Okay. Okay. That basically

1374
01:03:26,730 --> 01:03:29,830
finish our second is study.
I hope it's interesting.

1375
01:03:29,830 --> 01:03:31,109
Okay? I hope to teach you

1376
01:03:31,109 --> 01:03:33,309
some economics in
Cynic Valley, okay?

1377
01:03:33,309 --> 01:03:35,130
Is it sorry?

1378
01:03:35,130 --> 01:03:36,749
Yeah, Intel is
studying doing that,

1379
01:03:36,749 --> 01:03:37,789
but they are catching up.

1380
01:03:37,789 --> 01:03:42,510
Yeah. And also, Intel does
not own software layer.

1381
01:03:42,510 --> 01:03:44,250
Mc OS is Apples,

1382
01:03:44,250 --> 01:03:45,950
right? Windows is Microsoft.

1383
01:03:45,950 --> 01:03:51,190
Okay. Okay. The third study
will wrap up this lecture.

1384
01:03:51,190 --> 01:03:53,069
Okay. Let's try to see what's

1385
01:03:53,069 --> 01:03:55,149
going on and what people are
innovating Cynical Valley.

1386
01:03:55,149 --> 01:03:57,349
So what they are
trying to how you can

1387
01:03:57,349 --> 01:04:01,295
beat MD Apple to monopoly, okay?

1388
01:04:01,295 --> 01:04:04,179
I believe most of you
already heard about this,

1389
01:04:04,179 --> 01:04:07,919
um, three companies, Grock,
cerebrus and Sabava.

1390
01:04:07,919 --> 01:04:08,399
Okay.

1391
01:04:08,399 --> 01:04:10,819
These are three companies
that are basically trying to

1392
01:04:10,819 --> 01:04:12,920
manufacture a new
next generation

1393
01:04:12,920 --> 01:04:15,019
of chips that are so good
at machine learning.

1394
01:04:15,019 --> 01:04:16,519
So what are the cooking, Okay?

1395
01:04:16,519 --> 01:04:18,099
So they basically fall into

1396
01:04:18,099 --> 01:04:21,019
the sort of category I mentioned
in my previous slides.

1397
01:04:21,019 --> 01:04:23,959
They are trying to tune
the configurations,

1398
01:04:23,959 --> 01:04:27,379
for example, catch
size, uh, memory, uh,

1399
01:04:27,379 --> 01:04:30,359
like register size, to make sure

1400
01:04:30,359 --> 01:04:32,459
that they try to

1401
01:04:32,459 --> 01:04:34,780
find a combination that works
best for certain workloads,

1402
01:04:34,780 --> 01:04:37,280
for example, language
model inference.

1403
01:04:37,280 --> 01:04:40,199
Okay? And if you look at
this performance chart,

1404
01:04:40,199 --> 01:04:46,020
you can see the chips produced
by Grock on this workload,

1405
01:04:46,020 --> 01:04:48,740
which is basically trying to
generate tokens from Lama,

1406
01:04:48,740 --> 01:04:51,819
it can achieve almost ten
times faster than the rest.

1407
01:04:51,819 --> 01:04:54,140
The rest of companies are
some endpoint companies,

1408
01:04:54,140 --> 01:04:56,399
startups, very
famous ones, okay?

1409
01:04:56,399 --> 01:04:59,119
But they are bit so hard by

1410
01:04:59,119 --> 01:05:03,680
Grogu the rest of companies
are all adopting media GPUs.

1411
01:05:03,680 --> 01:05:06,740
This is basically the power
of reinventing hardware.

1412
01:05:06,740 --> 01:05:08,539
Okay. I don't think
you can achieve

1413
01:05:08,539 --> 01:05:11,699
this on software level,

1414
01:05:11,699 --> 01:05:12,899
but we're still
going to study that

1415
01:05:12,899 --> 01:05:15,099
okay because it's
a software class.

1416
01:05:15,099 --> 01:05:17,839
The reason it can
achieve this is

1417
01:05:17,839 --> 01:05:20,560
basically if you compare

1418
01:05:20,560 --> 01:05:22,860
the product sheet of
Grock card media,

1419
01:05:22,860 --> 01:05:24,439
you will find a key
difference that

1420
01:05:24,439 --> 01:05:26,920
is there's a line called SRM.

1421
01:05:26,920 --> 01:05:31,719
What is RM? Okay, fine,

1422
01:05:31,719 --> 01:05:32,999
you don't understand it because

1423
01:05:32,999 --> 01:05:34,319
I'm going to teach
that next week,

1424
01:05:34,319 --> 01:05:36,020
but you can understand SRM

1425
01:05:36,020 --> 01:05:38,119
as something similar to LON cat.

1426
01:05:38,119 --> 01:05:39,839
It's basically a layer

1427
01:05:39,839 --> 01:05:41,739
in the memory hierarchy,
but it's not a memory,

1428
01:05:41,739 --> 01:05:43,400
it's faster memory,
but it's slower

1429
01:05:43,400 --> 01:05:46,379
than register. It's
a layer there.

1430
01:05:46,379 --> 01:05:51,019
And if you compare the SRAM
says on the left hand side,

1431
01:05:51,019 --> 01:05:53,659
you'll find this GR card
basically make a card

1432
01:05:53,659 --> 01:05:57,180
with 230 megabytes of SRM.

1433
01:05:57,180 --> 01:05:59,900
But if you compare to
the latest idea chip,

1434
01:05:59,900 --> 01:06:05,220
they only have 164
kilowatt of SRM.

1435
01:06:05,220 --> 01:06:06,699
That's how Grock wins,

1436
01:06:06,699 --> 01:06:08,869
okay. Why it wins?

1437
01:06:08,869 --> 01:06:10,809
Because if I remind
you about this loop,

1438
01:06:10,809 --> 01:06:12,490
you will find that suppose

1439
01:06:12,490 --> 01:06:14,910
this register is
equivalent to SRAM.

1440
01:06:14,910 --> 01:06:18,330
You can imagine if I
have a register of size

1441
01:06:18,330 --> 01:06:20,129
of 100 megabytes versus

1442
01:06:20,129 --> 01:06:22,509
100 kilobyts, which
one will be faster.

1443
01:06:22,509 --> 01:06:24,370
Because I can't tell my matrix

1444
01:06:24,370 --> 01:06:25,529
multiplication in a way that

1445
01:06:25,529 --> 01:06:28,630
is I don't need to even
move anything actually.

1446
01:06:28,630 --> 01:06:30,510
I just put everything
in my register.

1447
01:06:30,510 --> 01:06:32,469
That's why it's so fast.

1448
01:06:32,469 --> 01:06:33,949
I hope I give you

1449
01:06:33,949 --> 01:06:36,469
a high level intuition
what's going on here.

1450
01:06:37,410 --> 01:06:42,029
Okay, that pretty much
is all I have for today,

1451
01:06:42,029 --> 01:06:44,649
but I'm going to give
you two questions. Okay.

1452
01:06:44,649 --> 01:06:48,170
The first question is, I
want you to study B 100.

1453
01:06:48,170 --> 01:06:50,029
You should basically follow what

1454
01:06:50,029 --> 01:06:51,509
I teach you today
and you try to study

1455
01:06:51,509 --> 01:06:55,309
the pronoun sheet of B
100 and C. You know,

1456
01:06:55,309 --> 01:06:56,929
media is claiming that B

1457
01:06:56,929 --> 01:07:00,129
100 is continuing the
Morse law, right?

1458
01:07:00,129 --> 01:07:03,649
By Mrs law, it means
that every 16 months

1459
01:07:03,649 --> 01:07:05,769
you can double your
computing power.

1460
01:07:05,769 --> 01:07:07,649
I want you to answer
this question, how B

1461
01:07:07,649 --> 01:07:09,829
100 continue Morris law

1462
01:07:09,829 --> 01:07:12,150
compared to the previous
generation which is h 100,

1463
01:07:12,150 --> 01:07:15,170
o also B 200, which is
another generation.

1464
01:07:15,170 --> 01:07:21,230
Okay. The ultimate question
here is basically,

1465
01:07:21,230 --> 01:07:23,929
you know, OID stock
is like this, right?

1466
01:07:24,410 --> 01:07:27,849
Is buy it. Yeah. Okay.

1467
01:07:28,090 --> 01:07:32,049
And the question I ask
is so you already say,

1468
01:07:32,049 --> 01:07:33,770
I introduced the economics,

1469
01:07:33,770 --> 01:07:35,089
the dynamics in sic value,

1470
01:07:35,089 --> 01:07:36,749
how people manufacture chips.

1471
01:07:36,749 --> 01:07:38,069
But you can see,

1472
01:07:38,069 --> 01:07:39,809
there are many companies who can

1473
01:07:39,809 --> 01:07:42,190
make really better
chips than OIDiA.

1474
01:07:42,190 --> 01:07:44,669
But then what is OID mode?

1475
01:07:44,669 --> 01:07:47,579
Like why OID stock is so good?

1476
01:07:47,579 --> 01:07:50,450
Yeah. Yeah, the software.

1477
01:07:50,450 --> 01:07:53,190
One primary is the software.

1478
01:07:53,190 --> 01:07:56,270
And if you want to
be more precise,

1479
01:07:56,270 --> 01:07:59,650
I think the reason
is because Koda

1480
01:07:59,650 --> 01:08:01,950
they have Koda and other
chips do not have Koda.

1481
01:08:01,950 --> 01:08:05,189
And KODA will be our
next week's focus. Okay.

1482
01:08:05,189 --> 01:08:06,789
Thank you.
